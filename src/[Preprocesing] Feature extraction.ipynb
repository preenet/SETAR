{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "139d4b2f",
   "metadata": {},
   "source": [
    "### Feature extractions\n",
    "\n",
    "This script extracted features from two sentimental corpora, kt4.0 (ours) and wisesight. By training from kt4.0 corpus, we expect to see an improvement in the wisesight corpus' classification performance.\n",
    "\n",
    "Several feature extraction methods were applied on text feature to both corpuses as follows:  \n",
    "\n",
    "* Bag of words for unigram and bigrams\n",
    "* TF-IDF for unigram and bigrams\n",
    "* Word2Vec with TF-IDF vector (300 dimension)\n",
    "* POS_tagging with flatten dataframe for unigram and bigrams\n",
    "* Dictionary-based with list of Thai positive and negative words for unigram and bigrams\n",
    "\n",
    "Output:  \n",
    "for all the feature extraction methods above, Joblib objects as numpy array and sparse matrix on text feature were dumped.     \n",
    "pree.t@cmu.ac.th  \n",
    "\n",
    "Todo:   \n",
    "* work on pad and emoticon tags  \n",
    "* test pos-tagging performace  \n",
    "* add thaitale corpus  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a978d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pythainlp\n",
    "import re\n",
    "import emoji\n",
    "from pythainlp import word_tokenize\n",
    "\n",
    "# for visualize\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams['font.family'] = 'tahoma'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47e606a",
   "metadata": {},
   "source": [
    "## Load original datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fe523d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60081, 14), (26737, 2))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.path.dirname(os.getcwd())\n",
    "\n",
    "data_path_kt = os.path.dirname(os.getcwd()) + '\\\\data\\kt4.0\\\\'\n",
    "data_path_ws = os.path.dirname(os.getcwd()) + '\\\\data\\wisesight\\\\'\n",
    "model_path = os.path.dirname(os.getcwd()) + '\\\\model\\\\'\n",
    "df_kt = pd.read_csv(data_path_kt + 'pantip_cleaned_1.csv')\n",
    "\n",
    "# we use the original wisesight corpus and reconstruct a new dataframe\n",
    "texts = []\n",
    "targets = []\n",
    "\n",
    "with open(str(data_path_ws) + '\\\\' + 'neg.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        texts.append(line.strip())\n",
    "        targets.append('neg')\n",
    "\n",
    "with open(str(data_path_ws) + '\\\\' + 'neu.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        texts.append(line.strip())\n",
    "        targets.append('neu')\n",
    "\n",
    "with open(str(data_path_ws) + '\\\\' + 'pos.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        texts.append(line.strip())\n",
    "        targets.append('pos')\n",
    "\n",
    "with open(str(data_path_ws) + '\\\\' + 'q.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        texts.append(line.strip())\n",
    "        targets.append('q')\n",
    "        \n",
    "df_ws = pd.DataFrame({'texts': texts, 'targets': targets})\n",
    "df_ws.to_csv(os.path.dirname(os.getcwd()) + '\\\\' + 'wisesight.csv', index=False)\n",
    "df_kt.shape, df_ws.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57eaffe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2, 0, 1], dtype=int8), array([0, 1, 2, 3], dtype=int8))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_kt = df_kt['vote'].astype('category').cat.codes\n",
    "y_ws = df_ws['targets'].astype('category').cat.codes\n",
    "y_kt.unique(), y_ws.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcb88291",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_kt.to_csv(os.path.dirname(os.getcwd()) + '\\\\' + 'label_kt.csv', index=False)\n",
    "#y_ws.to_csv(os.path.dirname(os.getcwd()) + '\\\\' + 'label_ws.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b8219fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_links(text):\n",
    "    text = re.sub(r'http\\S+', '', text) # remove http links\n",
    "    text = re.sub(r'bit.ly/\\S+', '', text) # remove bitly links\n",
    "    text = text.strip('[link]') # remove [links]\n",
    "    return text\n",
    "    \n",
    "def replace_rep(text):\n",
    "    def _replace_rep(m):\n",
    "        c,cc = m.groups()\n",
    "        return f'{c}xxrep'\n",
    "    re_rep = re.compile(r'(\\S)(\\1{2,})')\n",
    "    return re_rep.sub(_replace_rep, text)\n",
    "\n",
    "def remove_emoji(text):\n",
    "    regrex_pattern = re.compile(pattern = \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags = re.UNICODE)\n",
    "    return regrex_pattern.sub(r'',text)\n",
    "\n",
    "def remove_word_len(text):\n",
    "    new_list = [ word for word in text if len(word) >= 2 ]\n",
    "    return new_list\n",
    "\n",
    "def process_text(text):\n",
    "    #pre rules\n",
    "    res = remove_links(text)\n",
    "    res = text.lower().strip()\n",
    "    res = replace_rep(res)\n",
    "    res = remove_emoji(res)\n",
    "    \n",
    "    #tokenize with the newmm algo. and get rid of whitespaces\n",
    "    res = [word for word in word_tokenize(res, engine='newmm', keep_whitespace=False)if word and not re.search(pattern=r\"\\s+\", string=word)]\n",
    "    \n",
    "    #post rules\n",
    "    res = remove_word_len(res)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82ef6037",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kt['processed'] = df_kt['text'].apply(str).apply(process_text)\n",
    "df_ws['processed'] = df_ws['texts'].apply(str).apply(process_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "558ceee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texts</th>\n",
       "      <th>targets</th>\n",
       "      <th>processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>‚òπÔ∏è</td>\n",
       "      <td>neg</td>\n",
       "      <td>[‚òπÔ∏è]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>üòî</td>\n",
       "      <td>neg</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>üòû</td>\n",
       "      <td>neg</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>üò•</td>\n",
       "      <td>neg</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>‡∏£‡∏≥</td>\n",
       "      <td>neg</td>\n",
       "      <td>[‡∏£‡∏≥]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>No‡πÜ</td>\n",
       "      <td>neg</td>\n",
       "      <td>[no]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Rip</td>\n",
       "      <td>neg</td>\n",
       "      <td>[rip]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>T_T</td>\n",
       "      <td>neg</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>‡∏Å‡∏≤‡∏Å</td>\n",
       "      <td>neg</td>\n",
       "      <td>[‡∏Å‡∏≤‡∏Å]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>‡πÇ‡∏Å‡∏á</td>\n",
       "      <td>neg</td>\n",
       "      <td>[‡πÇ‡∏Å‡∏á]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  texts targets processed\n",
       "0    ‚òπÔ∏è     neg      [‚òπÔ∏è]\n",
       "1     üòî     neg        []\n",
       "2     üòû     neg        []\n",
       "3     üò•     neg        []\n",
       "4    ‡∏£‡∏≥     neg      [‡∏£‡∏≥]\n",
       "5   No‡πÜ     neg      [no]\n",
       "6   Rip     neg     [rip]\n",
       "7   T_T     neg        []\n",
       "8   ‡∏Å‡∏≤‡∏Å     neg     [‡∏Å‡∏≤‡∏Å]\n",
       "9   ‡πÇ‡∏Å‡∏á     neg     [‡πÇ‡∏Å‡∏á]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ws[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c128ab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>post_date</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_name</th>\n",
       "      <th>text</th>\n",
       "      <th>tag</th>\n",
       "      <th>emotion</th>\n",
       "      <th>length</th>\n",
       "      <th>num_sent</th>\n",
       "      <th>sent_length</th>\n",
       "      <th>label</th>\n",
       "      <th>label_1</th>\n",
       "      <th>label_2</th>\n",
       "      <th>vote</th>\n",
       "      <th>processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39839097</td>\n",
       "      <td>2020-04-25 13:24:00</td>\n",
       "      <td>https://pantip.com/profile/5798163</td>\n",
       "      <td>‡∏™‡∏°‡∏≤‡∏ä‡∏¥‡∏Å‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏•‡∏Ç 5798163</td>\n",
       "      <td>[CR] ‡πÅ‡∏õ‡∏±‡∏á‡∏û‡∏±‡∏ü‡∏Ñ‡∏∏‡∏°‡∏°‡∏±‡∏ô ‡∏à‡∏±‡∏î‡πÄ‡∏ï‡πá‡∏°‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏ö‡∏≤‡∏á‡πÄ‡∏ö‡∏≤</td>\n",
       "      <td>‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏™‡∏≥‡∏≠‡∏≤‡∏á</td>\n",
       "      <td>‡∏ñ‡∏π‡∏Å‡πÉ‡∏à 0 ‡∏Ç‡∏≥‡∏Å‡∏•‡∏¥‡πâ‡∏á 0 ‡∏´‡∏•‡∏á‡∏£‡∏±‡∏Å 0 ‡∏ã‡∏∂‡πâ‡∏á 0 ‡∏™‡∏¢‡∏≠‡∏á 0 ‡∏ó‡∏∂‡πà‡∏á 0</td>\n",
       "      <td>36</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>pos</td>\n",
       "      <td>[cr, ‡πÅ‡∏õ, ‡∏±‡∏á‡∏û‡∏±‡∏ü, ‡∏Ñ‡∏∏‡∏°, ‡∏°‡∏±‡∏ô, ‡∏à‡∏±‡∏î, ‡πÄ‡∏ï‡πá‡∏°, ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠, ‡∏ö‡∏≤...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39839097</td>\n",
       "      <td>2020-04-25 13:24:00</td>\n",
       "      <td>https://pantip.com/profile/5798163</td>\n",
       "      <td>‡∏™‡∏°‡∏≤‡∏ä‡∏¥‡∏Å‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏•‡∏Ç 5798163</td>\n",
       "      <td>‡πÑ‡∏°‡πà‡∏≠‡∏∏‡∏î‡∏ï‡∏±‡∏ô ‡πÅ‡∏ï‡πà‡∏õ‡∏Å‡∏õ‡∏¥‡∏î‡πÅ‡∏ô‡πà‡∏ô‡∏°‡∏≤‡∏Å</td>\n",
       "      <td>‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏™‡∏≥‡∏≠‡∏≤‡∏á</td>\n",
       "      <td>‡∏ñ‡∏π‡∏Å‡πÉ‡∏à 0 ‡∏Ç‡∏≥‡∏Å‡∏•‡∏¥‡πâ‡∏á 0 ‡∏´‡∏•‡∏á‡∏£‡∏±‡∏Å 0 ‡∏ã‡∏∂‡πâ‡∏á 0 ‡∏™‡∏¢‡∏≠‡∏á 0 ‡∏ó‡∏∂‡πà‡∏á 0</td>\n",
       "      <td>36</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>pos</td>\n",
       "      <td>[‡πÑ‡∏°‡πà, ‡∏≠‡∏∏‡∏î‡∏ï‡∏±‡∏ô, ‡πÅ‡∏ï‡πà, ‡∏õ‡∏Å‡∏õ‡∏¥‡∏î, ‡πÅ‡∏ô‡πà‡∏ô, ‡∏°‡∏≤‡∏Å]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39839097</td>\n",
       "      <td>2020-04-25 13:24:00</td>\n",
       "      <td>https://pantip.com/profile/5798163</td>\n",
       "      <td>‡∏™‡∏°‡∏≤‡∏ä‡∏¥‡∏Å‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏•‡∏Ç 5798163</td>\n",
       "      <td>‡∏£‡∏µ‡∏ß‡∏¥‡∏ß‡πÅ‡∏õ‡πâ‡∏á Lady Audrey Ready All Day ‡∏à‡πâ‡∏≤</td>\n",
       "      <td>‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏™‡∏≥‡∏≠‡∏≤‡∏á</td>\n",
       "      <td>‡∏ñ‡∏π‡∏Å‡πÉ‡∏à 0 ‡∏Ç‡∏≥‡∏Å‡∏•‡∏¥‡πâ‡∏á 0 ‡∏´‡∏•‡∏á‡∏£‡∏±‡∏Å 0 ‡∏ã‡∏∂‡πâ‡∏á 0 ‡∏™‡∏¢‡∏≠‡∏á 0 ‡∏ó‡∏∂‡πà‡∏á 0</td>\n",
       "      <td>36</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>pos</td>\n",
       "      <td>[‡∏£‡∏µ‡∏ß‡∏¥‡∏ß, ‡πÅ‡∏õ‡πâ‡∏á, lady, audrey, ready, all, day, ‡∏à‡πâ‡∏≤]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39838736</td>\n",
       "      <td>2020-04-25 10:52:00</td>\n",
       "      <td>https://pantip.com/profile/5730006</td>\n",
       "      <td>‡∏™‡∏°‡∏≤‡∏ä‡∏¥‡∏Å‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏•‡∏Ç 5730006</td>\n",
       "      <td>‡∏Ç‡∏≠‡∏ö‡∏ï‡∏≤‡∏î‡∏≥‡∏°‡∏≤‡∏Å‡∏Ñ‡πà‡∏∞ ‡∏Ñ‡∏≠‡∏£‡πå‡πÄ‡∏•‡πá‡∏Ñ‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏Å‡πá‡πÄ‡∏≠‡∏≤‡πÑ‡∏°‡πà‡∏≠‡∏¢‡∏π‡πà</td>\n",
       "      <td>‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏™‡∏≥‡∏≠‡∏≤‡∏á</td>\n",
       "      <td>‡∏ñ‡∏π‡∏Å‡πÉ‡∏à 0 ‡∏Ç‡∏≥‡∏Å‡∏•‡∏¥‡πâ‡∏á 0 ‡∏´‡∏•‡∏á‡∏£‡∏±‡∏Å 0 ‡∏ã‡∏∂‡πâ‡∏á 0 ‡∏™‡∏¢‡∏≠‡∏á 0 ‡∏ó‡∏∂‡πà‡∏á 0</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>neg</td>\n",
       "      <td>[‡∏Ç‡∏≠‡∏ö, ‡∏ï‡∏≤‡∏î‡∏≥, ‡∏°‡∏≤‡∏Å, ‡∏Ñ‡πà‡∏∞, ‡∏Ñ‡∏≠‡∏£‡πå, ‡πÄ‡∏•‡πá‡∏Ñ, ‡πÄ‡∏ï, ‡∏≠‡∏£, ‡∏Å‡πá, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>39837384</td>\n",
       "      <td>2020-04-24 20:39:00</td>\n",
       "      <td>https://pantip.com/profile/4975838</td>\n",
       "      <td>‡∏™‡∏°‡∏≤‡∏ä‡∏¥‡∏Å‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏•‡∏Ç 4975838</td>\n",
       "      <td>‡πÄ‡∏≠‡∏≤aloe Vera ‡πÅ‡∏ä‡πà‡∏ï‡∏π‡πâ‡πÄ‡∏¢‡πá‡∏ô ‡∏à‡∏ô‡∏Å‡∏•‡∏≤‡∏¢‡πÄ‡∏õ‡πá‡∏ô‡∏ô‡πâ‡∏≥‡πÅ‡∏Ç‡πá‡∏á</td>\n",
       "      <td>‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏™‡∏≥‡∏≠‡∏≤‡∏á</td>\n",
       "      <td>‡∏ñ‡∏π‡∏Å‡πÉ‡∏à 0 ‡∏Ç‡∏≥‡∏Å‡∏•‡∏¥‡πâ‡∏á 0 ‡∏´‡∏•‡∏á‡∏£‡∏±‡∏Å 0 ‡∏ã‡∏∂‡πâ‡∏á 0 ‡∏™‡∏¢‡∏≠‡∏á 0 ‡∏ó‡∏∂‡πà‡∏á 0</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>neu</td>\n",
       "      <td>[‡πÄ‡∏≠‡∏≤, aloe, vera, ‡πÅ‡∏ä‡πà, ‡∏ï‡∏π‡πâ‡πÄ‡∏¢‡πá‡∏ô, ‡∏à‡∏ô, ‡∏Å‡∏•‡∏≤‡∏¢‡πÄ‡∏õ‡πá‡∏ô, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>39838990</td>\n",
       "      <td>2020-04-25 12:36:00</td>\n",
       "      <td>https://pantip.com/profile/5655853</td>\n",
       "      <td>chdewxx</td>\n",
       "      <td>[SR] ‡πÑ‡∏≠‡πÄ‡∏ó‡∏° #‡πÄ‡∏ã‡∏£‡∏±‡πà‡∏°‡∏™‡∏¥‡∏ß ‡∏•‡∏î‡∏™‡∏¥‡∏ß ‡∏™‡∏¥‡∏ß‡∏≠‡∏∏‡∏î‡∏ï‡∏±‡∏ô ‡∏™‡∏¥‡∏ß‡∏ú‡∏î ‡∏ö‡∏≥...</td>\n",
       "      <td>‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏™‡∏≥‡∏≠‡∏≤‡∏á</td>\n",
       "      <td>‡∏ñ‡∏π‡∏Å‡πÉ‡∏à 0 ‡∏Ç‡∏≥‡∏Å‡∏•‡∏¥‡πâ‡∏á 0 ‡∏´‡∏•‡∏á‡∏£‡∏±‡∏Å 0 ‡∏ã‡∏∂‡πâ‡∏á 0 ‡∏™‡∏¢‡∏≠‡∏á 0 ‡∏ó‡∏∂‡πà‡∏á 0</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>pos</td>\n",
       "      <td>[sr, ‡πÑ‡∏≠‡πÄ‡∏ó‡∏°, ‡πÄ‡∏ã, ‡∏£‡∏±‡πà‡∏°, ‡∏™‡∏¥‡∏ß, ‡∏•‡∏î, ‡∏™‡∏¥‡∏ß, ‡∏™‡∏¥‡∏ß, ‡∏≠‡∏∏‡∏î‡∏ï‡∏±...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>39838619</td>\n",
       "      <td>2020-04-25 10:01:00</td>\n",
       "      <td>https://pantip.com/profile/5656639</td>\n",
       "      <td>‡∏Ñ‡∏π‡∏à‡∏≠‡∏á‡∏¢‡∏≠‡∏ô‡πÅ‡∏•‡∏∞‡∏ß‡∏µ‡∏£‡∏¢‡∏≤</td>\n",
       "      <td>‡∏£‡∏ö‡∏Å‡∏ß‡∏ô‡∏™‡∏≤‡∏ß‡πÜ‡∏ä‡πà‡∏ß‡∏¢‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏™‡∏Å‡∏¥‡∏ô‡πÅ‡∏Ñ‡∏£‡πå ‡∏ó‡∏µ‡πà‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡∏ú‡∏¥‡∏ß‡∏´‡∏ô‡πâ‡∏≤‡∏Ç‡∏≤...</td>\n",
       "      <td>‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏™‡∏≥‡∏≠‡∏≤‡∏á</td>\n",
       "      <td>‡∏ñ‡∏π‡∏Å‡πÉ‡∏à 0 ‡∏Ç‡∏≥‡∏Å‡∏•‡∏¥‡πâ‡∏á 0 ‡∏´‡∏•‡∏á‡∏£‡∏±‡∏Å 0 ‡∏ã‡∏∂‡πâ‡∏á 0 ‡∏™‡∏¢‡∏≠‡∏á 0 ‡∏ó‡∏∂‡πà‡∏á 0</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>pos</td>\n",
       "      <td>[‡∏£‡∏ö‡∏Å‡∏ß‡∏ô, ‡∏™‡∏≤‡∏ß, ‡∏ä‡πà‡∏ß‡∏¢, ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥, ‡∏Å‡∏¥‡∏ô, ‡πÅ‡∏Ñ‡∏£‡πå, ‡∏ó‡∏µ‡πà, ‡∏ä‡πà‡∏ß‡∏¢...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>39837266</td>\n",
       "      <td>2020-04-24 19:58:00</td>\n",
       "      <td>https://pantip.com/profile/632132</td>\n",
       "      <td>‡∏´‡∏°‡∏π‡∏Å‡∏•‡∏°‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå‡∏î‡∏µ</td>\n",
       "      <td>‡∏ó‡∏î‡∏•‡∏≠‡∏á‡πÉ‡∏ä‡πâ ‡πÅ‡∏Ñ‡∏£‡∏≠‡∏ó‡∏ß‡∏¥‡∏ï‡∏ã‡∏µ‡∏´‡∏ô‡πâ‡∏≤‡πÉ‡∏™</td>\n",
       "      <td>‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏™‡∏≥‡∏≠‡∏≤‡∏á</td>\n",
       "      <td>‡∏ñ‡∏π‡∏Å‡πÉ‡∏à 0 ‡∏Ç‡∏≥‡∏Å‡∏•‡∏¥‡πâ‡∏á 0 ‡∏´‡∏•‡∏á‡∏£‡∏±‡∏Å 0 ‡∏ã‡∏∂‡πâ‡∏á 0 ‡∏™‡∏¢‡∏≠‡∏á 0 ‡∏ó‡∏∂‡πà‡∏á 0</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>pos</td>\n",
       "      <td>[‡∏ó‡∏î‡∏•‡∏≠‡∏á‡πÉ‡∏ä‡πâ, ‡πÅ‡∏Ñ‡∏£‡∏≠‡∏ó, ‡∏ß‡∏¥, ‡∏ã‡∏µ, ‡∏´‡∏ô‡πâ‡∏≤, ‡πÉ‡∏™]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>39837266</td>\n",
       "      <td>2020-04-24 19:58:00</td>\n",
       "      <td>https://pantip.com/profile/632132</td>\n",
       "      <td>‡∏´‡∏°‡∏π‡∏Å‡∏•‡∏°‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå‡∏î‡∏µ</td>\n",
       "      <td>‡πÉ‡∏ô 1 ‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå</td>\n",
       "      <td>‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏™‡∏≥‡∏≠‡∏≤‡∏á</td>\n",
       "      <td>‡∏ñ‡∏π‡∏Å‡πÉ‡∏à 0 ‡∏Ç‡∏≥‡∏Å‡∏•‡∏¥‡πâ‡∏á 0 ‡∏´‡∏•‡∏á‡∏£‡∏±‡∏Å 0 ‡∏ã‡∏∂‡πâ‡∏á 0 ‡∏™‡∏¢‡∏≠‡∏á 0 ‡∏ó‡∏∂‡πà‡∏á 0</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>neu</td>\n",
       "      <td>[‡πÉ‡∏ô, ‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>39835926</td>\n",
       "      <td>2020-04-24 12:03:00</td>\n",
       "      <td>https://pantip.com/profile/3826851</td>\n",
       "      <td>‡∏™‡∏°‡∏≤‡∏ä‡∏¥‡∏Å‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏•‡∏Ç 3826851</td>\n",
       "      <td>‡∏ß‡∏¥‡∏ò‡∏µ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å \"‡∏£‡∏≠‡∏á‡∏û‡∏∑‡πâ‡∏ô\" ‡πÅ‡∏•‡∏∞ \"‡∏Ñ‡∏≠‡∏ô‡∏ã‡∏µ‡∏•‡πÄ‡∏•‡∏≠‡∏£‡πå\"</td>\n",
       "      <td>‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏™‡∏≥‡∏≠‡∏≤‡∏á</td>\n",
       "      <td>‡∏ñ‡∏π‡∏Å‡πÉ‡∏à 0 ‡∏Ç‡∏≥‡∏Å‡∏•‡∏¥‡πâ‡∏á 0 ‡∏´‡∏•‡∏á‡∏£‡∏±‡∏Å 0 ‡∏ã‡∏∂‡πâ‡∏á 0 ‡∏™‡∏¢‡∏≠‡∏á 0 ‡∏ó‡∏∂‡πà‡∏á 0</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>pos</td>\n",
       "      <td>[‡∏ß‡∏¥‡∏ò‡∏µ, ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å, ‡∏£‡∏≠‡∏á‡∏û‡∏∑‡πâ‡∏ô, ‡πÅ‡∏•‡∏∞, ‡∏Ñ‡∏≠‡∏ô, ‡∏ã‡∏µ‡∏•, ‡πÄ‡∏•‡∏≠, ‡∏£‡πå\"]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    post_id            post_date                             user_id  \\\n",
       "0  39839097  2020-04-25 13:24:00  https://pantip.com/profile/5798163   \n",
       "1  39839097  2020-04-25 13:24:00  https://pantip.com/profile/5798163   \n",
       "2  39839097  2020-04-25 13:24:00  https://pantip.com/profile/5798163   \n",
       "3  39838736  2020-04-25 10:52:00  https://pantip.com/profile/5730006   \n",
       "4  39837384  2020-04-24 20:39:00  https://pantip.com/profile/4975838   \n",
       "5  39838990  2020-04-25 12:36:00  https://pantip.com/profile/5655853   \n",
       "6  39838619  2020-04-25 10:01:00  https://pantip.com/profile/5656639   \n",
       "7  39837266  2020-04-24 19:58:00   https://pantip.com/profile/632132   \n",
       "8  39837266  2020-04-24 19:58:00   https://pantip.com/profile/632132   \n",
       "9  39835926  2020-04-24 12:03:00  https://pantip.com/profile/3826851   \n",
       "\n",
       "               user_name                                               text  \\\n",
       "0  ‡∏™‡∏°‡∏≤‡∏ä‡∏¥‡∏Å‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏•‡∏Ç 5798163              [CR] ‡πÅ‡∏õ‡∏±‡∏á‡∏û‡∏±‡∏ü‡∏Ñ‡∏∏‡∏°‡∏°‡∏±‡∏ô ‡∏à‡∏±‡∏î‡πÄ‡∏ï‡πá‡∏°‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏ö‡∏≤‡∏á‡πÄ‡∏ö‡∏≤   \n",
       "1  ‡∏™‡∏°‡∏≤‡∏ä‡∏¥‡∏Å‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏•‡∏Ç 5798163                          ‡πÑ‡∏°‡πà‡∏≠‡∏∏‡∏î‡∏ï‡∏±‡∏ô ‡πÅ‡∏ï‡πà‡∏õ‡∏Å‡∏õ‡∏¥‡∏î‡πÅ‡∏ô‡πà‡∏ô‡∏°‡∏≤‡∏Å   \n",
       "2  ‡∏™‡∏°‡∏≤‡∏ä‡∏¥‡∏Å‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏•‡∏Ç 5798163            ‡∏£‡∏µ‡∏ß‡∏¥‡∏ß‡πÅ‡∏õ‡πâ‡∏á Lady Audrey Ready All Day ‡∏à‡πâ‡∏≤   \n",
       "3  ‡∏™‡∏°‡∏≤‡∏ä‡∏¥‡∏Å‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏•‡∏Ç 5730006            ‡∏Ç‡∏≠‡∏ö‡∏ï‡∏≤‡∏î‡∏≥‡∏°‡∏≤‡∏Å‡∏Ñ‡πà‡∏∞ ‡∏Ñ‡∏≠‡∏£‡πå‡πÄ‡∏•‡πá‡∏Ñ‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏Å‡πá‡πÄ‡∏≠‡∏≤‡πÑ‡∏°‡πà‡∏≠‡∏¢‡∏π‡πà   \n",
       "4  ‡∏™‡∏°‡∏≤‡∏ä‡∏¥‡∏Å‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏•‡∏Ç 4975838          ‡πÄ‡∏≠‡∏≤aloe Vera ‡πÅ‡∏ä‡πà‡∏ï‡∏π‡πâ‡πÄ‡∏¢‡πá‡∏ô ‡∏à‡∏ô‡∏Å‡∏•‡∏≤‡∏¢‡πÄ‡∏õ‡πá‡∏ô‡∏ô‡πâ‡∏≥‡πÅ‡∏Ç‡πá‡∏á   \n",
       "5                chdewxx  [SR] ‡πÑ‡∏≠‡πÄ‡∏ó‡∏° #‡πÄ‡∏ã‡∏£‡∏±‡πà‡∏°‡∏™‡∏¥‡∏ß ‡∏•‡∏î‡∏™‡∏¥‡∏ß ‡∏™‡∏¥‡∏ß‡∏≠‡∏∏‡∏î‡∏ï‡∏±‡∏ô ‡∏™‡∏¥‡∏ß‡∏ú‡∏î ‡∏ö‡∏≥...   \n",
       "6       ‡∏Ñ‡∏π‡∏à‡∏≠‡∏á‡∏¢‡∏≠‡∏ô‡πÅ‡∏•‡∏∞‡∏ß‡∏µ‡∏£‡∏¢‡∏≤  ‡∏£‡∏ö‡∏Å‡∏ß‡∏ô‡∏™‡∏≤‡∏ß‡πÜ‡∏ä‡πà‡∏ß‡∏¢‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏™‡∏Å‡∏¥‡∏ô‡πÅ‡∏Ñ‡∏£‡πå ‡∏ó‡∏µ‡πà‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡∏ú‡∏¥‡∏ß‡∏´‡∏ô‡πâ‡∏≤‡∏Ç‡∏≤...   \n",
       "7         ‡∏´‡∏°‡∏π‡∏Å‡∏•‡∏°‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå‡∏î‡∏µ                          ‡∏ó‡∏î‡∏•‡∏≠‡∏á‡πÉ‡∏ä‡πâ ‡πÅ‡∏Ñ‡∏£‡∏≠‡∏ó‡∏ß‡∏¥‡∏ï‡∏ã‡∏µ‡∏´‡∏ô‡πâ‡∏≤‡πÉ‡∏™   \n",
       "8         ‡∏´‡∏°‡∏π‡∏Å‡∏•‡∏°‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå‡∏î‡∏µ                                       ‡πÉ‡∏ô 1 ‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå   \n",
       "9  ‡∏™‡∏°‡∏≤‡∏ä‡∏¥‡∏Å‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏•‡∏Ç 3826851              ‡∏ß‡∏¥‡∏ò‡∏µ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å \"‡∏£‡∏≠‡∏á‡∏û‡∏∑‡πâ‡∏ô\" ‡πÅ‡∏•‡∏∞ \"‡∏Ñ‡∏≠‡∏ô‡∏ã‡∏µ‡∏•‡πÄ‡∏•‡∏≠‡∏£‡πå\"   \n",
       "\n",
       "            tag                                          emotion  length  \\\n",
       "0  ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏™‡∏≥‡∏≠‡∏≤‡∏á  ‡∏ñ‡∏π‡∏Å‡πÉ‡∏à 0 ‡∏Ç‡∏≥‡∏Å‡∏•‡∏¥‡πâ‡∏á 0 ‡∏´‡∏•‡∏á‡∏£‡∏±‡∏Å 0 ‡∏ã‡∏∂‡πâ‡∏á 0 ‡∏™‡∏¢‡∏≠‡∏á 0 ‡∏ó‡∏∂‡πà‡∏á 0      36   \n",
       "1  ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏™‡∏≥‡∏≠‡∏≤‡∏á  ‡∏ñ‡∏π‡∏Å‡πÉ‡∏à 0 ‡∏Ç‡∏≥‡∏Å‡∏•‡∏¥‡πâ‡∏á 0 ‡∏´‡∏•‡∏á‡∏£‡∏±‡∏Å 0 ‡∏ã‡∏∂‡πâ‡∏á 0 ‡∏™‡∏¢‡∏≠‡∏á 0 ‡∏ó‡∏∂‡πà‡∏á 0      36   \n",
       "2  ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏™‡∏≥‡∏≠‡∏≤‡∏á  ‡∏ñ‡∏π‡∏Å‡πÉ‡∏à 0 ‡∏Ç‡∏≥‡∏Å‡∏•‡∏¥‡πâ‡∏á 0 ‡∏´‡∏•‡∏á‡∏£‡∏±‡∏Å 0 ‡∏ã‡∏∂‡πâ‡∏á 0 ‡∏™‡∏¢‡∏≠‡∏á 0 ‡∏ó‡∏∂‡πà‡∏á 0      36   \n",
       "3  ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏™‡∏≥‡∏≠‡∏≤‡∏á  ‡∏ñ‡∏π‡∏Å‡πÉ‡∏à 0 ‡∏Ç‡∏≥‡∏Å‡∏•‡∏¥‡πâ‡∏á 0 ‡∏´‡∏•‡∏á‡∏£‡∏±‡∏Å 0 ‡∏ã‡∏∂‡πâ‡∏á 0 ‡∏™‡∏¢‡∏≠‡∏á 0 ‡∏ó‡∏∂‡πà‡∏á 0      15   \n",
       "4  ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏™‡∏≥‡∏≠‡∏≤‡∏á  ‡∏ñ‡∏π‡∏Å‡πÉ‡∏à 0 ‡∏Ç‡∏≥‡∏Å‡∏•‡∏¥‡πâ‡∏á 0 ‡∏´‡∏•‡∏á‡∏£‡∏±‡∏Å 0 ‡∏ã‡∏∂‡πâ‡∏á 0 ‡∏™‡∏¢‡∏≠‡∏á 0 ‡∏ó‡∏∂‡πà‡∏á 0      11   \n",
       "5  ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏™‡∏≥‡∏≠‡∏≤‡∏á  ‡∏ñ‡∏π‡∏Å‡πÉ‡∏à 0 ‡∏Ç‡∏≥‡∏Å‡∏•‡∏¥‡πâ‡∏á 0 ‡∏´‡∏•‡∏á‡∏£‡∏±‡∏Å 0 ‡∏ã‡∏∂‡πâ‡∏á 0 ‡∏™‡∏¢‡∏≠‡∏á 0 ‡∏ó‡∏∂‡πà‡∏á 0      29   \n",
       "6  ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏™‡∏≥‡∏≠‡∏≤‡∏á  ‡∏ñ‡∏π‡∏Å‡πÉ‡∏à 0 ‡∏Ç‡∏≥‡∏Å‡∏•‡∏¥‡πâ‡∏á 0 ‡∏´‡∏•‡∏á‡∏£‡∏±‡∏Å 0 ‡∏ã‡∏∂‡πâ‡∏á 0 ‡∏™‡∏¢‡∏≠‡∏á 0 ‡∏ó‡∏∂‡πà‡∏á 0      23   \n",
       "7  ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏™‡∏≥‡∏≠‡∏≤‡∏á  ‡∏ñ‡∏π‡∏Å‡πÉ‡∏à 0 ‡∏Ç‡∏≥‡∏Å‡∏•‡∏¥‡πâ‡∏á 0 ‡∏´‡∏•‡∏á‡∏£‡∏±‡∏Å 0 ‡∏ã‡∏∂‡πâ‡∏á 0 ‡∏™‡∏¢‡∏≠‡∏á 0 ‡∏ó‡∏∂‡πà‡∏á 0      14   \n",
       "8  ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏™‡∏≥‡∏≠‡∏≤‡∏á  ‡∏ñ‡∏π‡∏Å‡πÉ‡∏à 0 ‡∏Ç‡∏≥‡∏Å‡∏•‡∏¥‡πâ‡∏á 0 ‡∏´‡∏•‡∏á‡∏£‡∏±‡∏Å 0 ‡∏ã‡∏∂‡πâ‡∏á 0 ‡∏™‡∏¢‡∏≠‡∏á 0 ‡∏ó‡∏∂‡πà‡∏á 0      14   \n",
       "9  ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏™‡∏≥‡∏≠‡∏≤‡∏á  ‡∏ñ‡∏π‡∏Å‡πÉ‡∏à 0 ‡∏Ç‡∏≥‡∏Å‡∏•‡∏¥‡πâ‡∏á 0 ‡∏´‡∏•‡∏á‡∏£‡∏±‡∏Å 0 ‡∏ã‡∏∂‡πâ‡∏á 0 ‡∏™‡∏¢‡∏≠‡∏á 0 ‡∏ó‡∏∂‡πà‡∏á 0      14   \n",
       "\n",
       "   num_sent  sent_length  label  label_1  label_2 vote  \\\n",
       "0         3           14      2        2        2  pos   \n",
       "1         3            8      2        2        2  pos   \n",
       "2         3           14      2        2        1  pos   \n",
       "3         2           13      1        3        3  neg   \n",
       "4         1           11      1        1        3  neu   \n",
       "5         1           29      2        2        2  pos   \n",
       "6         1           23      2        2        1  pos   \n",
       "7         2            9      1        2        2  pos   \n",
       "8         2            5      1        1        1  neu   \n",
       "9         1           14      2        2        2  pos   \n",
       "\n",
       "                                           processed  \n",
       "0  [cr, ‡πÅ‡∏õ, ‡∏±‡∏á‡∏û‡∏±‡∏ü, ‡∏Ñ‡∏∏‡∏°, ‡∏°‡∏±‡∏ô, ‡∏à‡∏±‡∏î, ‡πÄ‡∏ï‡πá‡∏°, ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠, ‡∏ö‡∏≤...  \n",
       "1               [‡πÑ‡∏°‡πà, ‡∏≠‡∏∏‡∏î‡∏ï‡∏±‡∏ô, ‡πÅ‡∏ï‡πà, ‡∏õ‡∏Å‡∏õ‡∏¥‡∏î, ‡πÅ‡∏ô‡πà‡∏ô, ‡∏°‡∏≤‡∏Å]  \n",
       "2  [‡∏£‡∏µ‡∏ß‡∏¥‡∏ß, ‡πÅ‡∏õ‡πâ‡∏á, lady, audrey, ready, all, day, ‡∏à‡πâ‡∏≤]  \n",
       "3  [‡∏Ç‡∏≠‡∏ö, ‡∏ï‡∏≤‡∏î‡∏≥, ‡∏°‡∏≤‡∏Å, ‡∏Ñ‡πà‡∏∞, ‡∏Ñ‡∏≠‡∏£‡πå, ‡πÄ‡∏•‡πá‡∏Ñ, ‡πÄ‡∏ï, ‡∏≠‡∏£, ‡∏Å‡πá, ...  \n",
       "4  [‡πÄ‡∏≠‡∏≤, aloe, vera, ‡πÅ‡∏ä‡πà, ‡∏ï‡∏π‡πâ‡πÄ‡∏¢‡πá‡∏ô, ‡∏à‡∏ô, ‡∏Å‡∏•‡∏≤‡∏¢‡πÄ‡∏õ‡πá‡∏ô, ...  \n",
       "5  [sr, ‡πÑ‡∏≠‡πÄ‡∏ó‡∏°, ‡πÄ‡∏ã, ‡∏£‡∏±‡πà‡∏°, ‡∏™‡∏¥‡∏ß, ‡∏•‡∏î, ‡∏™‡∏¥‡∏ß, ‡∏™‡∏¥‡∏ß, ‡∏≠‡∏∏‡∏î‡∏ï‡∏±...  \n",
       "6  [‡∏£‡∏ö‡∏Å‡∏ß‡∏ô, ‡∏™‡∏≤‡∏ß, ‡∏ä‡πà‡∏ß‡∏¢, ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥, ‡∏Å‡∏¥‡∏ô, ‡πÅ‡∏Ñ‡∏£‡πå, ‡∏ó‡∏µ‡πà, ‡∏ä‡πà‡∏ß‡∏¢...  \n",
       "7                [‡∏ó‡∏î‡∏•‡∏≠‡∏á‡πÉ‡∏ä‡πâ, ‡πÅ‡∏Ñ‡∏£‡∏≠‡∏ó, ‡∏ß‡∏¥, ‡∏ã‡∏µ, ‡∏´‡∏ô‡πâ‡∏≤, ‡πÉ‡∏™]  \n",
       "8                                      [‡πÉ‡∏ô, ‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå]  \n",
       "9    [‡∏ß‡∏¥‡∏ò‡∏µ, ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å, ‡∏£‡∏≠‡∏á‡∏û‡∏∑‡πâ‡∏ô, ‡πÅ‡∏•‡∏∞, ‡∏Ñ‡∏≠‡∏ô, ‡∏ã‡∏µ‡∏•, ‡πÄ‡∏•‡∏≠, ‡∏£‡πå\"]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_kt.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d84766f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>length</th>\n",
       "      <th>num_sent</th>\n",
       "      <th>sent_length</th>\n",
       "      <th>label</th>\n",
       "      <th>label_1</th>\n",
       "      <th>label_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6.008100e+04</td>\n",
       "      <td>60081.000000</td>\n",
       "      <td>60081.000000</td>\n",
       "      <td>60081.000000</td>\n",
       "      <td>60081.000000</td>\n",
       "      <td>60081.000000</td>\n",
       "      <td>60081.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.964936e+07</td>\n",
       "      <td>116.994574</td>\n",
       "      <td>8.502172</td>\n",
       "      <td>13.978329</td>\n",
       "      <td>1.577304</td>\n",
       "      <td>1.362644</td>\n",
       "      <td>1.662156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.559919e+05</td>\n",
       "      <td>118.647716</td>\n",
       "      <td>7.575442</td>\n",
       "      <td>12.083572</td>\n",
       "      <td>0.777527</td>\n",
       "      <td>0.639271</td>\n",
       "      <td>0.800034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.917283e+07</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.958755e+07</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.968929e+07</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.976947e+07</td>\n",
       "      <td>159.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.983970e+07</td>\n",
       "      <td>499.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>301.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            post_id        length      num_sent   sent_length         label  \\\n",
       "count  6.008100e+04  60081.000000  60081.000000  60081.000000  60081.000000   \n",
       "mean   3.964936e+07    116.994574      8.502172     13.978329      1.577304   \n",
       "std    1.559919e+05    118.647716      7.575442     12.083572      0.777527   \n",
       "min    3.917283e+07      3.000000      1.000000      3.000000      1.000000   \n",
       "25%    3.958755e+07     31.000000      3.000000      6.000000      1.000000   \n",
       "50%    3.968929e+07     72.000000      6.000000     10.000000      1.000000   \n",
       "75%    3.976947e+07    159.000000     11.000000     17.000000      2.000000   \n",
       "max    3.983970e+07    499.000000     44.000000    301.000000      3.000000   \n",
       "\n",
       "            label_1       label_2  \n",
       "count  60081.000000  60081.000000  \n",
       "mean       1.362644      1.662156  \n",
       "std        0.639271      0.800034  \n",
       "min        1.000000      1.000000  \n",
       "25%        1.000000      1.000000  \n",
       "50%        1.000000      1.000000  \n",
       "75%        2.000000      2.000000  \n",
       "max        3.000000      3.000000  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_kt.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5ec5386",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texts</th>\n",
       "      <th>targets</th>\n",
       "      <th>processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26727</th>\n",
       "      <td>‡∏û‡∏µ‡πà‡∏Ç‡∏ß‡∏±‡∏ç‡∏Ñ‡∏∞ ‡∏ñ‡πâ‡∏≤‡πÄ‡∏£‡∏≤‡∏°‡∏µ ‡∏ã‡∏∏‡∏•‡∏ß‡∏≤‡∏ã‡∏∏‡∏•‡πÄ‡∏ü‡∏¥‡∏£‡πå‡∏™‡πÅ‡∏Ñ‡∏£‡πå ‡∏Å‡∏±‡∏ö vich...</td>\n",
       "      <td>q</td>\n",
       "      <td>[‡∏û‡∏µ‡πà, ‡∏Ç‡∏ß‡∏±‡∏ç, ‡∏Ñ‡∏∞, ‡∏ñ‡πâ‡∏≤, ‡πÄ‡∏£‡∏≤, ‡∏°‡∏µ, ‡∏ã‡∏∏‡∏•, ‡∏ß‡∏≤, ‡∏ã‡∏∏‡∏•‡πÄ‡∏ü‡∏¥‡∏£...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26728</th>\n",
       "      <td>‡∏°‡∏µ‡∏™‡∏≤‡∏ß‡∏ú‡∏¥‡∏ß‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏á‡∏Ñ‡∏ô‡πÑ‡∏´‡∏ô‡∏•‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡∏£‡∏≠‡∏á‡∏û‡∏∑‡πâ‡∏ô‡∏£‡∏∏‡πà‡∏ô‡πÉ‡∏´‡∏°‡πà‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î...</td>\n",
       "      <td>q</td>\n",
       "      <td>[‡∏°‡∏µ, ‡∏™‡∏≤‡∏ß, ‡∏ú‡∏¥‡∏ß‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏á, ‡∏Ñ‡∏ô, ‡πÑ‡∏´‡∏ô, ‡∏•‡∏≠‡∏á, ‡πÉ‡∏ä‡πâ, ‡∏£‡∏≠‡∏á‡∏û‡∏∑‡πâ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26729</th>\n",
       "      <td>‡πÅ‡∏≠‡∏î‡∏°‡∏¥‡∏ô‡∏Ñ‡∏∞ ‡∏ó‡∏≥‡πÑ‡∏° True Money Wallet ‡∏´‡∏±‡∏Å‡πÄ‡∏á‡∏¥‡∏ô‡∏Ñ‡∏á‡πÄ‡∏´‡∏•‡∏∑‡∏≠...</td>\n",
       "      <td>q</td>\n",
       "      <td>[‡πÅ‡∏≠‡∏î‡∏°‡∏¥‡∏ô, ‡∏Ñ‡∏∞, ‡∏ó‡∏≥‡πÑ‡∏°, true, money, wallet, ‡∏´‡∏±‡∏Å, ‡πÄ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26730</th>\n",
       "      <td>‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏•‡∏¥‡∏õ‡∏Ç‡∏≠‡∏á cute press ‡∏™‡∏µ‡∏™‡πâ‡∏°‡∏≠‡∏¥‡∏ê‡∏´‡∏ô‡πà‡∏≠‡∏¢‡∏Ñ‡∏£‡∏±‡∏ö ‡∏≠‡∏±‡∏ô‡πÑ‡∏´...</td>\n",
       "      <td>q</td>\n",
       "      <td>[‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥, ‡∏•‡∏¥‡∏õ, ‡∏Ç‡∏≠‡∏á, cute, press, ‡∏™‡∏µ‡∏™‡πâ‡∏°, ‡∏≠‡∏¥‡∏ê, ‡∏´‡∏ô‡πà...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26731</th>\n",
       "      <td>‡πÄ‡∏ß‡∏•‡∏≤‡∏ß‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß 80 ‡∏Ç‡∏∂‡πâ‡∏ô‡πÑ‡∏õ‡∏û‡∏ß‡∏á‡∏°‡∏≤‡∏•‡∏±‡∏¢‡∏à‡∏∞‡∏™‡∏±‡πà‡∏ô‡∏Ñ‡∏£‡∏±...</td>\n",
       "      <td>q</td>\n",
       "      <td>[‡πÄ‡∏ß‡∏•‡∏≤, ‡∏ß‡∏¥‡πà‡∏á, ‡∏ó‡∏µ‡πà, ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß, 80, ‡∏Ç‡∏∂‡πâ‡∏ô‡πÑ‡∏õ, ‡∏û‡∏ß‡∏á‡∏°‡∏≤‡∏•...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26732</th>\n",
       "      <td>‡∏ß‡∏¥‡∏ò‡∏µ‡∏ã‡∏∑‡πâ‡∏≠‡∏£‡∏ñ‡∏°‡∏∑‡∏≠‡∏™‡∏≠‡∏á‡∏ï‡πâ‡∏≠‡∏á‡∏î‡∏π‡∏¢‡∏±‡∏á‡πÑ‡∏á‡∏î‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö‡∏ï‡∏≠‡∏ô‡∏ô‡∏±‡πâ‡∏ô‡∏û‡∏µ‡∏à‡∏¥‡∏°‡πÅ...</td>\n",
       "      <td>q</td>\n",
       "      <td>[‡∏ß‡∏¥‡∏ò‡∏µ, ‡∏ã‡∏∑‡πâ‡∏≠, ‡∏£‡∏ñ, ‡∏°‡∏∑‡∏≠‡∏™‡∏≠‡∏á, ‡∏ï‡πâ‡∏≠‡∏á, ‡∏î‡∏π, ‡∏¢‡∏±‡∏á‡πÑ‡∏á, ‡∏î‡∏µ, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26733</th>\n",
       "      <td>- ‡∏ô‡πâ‡∏≥‡∏°‡∏±‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á ‡∏ó‡∏±‡πâ‡∏á Fully / Semi ‡πÄ‡∏ö‡∏ô‡∏ã‡∏¥‡∏ô - ‡∏ô‡πâ‡∏≥...</td>\n",
       "      <td>q</td>\n",
       "      <td>[‡∏ô‡πâ‡∏≥‡∏°‡∏±‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á, ‡∏ó‡∏±‡πâ‡∏á, fully, semi, ‡πÄ‡∏ö‡∏ô‡∏ã‡∏¥‡∏ô, ‡∏ô‡πâ‡∏≥...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26734</th>\n",
       "      <td>Honda Civic Hatchback ‡∏ó‡∏µ‡πà‡∏°‡∏≤‡∏Å‡∏±‡∏ö‡∏™‡∏µ‡πÅ‡∏î‡∏á ‡∏£‡∏≤‡∏Ñ‡∏≤‡πÄ‡∏ó‡πà‡∏≤‡πÄ‡∏î...</td>\n",
       "      <td>q</td>\n",
       "      <td>[honda, civic, hatchback, ‡∏ó‡∏µ‡πà‡∏°‡∏≤, ‡∏Å‡∏±‡∏ö, ‡∏™‡∏µ‡πÅ‡∏î‡∏á, ‡∏£...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26735</th>\n",
       "      <td>‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏à‡∏∞‡πÄ‡∏£‡∏¥‡πà‡∏° skincare 2 ‡∏ï‡∏±‡∏ß‡∏Ñ‡∏∑‡∏≠ stemfactor ‡∏Å‡∏±‡∏ö ...</td>\n",
       "      <td>q</td>\n",
       "      <td>[‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏à‡∏∞, ‡πÄ‡∏£‡∏¥‡πà‡∏°, skincare, ‡∏ï‡∏±‡∏ß, ‡∏Ñ‡∏∑‡∏≠, stemfacto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26736</th>\n",
       "      <td>‡∏û‡∏µ‡πà‡∏Ñ‡∏∞ ‡∏´‡∏ô‡∏π‡∏≠‡∏¢‡∏≤‡∏Å‡∏™‡∏≠‡∏ö‡∏ñ‡∏≤‡∏°‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ô‡∏∂‡∏á‡∏Ñ‡πà‡∏∞ ‡∏≠‡∏≤‡∏à‡∏∞‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å...</td>\n",
       "      <td>q</td>\n",
       "      <td>[‡∏û‡∏µ‡πà, ‡∏Ñ‡∏∞, ‡∏´‡∏ô‡∏π, ‡∏≠‡∏¢‡∏≤‡∏Å, ‡∏™‡∏≠‡∏ö‡∏ñ‡∏≤‡∏°, ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á, ‡∏ô‡∏∂‡∏á, ‡∏Ñ‡πà‡∏∞,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   texts targets  \\\n",
       "26727  ‡∏û‡∏µ‡πà‡∏Ç‡∏ß‡∏±‡∏ç‡∏Ñ‡∏∞ ‡∏ñ‡πâ‡∏≤‡πÄ‡∏£‡∏≤‡∏°‡∏µ ‡∏ã‡∏∏‡∏•‡∏ß‡∏≤‡∏ã‡∏∏‡∏•‡πÄ‡∏ü‡∏¥‡∏£‡πå‡∏™‡πÅ‡∏Ñ‡∏£‡πå ‡∏Å‡∏±‡∏ö vich...       q   \n",
       "26728  ‡∏°‡∏µ‡∏™‡∏≤‡∏ß‡∏ú‡∏¥‡∏ß‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏á‡∏Ñ‡∏ô‡πÑ‡∏´‡∏ô‡∏•‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡∏£‡∏≠‡∏á‡∏û‡∏∑‡πâ‡∏ô‡∏£‡∏∏‡πà‡∏ô‡πÉ‡∏´‡∏°‡πà‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î...       q   \n",
       "26729  ‡πÅ‡∏≠‡∏î‡∏°‡∏¥‡∏ô‡∏Ñ‡∏∞ ‡∏ó‡∏≥‡πÑ‡∏° True Money Wallet ‡∏´‡∏±‡∏Å‡πÄ‡∏á‡∏¥‡∏ô‡∏Ñ‡∏á‡πÄ‡∏´‡∏•‡∏∑‡∏≠...       q   \n",
       "26730  ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏•‡∏¥‡∏õ‡∏Ç‡∏≠‡∏á cute press ‡∏™‡∏µ‡∏™‡πâ‡∏°‡∏≠‡∏¥‡∏ê‡∏´‡∏ô‡πà‡∏≠‡∏¢‡∏Ñ‡∏£‡∏±‡∏ö ‡∏≠‡∏±‡∏ô‡πÑ‡∏´...       q   \n",
       "26731  ‡πÄ‡∏ß‡∏•‡∏≤‡∏ß‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß 80 ‡∏Ç‡∏∂‡πâ‡∏ô‡πÑ‡∏õ‡∏û‡∏ß‡∏á‡∏°‡∏≤‡∏•‡∏±‡∏¢‡∏à‡∏∞‡∏™‡∏±‡πà‡∏ô‡∏Ñ‡∏£‡∏±...       q   \n",
       "26732  ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ã‡∏∑‡πâ‡∏≠‡∏£‡∏ñ‡∏°‡∏∑‡∏≠‡∏™‡∏≠‡∏á‡∏ï‡πâ‡∏≠‡∏á‡∏î‡∏π‡∏¢‡∏±‡∏á‡πÑ‡∏á‡∏î‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö‡∏ï‡∏≠‡∏ô‡∏ô‡∏±‡πâ‡∏ô‡∏û‡∏µ‡∏à‡∏¥‡∏°‡πÅ...       q   \n",
       "26733  - ‡∏ô‡πâ‡∏≥‡∏°‡∏±‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á ‡∏ó‡∏±‡πâ‡∏á Fully / Semi ‡πÄ‡∏ö‡∏ô‡∏ã‡∏¥‡∏ô - ‡∏ô‡πâ‡∏≥...       q   \n",
       "26734  Honda Civic Hatchback ‡∏ó‡∏µ‡πà‡∏°‡∏≤‡∏Å‡∏±‡∏ö‡∏™‡∏µ‡πÅ‡∏î‡∏á ‡∏£‡∏≤‡∏Ñ‡∏≤‡πÄ‡∏ó‡πà‡∏≤‡πÄ‡∏î...       q   \n",
       "26735  ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏à‡∏∞‡πÄ‡∏£‡∏¥‡πà‡∏° skincare 2 ‡∏ï‡∏±‡∏ß‡∏Ñ‡∏∑‡∏≠ stemfactor ‡∏Å‡∏±‡∏ö ...       q   \n",
       "26736  ‡∏û‡∏µ‡πà‡∏Ñ‡∏∞ ‡∏´‡∏ô‡∏π‡∏≠‡∏¢‡∏≤‡∏Å‡∏™‡∏≠‡∏ö‡∏ñ‡∏≤‡∏°‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ô‡∏∂‡∏á‡∏Ñ‡πà‡∏∞ ‡∏≠‡∏≤‡∏à‡∏∞‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å...       q   \n",
       "\n",
       "                                               processed  \n",
       "26727  [‡∏û‡∏µ‡πà, ‡∏Ç‡∏ß‡∏±‡∏ç, ‡∏Ñ‡∏∞, ‡∏ñ‡πâ‡∏≤, ‡πÄ‡∏£‡∏≤, ‡∏°‡∏µ, ‡∏ã‡∏∏‡∏•, ‡∏ß‡∏≤, ‡∏ã‡∏∏‡∏•‡πÄ‡∏ü‡∏¥‡∏£...  \n",
       "26728  [‡∏°‡∏µ, ‡∏™‡∏≤‡∏ß, ‡∏ú‡∏¥‡∏ß‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏á, ‡∏Ñ‡∏ô, ‡πÑ‡∏´‡∏ô, ‡∏•‡∏≠‡∏á, ‡πÉ‡∏ä‡πâ, ‡∏£‡∏≠‡∏á‡∏û‡∏∑‡πâ...  \n",
       "26729  [‡πÅ‡∏≠‡∏î‡∏°‡∏¥‡∏ô, ‡∏Ñ‡∏∞, ‡∏ó‡∏≥‡πÑ‡∏°, true, money, wallet, ‡∏´‡∏±‡∏Å, ‡πÄ...  \n",
       "26730  [‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥, ‡∏•‡∏¥‡∏õ, ‡∏Ç‡∏≠‡∏á, cute, press, ‡∏™‡∏µ‡∏™‡πâ‡∏°, ‡∏≠‡∏¥‡∏ê, ‡∏´‡∏ô‡πà...  \n",
       "26731  [‡πÄ‡∏ß‡∏•‡∏≤, ‡∏ß‡∏¥‡πà‡∏á, ‡∏ó‡∏µ‡πà, ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß, 80, ‡∏Ç‡∏∂‡πâ‡∏ô‡πÑ‡∏õ, ‡∏û‡∏ß‡∏á‡∏°‡∏≤‡∏•...  \n",
       "26732  [‡∏ß‡∏¥‡∏ò‡∏µ, ‡∏ã‡∏∑‡πâ‡∏≠, ‡∏£‡∏ñ, ‡∏°‡∏∑‡∏≠‡∏™‡∏≠‡∏á, ‡∏ï‡πâ‡∏≠‡∏á, ‡∏î‡∏π, ‡∏¢‡∏±‡∏á‡πÑ‡∏á, ‡∏î‡∏µ, ...  \n",
       "26733  [‡∏ô‡πâ‡∏≥‡∏°‡∏±‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á, ‡∏ó‡∏±‡πâ‡∏á, fully, semi, ‡πÄ‡∏ö‡∏ô‡∏ã‡∏¥‡∏ô, ‡∏ô‡πâ‡∏≥...  \n",
       "26734  [honda, civic, hatchback, ‡∏ó‡∏µ‡πà‡∏°‡∏≤, ‡∏Å‡∏±‡∏ö, ‡∏™‡∏µ‡πÅ‡∏î‡∏á, ‡∏£...  \n",
       "26735  [‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏à‡∏∞, ‡πÄ‡∏£‡∏¥‡πà‡∏°, skincare, ‡∏ï‡∏±‡∏ß, ‡∏Ñ‡∏∑‡∏≠, stemfacto...  \n",
       "26736  [‡∏û‡∏µ‡πà, ‡∏Ñ‡∏∞, ‡∏´‡∏ô‡∏π, ‡∏≠‡∏¢‡∏≤‡∏Å, ‡∏™‡∏≠‡∏ö‡∏ñ‡∏≤‡∏°, ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á, ‡∏ô‡∏∂‡∏á, ‡∏Ñ‡πà‡∏∞,...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ws.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7077f068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texts</th>\n",
       "      <th>targets</th>\n",
       "      <th>processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>26737</td>\n",
       "      <td>26737</td>\n",
       "      <td>26737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>26713</td>\n",
       "      <td>4</td>\n",
       "      <td>26209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>‡∏≠‡∏∏‡∏î‡∏£‡∏°‡∏µ‡πÑ‡∏´‡∏°‡∏Ñ‡πà‡∏∞</td>\n",
       "      <td>neu</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>2</td>\n",
       "      <td>14561</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               texts targets processed\n",
       "count          26737   26737     26737\n",
       "unique         26713       4     26209\n",
       "top     ‡∏≠‡∏∏‡∏î‡∏£‡∏°‡∏µ‡πÑ‡∏´‡∏°‡∏Ñ‡πà‡∏∞     neu        []\n",
       "freq               2   14561        56"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ws.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c7d3bcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neu    0.633378\n",
       "pos    0.205822\n",
       "neg    0.160800\n",
       "Name: vote, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# class distribution\n",
    "df_kt.vote.value_counts() / df_kt.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae548ccd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neu    0.544601\n",
       "neg    0.255189\n",
       "pos    0.178704\n",
       "q      0.021506\n",
       "Name: targets, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# class distribution\n",
    "df_ws.targets.value_counts() / df_ws.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c6e20f",
   "metadata": {},
   "source": [
    "## Bag of words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2eb3e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# BOW with unigram and bigrams\n",
    "bow1 = CountVectorizer(tokenizer=process_text, ngram_range=(1, 1), min_df=5)\n",
    "bow2 = CountVectorizer(tokenizer=process_text, ngram_range=(2, 2), min_df=5)\n",
    "\n",
    "# fit kt and transform to both datasets\n",
    "bow1_fit_kt = bow1.fit(df_kt['text'].apply(str))\n",
    "text_bow1_kt = bow1_fit_kt.transform(df_kt['text'].apply(str))\n",
    "text_bow1_ws = bow1_fit_kt.transform(df_ws['texts'].apply(str))\n",
    "\n",
    "bow2_fit_kt = bow2.fit(df_kt['text'].apply(str))\n",
    "text_bow2_kt = bow2_fit_kt.transform(df_kt['text'].apply(str))\n",
    "text_bow2_ws = bow2_fit_kt.transform(df_ws['texts'].apply(str))\n",
    "\n",
    "print(text_bow1_kt.toarray().shape,  text_bow1_kt.toarray().shape)\n",
    "print(text_bow2_kt.toarray().shape,  text_bow2_kt.toarray().shape)\n",
    "\n",
    "print(text_bow1_ws.toarray().shape,  text_bow1_ws.toarray().shape)\n",
    "print(text_bow2_ws.toarray().shape,  text_bow2_ws.toarray().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566e61aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize texts\n",
    "from visualize import top_feats_all, plot_top_feats\n",
    "features = bow1_fit_kt.get_feature_names()\n",
    "%time ts = top_feats_all(text_bow1_kt.toarray(), y_kt, features)\n",
    "print(ts[0].shape)\n",
    "ts[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8410282",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time plot_top_feats(ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d7b552",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3232d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF with unigram and bigrams\n",
    "tfidf1 = TfidfVectorizer(tokenizer=process_text, ngram_range=(1, 1), min_df=5)\n",
    "tfidf2 = TfidfVectorizer(tokenizer=process_text, ngram_range=(2, 2), min_df=5)\n",
    "\n",
    "# fit kt and transform to both datasets\n",
    "tfidf1_fit_kt = tfidf1.fit(df_kt['text'].apply(str))\n",
    "text_tfidf1_kt = tfidf1_fit_kt.transform(df_kt['text'].apply(str))\n",
    "text_tfidf1_ws = tfidf1_fit_kt.transform(df_ws['texts'].apply(str))\n",
    "\n",
    "tfidf2_fit_kt = tfidf2.fit(df_kt['text'].apply(str))\n",
    "text_tfidf2_kt = tfidf2_fit_kt.transform(df_kt['text'].apply(str))\n",
    "text_tfidf2_ws = tfidf2_fit_kt.transform(df_ws['texts'].apply(str))\n",
    "\n",
    "print(text_tfidf1_kt.toarray().shape,  text_tfidf1_kt.toarray().shape)\n",
    "print(text_tfidf2_kt.toarray().shape,  text_tfidf2_kt.toarray().shape)\n",
    "\n",
    "print(text_tfidf1_ws.toarray().shape,  text_tfidf1_ws.toarray().shape)\n",
    "print(text_tfidf2_ws.toarray().shape,  text_tfidf2_ws.toarray().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2867680",
   "metadata": {},
   "source": [
    "## Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8551fe6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "\n",
    "class TfidfEmbeddingVectorizer(object):\n",
    "    def __init__(self, model):\n",
    "        w2v = {w: vec for w, vec in zip(model.wv.index_to_key, model.wv.vectors)}\n",
    "        self.word2vec = w2v\n",
    "        self.word2weight = None\n",
    "        self.dim = model.vector_size\n",
    "    \n",
    "    def fit(self, X):\n",
    "        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "        tfidf.fit(X)\n",
    "        max_idf = max(tfidf.idf_)\n",
    "        self.word2weight = defaultdict(\n",
    "            lambda: max_idf,\n",
    "            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "                np.mean([self.word2vec[w] * self.word2weight[w]\n",
    "                         for w in words if w in self.word2vec] or\n",
    "                        [np.zeros(self.dim)], axis=0)\n",
    "                for words in X\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856dd1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from pythainlp import word_vector\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "\n",
    "# create word2vec for kt corpus\n",
    "w2v_kt = Word2Vec(vector_size=300, min_count=1, window=4, workers=4)\n",
    "w2v_kt.build_vocab(df_kt['processed'])\n",
    "w2v_kt.train(df_kt['processed'], total_examples=w2v_kt.corpus_count, epochs=100)\n",
    "w2v_kt.wv.most_similar(\"‡∏ö‡∏∞‡∏´‡∏°‡∏µ‡πà\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542624be",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_tfidf_emb_kt = TfidfEmbeddingVectorizer(w2v_kt)\n",
    "w2v_tifdf_fit_kt = w2v_tfidf_emb_kt.fit(df_kt['text'].apply(str))\n",
    "\n",
    "# transfrom on both corpuses\n",
    "text_w2v_tfidf_kt = w2v_tifdf_fit_kt.transform(df_kt['text'].apply(str))\n",
    "text_w2v_tfidf_ws = w2v_tifdf_fit_kt.transform(df_ws['texts'].apply(str))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2102229e",
   "metadata": {},
   "source": [
    "## POS Tagging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0387c5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of exisiting pos-tagging techniques.  \n",
    "# TODO: 1. work with emoji and pad\n",
    "def get_tag(text):\n",
    "    res = [el[1] for el in text]\n",
    "    return res\n",
    "\n",
    "# concate word and tag with underscore (‡∏°‡∏±‡∏ô_ADV)\n",
    "def word_tag(pos):\n",
    "    tag_list = []\n",
    "    for item in pos:\n",
    "        tag = ['_'.join(map(str, tups)) for tups in item]\n",
    "        tag_list.append(' '.join(tag))\n",
    "    return tag_list\n",
    "\n",
    "# use only tag\n",
    "def tag(pos):\n",
    "    tag_list = []\n",
    "    for item in pos:\n",
    "        tmp = get_tag(item)\n",
    "        tag = ' '.join(map(str, tmp))\n",
    "        tag_list.append(tag)\n",
    "    return tag_list\n",
    "\n",
    "# this approach convert nest-list to simply list of word follows by tag ['word', 'NOUN']\n",
    "def flatten(text):\n",
    "    res = list(sum(text, ()))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c257e9d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      processed  \\\n",
      "1000   [‡πÑ‡∏Å‡πà, ‡πÄ‡∏Ñ‡πá‡∏°, ‡πÑ‡∏õ, ‡∏ô‡∏∞, ‡∏Ñ‡∏£‡∏±‡∏ö, ‡πÑ‡∏°‡πà, ‡∏ä‡∏≠‡∏ö, ‡πÄ‡∏•‡∏¢]   \n",
      "1001      [‡πÑ‡∏Å‡πà, ‡πÄ‡∏Ñ‡πÄ‡∏≠‡∏ü‡∏ã‡∏µ, ‡∏´‡∏£‡πà‡∏≠‡∏¢, ‡∏Å‡∏ß‡πà‡∏≤, ‡πÅ‡∏°‡∏Ñ, ‡∏à‡∏∑‡∏î]   \n",
      "1002      [‡∏Ç‡∏≠‡∏á, ‡∏ô‡πâ‡∏≠‡∏¢, ‡∏°‡∏≤‡∏Å, ‡∏ä‡∏π, ‡∏ä‡∏¥, ‡∏≠‡πà‡∏∞, ‡πÇ‡∏Æ‡∏°‡πÇ‡∏õ‡∏£]   \n",
      "1003  [‡∏Ç‡∏≠, ‡πÉ‡∏ö, ‡∏≠‡∏ô‡∏∏, ‡∏ç‡∏≤‡∏ï‡∏¥, ‡πÄ‡∏õ‡πá‡∏ô, ‡∏ô‡∏±‡∏Å, ‡∏õ‡∏±‡πà‡∏ô, ‡∏•‡∏ß‡∏î]   \n",
      "1004         [‡∏Ç‡∏≠‡∏£‡πâ‡∏≠‡∏á, ‡∏•‡∏∞, ‡∏ï‡∏±‡∏ß‡∏´‡∏ô‡∏±‡∏á‡∏™‡∏∑‡∏≠, ‡∏ñ‡∏µ‡πà, ‡∏à‡∏±‡∏á]   \n",
      "1005      [‡∏Ç‡∏∏‡∏î, ‡∏ó‡∏≤‡∏á‡πÄ‡∏ó‡πâ‡∏≤, ‡∏ï‡∏≠‡∏ô, ‡∏´‡πâ‡∏≤, ‡∏Ñ‡∏£‡∏∂‡πà‡∏á, ‡∏≠‡∏∞‡∏ô‡∏∞]   \n",
      "1006     [‡∏Ñ‡∏ß‡∏¢, ‡∏ß‡∏±‡∏ô‡∏ô‡∏±‡πâ‡∏ô, ‡∏à‡∏∞, ‡∏û‡∏≤, ‡πÉ‡∏õ, ‡πÅ‡∏î‡∏Å, ‡πÉ‡∏°‡πà‡πÉ‡∏õ]   \n",
      "1007       [‡∏Ñ‡∏≠‡∏°, ‡πÄ‡∏û‡∏•‡πá‡∏Å, ‡∏°‡∏±‡∏ô, ‡πÑ‡∏°‡πà, ‡∏°‡∏µ, ‡∏Å‡∏£‡∏∞‡∏ó‡∏∞‡∏ó‡∏≠‡∏á]   \n",
      "1008   [‡∏Ñ‡πà‡∏≤, ‡∏ä‡∏∏‡∏î, ‡∏Å‡πá, ‡πÄ‡∏Å‡∏¥‡∏ô, ‡∏Ñ‡πà‡∏≤, ‡∏Å‡∏¥‡∏ô, ‡∏•‡∏∞, ‡∏°‡∏±‡πâ‡∏á]   \n",
      "1009         [‡∏Ñ‡πà‡∏≤, ‡∏ä‡∏∏‡∏î, ‡πÑ‡∏ó‡∏¢, ‡πÅ‡∏û‡∏á, ‡∏Å‡∏ß‡πà‡∏≤, ‡∏™‡πà‡∏ß‡∏ô‡∏•‡∏î]   \n",
      "\n",
      "                                    post_tag1  \n",
      "1000    NOUN VERB AUX VERB NOUN PART VERB ADV  \n",
      "1001            NOUN NOUN ADJ SCONJ NOUN NOUN  \n",
      "1002          ADP ADV ADV NOUN NOUN NOUN NOUN  \n",
      "1003  VERB VERB NOUN NOUN VERB NOUN NOUN NOUN  \n",
      "1004                    NOUN ADP NOUN ADJ ADV  \n",
      "1005               NOUN NOUN NOUN NUM NUM ADV  \n",
      "1006         NOUN NOUN AUX VERB ADP NOUN NOUN  \n",
      "1007            NOUN NOUN PRON PART VERB NOUN  \n",
      "1008   NOUN NOUN SCONJ VERB NOUN VERB ADP ADV  \n",
      "1009            NOUN NOUN NOUN ADJ SCONJ NOUN  \n",
      "\n",
      "\n",
      "1000    ‡πÑ‡∏Å‡πà_NOUN ‡πÄ‡∏Ñ‡πá‡∏°_VERB ‡πÑ‡∏õ_AUX ‡∏ô‡∏∞_VERB ‡∏Ñ‡∏£‡∏±‡∏ö_NOUN ‡πÑ‡∏°...\n",
      "1001    ‡πÑ‡∏Å‡πà_NOUN ‡πÄ‡∏Ñ‡πÄ‡∏≠‡∏ü‡∏ã‡∏µ_NOUN ‡∏´‡∏£‡πà‡∏≠‡∏¢_ADJ ‡∏Å‡∏ß‡πà‡∏≤_SCONJ ‡πÅ‡∏°‡∏Ñ...\n",
      "1002    ‡∏Ç‡∏≠‡∏á_ADP ‡∏ô‡πâ‡∏≠‡∏¢_ADV ‡∏°‡∏≤‡∏Å_ADV ‡∏ä‡∏π_NOUN ‡∏ä‡∏¥_NOUN ‡∏≠‡πà‡∏∞_N...\n",
      "1003    ‡∏Ç‡∏≠_VERB ‡πÉ‡∏ö_VERB ‡∏≠‡∏ô‡∏∏_NOUN ‡∏ç‡∏≤‡∏ï‡∏¥_NOUN ‡πÄ‡∏õ‡πá‡∏ô_VERB ‡∏ô...\n",
      "1004    ‡∏Ç‡∏≠‡∏£‡πâ‡∏≠‡∏á_NOUN ‡∏•‡∏∞_ADP ‡∏ï‡∏±‡∏ß‡∏´‡∏ô‡∏±‡∏á‡∏™‡∏∑‡∏≠_NOUN ‡∏ñ‡∏µ‡πà_ADJ ‡∏à‡∏±‡∏á...\n",
      "1005    ‡∏Ç‡∏∏‡∏î_NOUN ‡∏ó‡∏≤‡∏á‡πÄ‡∏ó‡πâ‡∏≤_NOUN ‡∏ï‡∏≠‡∏ô_NOUN ‡∏´‡πâ‡∏≤_NUM ‡∏Ñ‡∏£‡∏∂‡πà‡∏á_N...\n",
      "1006    ‡∏Ñ‡∏ß‡∏¢_NOUN ‡∏ß‡∏±‡∏ô‡∏ô‡∏±‡πâ‡∏ô_NOUN ‡∏à‡∏∞_AUX ‡∏û‡∏≤_VERB ‡πÉ‡∏õ_ADP ‡πÅ‡∏î...\n",
      "1007    ‡∏Ñ‡∏≠‡∏°_NOUN ‡πÄ‡∏û‡∏•‡πá‡∏Å_NOUN ‡∏°‡∏±‡∏ô_PRON ‡πÑ‡∏°‡πà_PART ‡∏°‡∏µ_VERB ...\n",
      "1008    ‡∏Ñ‡πà‡∏≤_NOUN ‡∏ä‡∏∏‡∏î_NOUN ‡∏Å‡πá_SCONJ ‡πÄ‡∏Å‡∏¥‡∏ô_VERB ‡∏Ñ‡πà‡∏≤_NOUN ...\n",
      "1009    ‡∏Ñ‡πà‡∏≤_NOUN ‡∏ä‡∏∏‡∏î_NOUN ‡πÑ‡∏ó‡∏¢_NOUN ‡πÅ‡∏û‡∏á_ADJ ‡∏Å‡∏ß‡πà‡∏≤_SCONJ ...\n",
      "Name: post_tag2, dtype: object\n",
      "CPU times: total: 45.1 s\n",
      "Wall time: 45.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pythainlp.tag import pos_tag_sents\n",
    "\n",
    "pos = pos_tag_sents(df_kt['processed'].tolist(), corpus='orchid_ud')\n",
    "df_kt['post_tag1'] = pd.DataFrame(tag(pos))\n",
    "df_kt['post_tag2'] = pd.DataFrame(word_tag(pos))\n",
    "\n",
    "pos = pos_tag_sents(df_ws['processed'].tolist(), corpus='orchid_ud')\n",
    "df_ws['post_tag1'] = pd.DataFrame(tag(pos))\n",
    "df_ws['post_tag2'] = pd.DataFrame(word_tag(pos))\n",
    "\n",
    "print(df_ws[['processed', 'post_tag1']].iloc[1000:1010])\n",
    "print(\"\\n\")\n",
    "print(df_ws['post_tag2'].iloc[1000:1010])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f3755ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'post_tag'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3621\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3620\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'post_tag'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CountVectorizer\n\u001b[0;32m      3\u001b[0m bow1 \u001b[38;5;241m=\u001b[39m CountVectorizer()\n\u001b[1;32m----> 5\u001b[0m text_pos_bow1_fit_kt \u001b[38;5;241m=\u001b[39m bow1\u001b[38;5;241m.\u001b[39mfit(\u001b[43mdf_kt\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpost_tag\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m      6\u001b[0m text_pos_bow1_kt \u001b[38;5;241m=\u001b[39m text_pos_bow1_fit_kt\u001b[38;5;241m.\u001b[39mtransform(df_kt[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpost_tag\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      7\u001b[0m text_pos_bow1_ws \u001b[38;5;241m=\u001b[39m text_pos_bow1_fit_kt\u001b[38;5;241m.\u001b[39mtransform(df_ws[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpost_tag\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3505\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3504\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3505\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3507\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3623\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3623\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3624\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3625\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3626\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3627\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3628\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'post_tag'"
     ]
    }
   ],
   "source": [
    "# create bow vectors\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "bow1 = CountVectorizer()\n",
    "\n",
    "text_pos_bow1_fit_kt = bow1.fit(df_kt['post_tag'])\n",
    "text_pos_bow1_kt = text_pos_bow1_fit_kt.transform(df_kt['post_tag'])\n",
    "text_pos_bow1_ws = text_pos_bow1_fit_kt.transform(df_ws['post_tag'])\n",
    "\n",
    "# text_pos_bow2_fit_kt = bow2.fit(df_kt['POSTags'].apply(str))\n",
    "# text_pos_bow2_kt = text_pos_bow2_fit_kt.transform(df_kt['POSTags'].apply(str))\n",
    "# text_pos_bow2_ws = text_pos_bow2_fit_kt.transform(df_ws['POSTags'].apply(str))\n",
    "\n",
    "print(text_pos_bow1_kt.toarray().shape,  text_pos_bow1_kt.toarray().shape)\n",
    "print(text_pos_bow1_ws.toarray().shape,  text_pos_bow1_ws.toarray().shape)\n",
    "\n",
    "# print(text_pos_bow2_kt.toarray().shape,  text_pos_bow2_kt.toarray().shape)\n",
    "# print(text_pos_bow2_ws.toarray().shape,  text_pos_bow2_ws.toarray().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568848ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tfidf vectors\n",
    "text_pos_tfidf1_fit_kt = tfidf1.fit(df_kt['POSTags'].apply(str))\n",
    "text_pos_tfidf1_kt = text_pos_tfidf1_fit_kt.transform(df_kt['POSTags'].apply(str))\n",
    "text_pos_tfidf1_ws = text_pos_tfidf1_fit_kt.transform(df_ws['POSTags'].apply(str))\n",
    "\n",
    "text_pos_tfidf2_fit_kt = tfidf2.fit(df_kt['POSTags'].apply(str))\n",
    "text_pos_tfidf2_kt = text_pos_tfidf2_fit_kt.transform(df_kt['POSTags'].apply(str))\n",
    "text_pos_tfidf2_ws = text_pos_tfidf2_fit_kt.transform(df_ws['POSTags'].apply(str))\n",
    "\n",
    "print(text_pos_tfidf1_kt.toarray().shape,  text_pos_tfidf1_kt.toarray().shape)\n",
    "print(text_pos_tfidf1_ws.toarray().shape,  text_pos_tfidf1_ws.toarray().shape)\n",
    "\n",
    "print(text_pos_tfidf2_kt.toarray().shape,  text_pos_tfidf2_kt.toarray().shape)\n",
    "print(text_pos_tfidf2_ws.toarray().shape,  text_pos_tfidf2_ws.toarray().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c6949e",
   "metadata": {},
   "source": [
    "## Dictionary-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e922a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load list of our custom positive and negative words\n",
    "with open(os.path.dirname(os.getcwd()) + '\\\\data\\\\' + 'pos_words.txt', encoding='UTF-8') as f:\n",
    "    pos_words = [line.rstrip('\\n') for line in f]\n",
    "\n",
    "with open(os.path.dirname(os.getcwd()) + '\\\\data\\\\' + 'neg_words.txt', encoding='UTF-8') as f:\n",
    "    neg_words = [line.rstrip('\\n') for line in f]\n",
    "pos_words = list(set(pos_words))\n",
    "neg_words = list(set(neg_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad848e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow1 = CountVectorizer(tokenizer=process_text, ngram_range=(1, 1))\n",
    "bow2 = CountVectorizer(tokenizer=process_text, ngram_range=(2, 2))\n",
    "\n",
    "my_vocabs = pos_words + neg_words\n",
    "print('dict size: ', len(my_vocabs))\n",
    "\n",
    "text_dict_bow1_fit = bow1.fit(my_vocabs)\n",
    "text_dict_bow1_kt = text_dict_bow1_fit.transform(df_kt['text'].apply(str))\n",
    "text_dict_bow1_ws = text_dict_bow1_fit.transform(df_ws['texts'].apply(str))\n",
    "\n",
    "text_dict_bow2_fit = bow2.fit(my_vocabs)\n",
    "text_dict_bow2_kt = text_dict_bow2_fit.transform(df_kt['text'].apply(str))\n",
    "text_dict_bow2_ws = text_dict_bow2_fit.transform(df_ws['texts'].apply(str))\n",
    "\n",
    "print(text_dict_bow1_kt.toarray().shape,  text_dict_bow1_kt.toarray().shape)\n",
    "print(text_dict_bow1_ws.toarray().shape,  text_dict_bow1_ws.toarray().shape)\n",
    "\n",
    "print(text_dict_bow2_kt.toarray().shape,  text_dict_bow2_kt.toarray().shape)\n",
    "print(text_dict_bow2_ws.toarray().shape,  text_dict_bow2_ws.toarray().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b616abd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf1 = TfidfVectorizer(tokenizer=process_text, ngram_range=(1, 1))\n",
    "tfidf2 = TfidfVectorizer(tokenizer=process_text, ngram_range=(2, 2))\n",
    "\n",
    "text_dict_tfidf1_fit = tfidf1.fit(my_vocabs)\n",
    "text_dict_tfidf1_kt = text_dict_tfidf1_fit.transform(df_kt['text'].apply(str))\n",
    "text_dict_tfidf1_ws = text_dict_tfidf1_fit.transform(df_ws['texts'].apply(str))\n",
    "\n",
    "text_dict_tfidf2_fit = tfidf2.fit(my_vocabs)\n",
    "text_dict_tfidf2_kt = text_dict_tfidf2_fit.transform(df_kt['text'].apply(str))\n",
    "text_dict_tfidf2_ws = text_dict_tfidf2_fit.transform(df_ws['texts'].apply(str))\n",
    "\n",
    "print(text_dict_tfidf1_kt.toarray().shape,  text_dict_tfidf1_kt.toarray().shape)\n",
    "print(text_dict_tfidf1_ws.toarray().shape,  text_dict_tfidf1_ws.toarray().shape)\n",
    "\n",
    "print(text_dict_tfidf2_kt.toarray().shape,  text_dict_tfidf2_kt.toarray().shape)\n",
    "print(text_dict_tfidf2_ws.toarray().shape,  text_dict_tfidf2_ws.toarray().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd1b6f8",
   "metadata": {},
   "source": [
    "## Save to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b62645",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import joblib\n",
    "from scipy import sparse\n",
    "\n",
    "y_t_kt = y_kt.to_numpy().reshape(-1, 1)\n",
    "y_t_ws = y_ws.to_numpy().reshape(-1, 1)\n",
    "\n",
    "y_t_kt = sparse.csr_matrix(y_t_kt)\n",
    "y_t_ws = sparse.csr_matrix(y_t_ws)\n",
    "# dump as nparray\n",
    "# arr_bow1_kt = np.concatenate((text_bow1_kt.toarray(), y_t_kt), axis= 1)\n",
    "# arr_bow2_kt = np.concatenate((text_bow2_kt.toarray(), y_t_kt), axis= 1)\n",
    "\n",
    "# # 20k dims is too big when using savetxt, we need to compress\n",
    "# #np.savetext('text_bow1_kt.csv', mat, delimiter=',')\n",
    "# joblib.dump(arr_bow1_kt, model_path+'text_bow1_kt.joblib')\n",
    "# joblib.dump(arr_bow2_kt, model_path+'text_bow2_kt.joblib')\n",
    "\n",
    "# dump sparse matrix\n",
    "# df_bow1_kt = pd.concat([pd.DataFrame.sparse.from_spmatrix(text_bow1_kt), y_kt], axis=1)\n",
    "# df_bow2_kt = pd.concat([pd.DataFrame.sparse.from_spmatrix(text_bow2_kt), y_kt], axis=1)\n",
    "# joblib.dump(df_bow1_kt, model_path+'text_bow1_kt.pkl')\n",
    "# joblib.dump(df_bow2_kt, model_path+'text_bow2_kt.pkl')\n",
    "\n",
    "\n",
    "arr_bow1_kt = np.hstack((text_bow1_kt, y_t_kt))\n",
    "arr_bow2_kt = np.hstack((text_bow2_kt, y_t_kt))\n",
    "joblib.dump(arr_bow1_kt, model_path+'text_bow1_kt.pkl')\n",
    "joblib.dump(arr_bow2_kt, model_path+'text_bow2_kt.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1deed356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arr_bow1_ws = np.concatenate((text_bow1_ws.toarray(), y_t_kt), axis= 1)\n",
    "# arr_bow2_ws = np.concatenate((text_bow2_ws.toarray(), y_t_kt), axis= 1)\n",
    "\n",
    "# joblib.dump(arr_bow1_ws, model_path+'text_bow1_ws.joblib')\n",
    "# joblib.dump(arr_bow2_ws, model_path+'text_bow2_ws.joblib')\n",
    "\n",
    "# dump sparse matrix\n",
    "# df_bow1_ws = pd.concat([pd.DataFrame.sparse.from_spmatrix(text_bow1_ws), y_ws], axis=1)\n",
    "# df_bow2_ws = pd.concat([pd.DataFrame.sparse.from_spmatrix(text_bow2_ws), y_ws], axis=1)\n",
    "# joblib.dump(df_bow1_ws, model_path+'text_bow1_ws.pkl')\n",
    "# joblib.dump(df_bow2_ws, model_path+'text_bow2_ws.pkl')\n",
    "\n",
    "arr_bow1_ws = np.hstack((text_bow1_ws, y_t_ws))\n",
    "arr_bow2_ws = np.hstack((text_bow2_ws, y_t_ws))\n",
    "joblib.dump(arr_bow1_ws, model_path+'text_bow1_ws.pkl')\n",
    "joblib.dump(arr_bow2_ws, model_path+'text_bow2_ws.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b779dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#arr_tfidf1_kt = np.concatenate((text_tfidf1_kt.toarray(), y_t_kt), axis= 1)\n",
    "#arr_tfidf2_kt = np.concatenate((text_tfidf2_kt.toarray(), y_t_kt), axis= 1)\n",
    "#np.savetext('text_bow1_kt.csv', mat, delimiter=',')\n",
    "# joblib.dump(arr_tfidf1_kt, model_path+'text_tfidf1_kt.joblib')\n",
    "# joblib.dump(arr_tfidf2_kt, model_path+'text_tfidf2_kt.joblib')\n",
    "\n",
    "# dump sparse matrix\n",
    "# df_tfidf1_kt = pd.concat([pd.DataFrame.sparse.from_spmatrix(text_tfidf1_kt), y_kt], axis=1)\n",
    "# df_tfidf2_kt = pd.concat([pd.DataFrame.sparse.from_spmatrix(text_tfidf2_kt), y_kt], axis=1)\n",
    "# joblib.dump(df_tfidf1_kt, model_path+'text_tfidf1_kt.pkl')\n",
    "# joblib.dump(df_tfidf2_kt, model_path+'text_tfidf2_kt.pkl')\n",
    "\n",
    "arr_tfidf1_kt = np.hstack((text_tfidf1_kt, y_t_kt))\n",
    "arr_tfidf2_kt = np.hstack((text_tfidf2_kt, y_t_kt))\n",
    "joblib.dump(arr_tfidf1_kt, model_path+'text_tfidf1_kt.pkl')\n",
    "joblib.dump(arr_tfidf2_kt, model_path+'text_tfidf2_kt.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0cd71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arr_tfidf1_ws = np.concatenate((text_tfidf1_ws.toarray(), y_ws_arr), axis= 1)\n",
    "# arr_tfidf2_ws = np.concatenate((text_tfidf2_ws.toarray(), y_ws_arr), axis= 1)\n",
    "\n",
    "# joblib.dump(arr_tfidf1_ws, model_path+'text_tfidf1_ws.joblib')\n",
    "# joblib.dump(arr_tfidf2_ws, model_path+'text_tfidf2_ws.joblib')\n",
    "\n",
    "# # dump sparse matrix\n",
    "# df_tfidf1_ws = pd.concat([pd.DataFrame.sparse.from_spmatrix(text_tfidf1_ws), y_ws], axis=1)\n",
    "# df_tfidf2_ws = pd.concat([pd.DataFrame.sparse.from_spmatrix(text_tfidf2_ws), y_ws], axis=1)\n",
    "# joblib.dump(df_tfidf1_ws, model_path+'text_tfidf1_ws.pkl')\n",
    "# joblib.dump(df_tfidf2_ws, model_path+'text_tfidf2_ws.pkl')\n",
    "\n",
    "arr_tfidf1_ws = np.hstack((text_tfidf1_ws, y_t_ws))\n",
    "arr_tfidf2_ws = np.hstack((text_tfidf2_ws, y_t_ws))\n",
    "joblib.dump(arr_tfidf1_ws, model_path+'text_tfidf1_ws.pkl')\n",
    "joblib.dump(arr_tfidf2_ws, model_path+'text_tfidf2_ws.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04c1868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arr_w2v_tfidf_kt = np.concatenate((text_w2v_tfidf_kt, y_kt_arr), axis= 1)\n",
    "# arr_w2v_tfidf_ws = np.concatenate((text_w2v_tfidf_ws, y_ws_arr), axis= 1)\n",
    "\n",
    "# joblib.dump(arr_w2v_tfidf_kt, model_path+'text_w2v_tfidf_kt.joblib')\n",
    "# joblib.dump(arr_w2v_tfidf_ws, model_path+'text_w2v_tfidf_ws.joblib')\n",
    "\n",
    "# df_w2v_tfidf_kt = pd.concat([pd.DataFrame.sparse.from_spmatrix(sparse.csr_matrix(text_w2v_tfidf_kt)), y_kt], axis=1)\n",
    "# df_w2v_tfidf_ws = pd.concat([pd.DataFrame.sparse.from_spmatrix(sparse.csr_matrix(text_w2v_tfidf_ws)), y_ws], axis=1)\n",
    "# joblib.dump(df_w2v_tfidf_kt, model_path+'text_w2v_tfidf_kt.pkl')\n",
    "# joblib.dump(df_w2v_tfidf_ws, model_path+'text_w2v_tfidf_ws.pkl')\n",
    "\n",
    "arr_w2v_tfidf_kt = np.hstack(( sparse.csr_matrix(text_w2v_tfidf_kt), y_t_kt))\n",
    "arr_w2v_tfidf_ws = np.hstack(( sparse.csr_matrix(text_w2v_tfidf_ws), y_t_ws))\n",
    "joblib.dump(arr_w2v_tfidf_kt, model_path+'text_w2v_tfidf_kt.pkl')\n",
    "joblib.dump(arr_w2v_tfidf_ws, model_path+'text_w2v_tfidf_ws.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7bbd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arr_pos_bow1_kt = np.concatenate((text_pos_bow1_kt.toarray(), y_kt_arr), axis= 1)\n",
    "# arr_pos_bow2_kt = np.concatenate((text_pos_bow2_kt.toarray(), y_kt_arr), axis= 1)\n",
    "\n",
    "# joblib.dump(arr_pos_bow1_kt, model_path+'text_pos_bow1_kt.joblib')\n",
    "# joblib.dump(arr_pos_bow2_kt, model_path+'text_pos_bow2_kt.joblib')\n",
    "\n",
    "# # dump sparse matrix\n",
    "# df_text_pos_bow1_kt = pd.concat([pd.DataFrame.sparse.from_spmatrix(text_pos_bow1_kt), y_kt], axis=1)\n",
    "# df_text_pos_bow2_kt = pd.concat([pd.DataFrame.sparse.from_spmatrix(text_pos_bow2_kt), y_kt], axis=1)\n",
    "# joblib.dump(df_text_pos_bow1_kt, model_path+'text_pos_bow1_kt.pkl')\n",
    "# joblib.dump(df_text_pos_bow2_kt, model_path+'text_pos_bow2_kt.pkl')\n",
    "\n",
    "arr_pos_bow1_kt = np.hstack((text_pos_bow1_kt, y_t_kt))\n",
    "arr_pos_bow2_kt = np.hstack((text_pos_bow2_kt, y_t_kt))\n",
    "joblib.dump(arr_pos_bow1_kt, model_path+'text_pos_bow1_kt.pkl')\n",
    "joblib.dump(arr_pos_bow2_kt, model_path+'text_pos_bow2_kt.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc35210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arr_pos_bow1_ws = np.concatenate((text_pos_bow1_ws.toarray(), y_ws_arr), axis= 1)\n",
    "# arr_pos_bow2_ws = np.concatenate((text_pos_bow2_ws.toarray(), y_ws_arr), axis= 1)\n",
    "\n",
    "# joblib.dump(arr_pos_bow1_ws, model_path+'text_pos_bow1_ws.joblib')\n",
    "# joblib.dump(arr_pos_bow2_ws, model_path+'text_pos_bow2_ws.joblib')\n",
    "\n",
    "# # dump sparse matrix\n",
    "# df_text_pos_bow1_ws = pd.concat([pd.DataFrame.sparse.from_spmatrix(text_pos_bow1_ws), y_ws], axis=1)\n",
    "# df_text_pos_bow2_ws = pd.concat([pd.DataFrame.sparse.from_spmatrix(text_pos_bow2_ws), y_ws], axis=1)\n",
    "# joblib.dump(df_text_pos_bow1_ws, model_path+'text_pos_bow1_ws.pkl')\n",
    "# joblib.dump(df_text_pos_bow2_ws, model_path+'text_pos_bow2_ws.pkl')\n",
    "\n",
    "arr_pos_bow1_ws = np.hstack((text_pos_bow1_ws, y_t_ws))\n",
    "arr_pos_bow2_ws = np.hstack((text_pos_bow2_ws, y_t_ws))\n",
    "joblib.dump(arr_pos_bow1_ws, model_path+'text_pos_bow1_ws.pkl')\n",
    "joblib.dump(arr_pos_bow2_ws, model_path+'text_pos_bow2_ws.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd754034",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_pos_tfidf1_kt = np.hstack((text_pos_tfidf1_kt, y_t_kt))\n",
    "arr_pos_tfidf2_kt = np.hstack((text_pos_tfidf2_kt, y_t_kt))\n",
    "joblib.dump(arr_pos_tfidf1_kt, model_path+'text_pos_tfidf1_kt.pkl')\n",
    "joblib.dump(arr_pos_tfidf2_kt, model_path+'text_pos_tfidf2_kt.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff31a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_pos_tfidf1_ws = np.hstack((text_pos_tfidf1_ws, y_t_ws))\n",
    "arr_pos_tfidf2_ws = np.hstack((text_pos_tfidf2_ws, y_t_ws))\n",
    "joblib.dump(arr_pos_tfidf1_ws, model_path+'text_pos_tfidf1_ws.pkl')\n",
    "joblib.dump(arr_pos_tfidf2_ws, model_path+'text_pos_tfidf2_ws.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78281902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arr_dict_bow1_kt = np.concatenate((text_pos_bow1_kt.toarray(), y_kt_arr), axis= 1)\n",
    "# arr_dict_bow2_kt = np.concatenate((text_pos_bow2_kt.toarray(), y_kt_arr), axis= 1)\n",
    "\n",
    "# joblib.dump(arr_dict_bow1_kt, model_path+'text_dict_bow1_kt.joblib')\n",
    "# joblib.dump(arr_dict_bow2_kt, model_path+'text_dict_bow2_kt.joblib')\n",
    "\n",
    "# # dump sparse matrix\n",
    "# df_text_pos_bow1_kt = pd.concat([pd.DataFrame.sparse.from_spmatrix(text_pos_bow1_kt), y_kt], axis=1)\n",
    "# df_text_pos_bow2_kt = pd.concat([pd.DataFrame.sparse.from_spmatrix(text_pos_bow2_kt), y_kt], axis=1)\n",
    "# joblib.dump(df_text_pos_bow1_kt, model_path+'text_pos_bow1_kt.pkl')\n",
    "# joblib.dump(df_text_pos_bow2_kt, model_path+'text_pos_bow2_kt.pkl')\n",
    "\n",
    "arr_dict_bow1_kt = np.hstack((text_dict_bow1_kt, y_t_kt))\n",
    "arr_dict_bow2_kt = np.hstack((text_dict_bow2_kt, y_t_kt))\n",
    "joblib.dump(arr_dict_bow1_kt, model_path+'text_dict_bow1_kt.pkl')\n",
    "joblib.dump(arr_dict_bow2_kt, model_path+'text_dict_bow2_kt.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a3abf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arr_dict_bow1_ws = np.concatenate((text_dict_bow1_ws.toarray(), y_ws_arr), axis= 1)\n",
    "# arr_dict_bow2_ws = np.concatenate((text_dict_bow2_ws.toarray(), y_ws_arr), axis= 1)\n",
    "\n",
    "# joblib.dump(arr_dict_bow1_ws, model_path+'text_dict_bow1_ws.joblib')\n",
    "# joblib.dump(arr_dict_bow2_ws, model_path+'text_dict_bow2_ws.joblib')\n",
    "\n",
    "# # dump sparse matrix\n",
    "# df_text_pos_bow1_ws = pd.concat([pd.DataFrame.sparse.from_spmatrix(text_pos_bow1_ws), y_ws], axis=1)\n",
    "# df_text_pos_bow2_ws = pd.concat([pd.DataFrame.sparse.from_spmatrix(text_pos_bow2_ws), y_ws], axis=1)\n",
    "# joblib.dump(df_text_pos_bow1_ws, model_path+'text_pos_bow1_ws.pkl')\n",
    "# joblib.dump(df_text_pos_bow2_ws, model_path+'text_pos_bow2_ws.pkl')\n",
    "\n",
    "arr_dict_bow1_ws = np.hstack((text_dict_bow1_ws, y_t_ws))\n",
    "arr_dict_bow2_ws = np.hstack((text_dict_bow2_ws, y_t_ws))\n",
    "joblib.dump(arr_dict_bow1_ws, model_path+'text_dict_bow1_ws.pkl')\n",
    "joblib.dump(arr_dict_bow2_ws, model_path+'text_dict_bow2_ws.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ccbf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arr_dict_tfidf1_kt = np.concatenate((text_dict_tfidf1_kt.toarray(), y_kt_arr), axis= 1)\n",
    "# arr_dict_tfidf2_kt = np.concatenate((text_dict_tfidf1_kt.toarray(), y_kt_arr), axis= 1)\n",
    "\n",
    "# joblib.dump(arr_dict_tfidf1_kt, model_path+'text_dict_tfidf1_kt.joblib')\n",
    "# joblib.dump(arr_dict_tfidf2_kt, model_path+'text_dict_tfidf2_kt.joblib')\n",
    "\n",
    "# # dump sparse matrix\n",
    "# df_dict_tfidf1_kt = pd.concat([pd.DataFrame.sparse.from_spmatrix(text_dict_tfidf1_kt), y_kt], axis=1)\n",
    "# df_dict_tfidf2_kt = pd.concat([pd.DataFrame.sparse.from_spmatrix(text_dict_tfidf2_kt), y_kt], axis=1)\n",
    "# joblib.dump(df_dict_tfidf1_kt, model_path+'text_dict_tfidf1_kt.pkl')\n",
    "# joblib.dump(df_dict_tfidf2_kt, model_path+'text_dict_tfidf2_kt.pkl')\n",
    "\n",
    "arr_dict_tfidf1_kt = np.hstack((text_dict_tfidf1_kt, y_t_kt))\n",
    "arr_dict_tfidf2_kt = np.hstack((text_dict_tfidf2_kt, y_t_kt))\n",
    "joblib.dump(arr_dict_tfidf1_kt, model_path+'text_dict_tfidf1_kt.pkl')\n",
    "joblib.dump(arr_dict_tfidf2_kt, model_path+'text_dict_tfidf2_kt.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa13781e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arr_dict_tfidf1_ws = np.concatenate((text_dict_tfidf1_ws.toarray(), y_ws_arr), axis= 1)\n",
    "# arr_dict_tfidf2_ws = np.concatenate((text_dict_tfidf1_ws.toarray(), y_ws_arr), axis= 1)\n",
    "\n",
    "# joblib.dump(arr_dict_tfidf1_ws, model_path+'text_dict_tfidf1_ws.joblib')\n",
    "# joblib.dump(arr_dict_tfidf2_ws, model_path+'text_dict_tfidf2_ws.joblib')\n",
    "\n",
    "# # dump sparse matrix\n",
    "# df_dict_tfidf1_ws = pd.concat([pd.DataFrame.sparse.from_spmatrix(text_dict_tfidf1_ws), y_ws], axis=1)\n",
    "# df_dict_tfidf2_ws = pd.concat([pd.DataFrame.sparse.from_spmatrix(text_dict_tfidf2_ws), y_ws], axis=1)\n",
    "# joblib.dump(df_dict_tfidf1_ws, model_path+'text_dict_tfidf1_ws.pkl')\n",
    "# joblib.dump(df_dict_tfidf2_ws, model_path+'text_dict_tfidf2_ws.pkl')\n",
    "\n",
    "arr_dict_tfidf1_ws = np.hstack((text_dict_tfidf1_ws, y_t_ws))\n",
    "arr_dict_tfidf2_ws = np.hstack((text_dict_tfidf2_ws, y_t_ws))\n",
    "joblib.dump(arr_dict_tfidf1_ws, model_path+'text_dict_tfidf1_ws.pkl')\n",
    "joblib.dump(arr_dict_tfidf2_ws, model_path+'text_dict_tfidf2_ws.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f1e6f2",
   "metadata": {},
   "source": [
    "## Demonstrate usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15f9a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load kt corpus extracted features array \n",
    "#arr_kt = np.load(model_path+'text_tfidf2_kt.joblib')\n",
    "    \n",
    "item = joblib.load(model_path+'text_tfidf2_kt.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9f719f",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_kt = np.concatenate((item[0].A, item[1].A), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79abee6",
   "metadata": {},
   "source": [
    "## Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2e673d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "train_df, test_df = train_test_split(arr_kt, test_size=0.20, random_state=42)\n",
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520d359d",
   "metadata": {},
   "source": [
    "## Train-valid split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87519fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(train_df[:,:-1], train_df[:,-1], test_size=0.15, random_state=42)\n",
    "X_train.shape, X_valid.shape, y_train.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3159c026",
   "metadata": {},
   "source": [
    "## Test the extracted features with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1413dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick test with out cv\n",
    "#fit logistic regression models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(C=2., penalty=\"l2\", solver=\"liblinear\", dual=False, multi_class=\"ovr\")\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_valid, y_valid)\n",
    "#y_pred = model.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6282242",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
    "def build_model(model):\n",
    "    scores = (cross_val_score(model, X_train, y_train, cv = 5).mean())\n",
    "    model = model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_valid)\n",
    "    acc_sc = accuracy_score(y_valid, y_pred)\n",
    "    pre_sc = precision_score(y_valid, y_pred, average='weighted')\n",
    "    rec_sc = recall_score(y_valid, y_pred, average='weighted')\n",
    "    f1_sc = f1_score(y_valid, y_pred, average='weighted')\n",
    "    print('Accuracy :',acc_sc)\n",
    "    print('Confusion Matrix :\\n', confusion_matrix(y_valid, y_pred))\n",
    "    print('Precision :', pre_sc)\n",
    "    print('Recall :', rec_sc)\n",
    "    print('F1-score :', f1_sc)\n",
    "    print('Classification Report :\\n', classification_report(y_valid, y_pred))\n",
    "    print('Average accuracy of k-fold (5-fold) :', scores ,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e120d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "build_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ed0983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on kt dataset\n",
    "#arr_ws = np.load(model_path+'text_tfidf2_ws.npy')\n",
    "item = joblib.load(model_path+'text_tfidf2_ws.pkl')\n",
    "arr_ws = np.concatenate((item[0].A, item[1].A), axis=1)\n",
    "arr_ws.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c086afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(arr_ws, test_size=0.20, random_state=42)\n",
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d73174f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(train_df[:,:-1], train_df[:,-1], test_size=0.15, random_state=42)\n",
    "X_train.shape, X_valid.shape, y_train.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640f33fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "build_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fa3504",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
