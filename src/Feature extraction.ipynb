{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "139d4b2f",
   "metadata": {},
   "source": [
    "### Feature extractions\n",
    "\n",
    "This script responds to features extracted from two sentimental corpora, kt4.0 (ours) and wisesight. By combining train data from both corpora, we expect to see an improvement in the wisesight corpus' classification performance.\n",
    "\n",
    "For both datasets, random stratify hold-out was performed with 80:20 ratio for train and test set. Feature engineering was carried out including dictionary-based (i.e., using list of good and bad Thai words), word count approches. Next, several feature extraction methods were applied and output as a joblib objects as follows:  \n",
    "\n",
    "* BOW1, BOW2\n",
    "* TF-IDF1, TF-IDF2\n",
    "* Word2Vec pretrained from Thai wiki. (300 dimension)\n",
    "* POS_tagging with flatten dataframe\n",
    "\n",
    "Dependencies\n",
    "* pythainlp >= 3.06dev\n",
    "* python >= 3.8.8\n",
    "* gensim >= 4.1.2\n",
    "* scikit-learn >= 1.0.2\n",
    "* joblib = 1.1.0\n",
    "* dill = 0.31\n",
    "\n",
    "The output vectors will be carried out in the next experiment.  \n",
    "pree.t@cmu.ac.th  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a978d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pythainlp\n",
    "from pythainlp.ulmfit import process_thai\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.rcParams['font.family'] = 'tahoma'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47e606a",
   "metadata": {},
   "source": [
    "## Load original datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fe523d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60081, 14), (26737, 2))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.path.dirname(os.getcwd())\n",
    "\n",
    "data_path_kt = os.path.dirname(os.getcwd()) + '\\\\data\\kt4.0\\\\'\n",
    "data_path_ws = os.path.dirname(os.getcwd()) + '\\\\data\\wisesight\\\\'\n",
    "df_kt = pd.read_csv(data_path_kt + 'pantip_cleaned_1.csv')\n",
    "\n",
    "# we use the original wisesight corpus and reconstruct a new dataframe\n",
    "texts = []\n",
    "targets = []\n",
    "\n",
    "with open(str(data_path_ws) + '/' + 'neg.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        texts.append(line.strip())\n",
    "        targets.append('neg')\n",
    "\n",
    "with open(str(data_path_ws) + '/' + 'neu.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        texts.append(line.strip())\n",
    "        targets.append('neu')\n",
    "\n",
    "with open(str(data_path_ws) + '/' + 'pos.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        texts.append(line.strip())\n",
    "        targets.append('pos')\n",
    "\n",
    "with open(str(data_path_ws) + '/' + 'q.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        texts.append(line.strip())\n",
    "        targets.append('q')\n",
    "        \n",
    "df_ws = pd.DataFrame({'texts': texts, 'targets': targets})\n",
    "df_ws.to_csv('wisesight.csv', index=False)\n",
    "df_kt.shape, df_ws.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c128ab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>post_date</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_name</th>\n",
       "      <th>text</th>\n",
       "      <th>tag</th>\n",
       "      <th>emotion</th>\n",
       "      <th>length</th>\n",
       "      <th>num_sent</th>\n",
       "      <th>sent_length</th>\n",
       "      <th>label</th>\n",
       "      <th>label_1</th>\n",
       "      <th>label_2</th>\n",
       "      <th>vote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39839097</td>\n",
       "      <td>2020-04-25 13:24:00</td>\n",
       "      <td>https://pantip.com/profile/5798163</td>\n",
       "      <td>สมาชิกหมายเลข 5798163</td>\n",
       "      <td>[CR] แปังพัฟคุมมัน จัดเต็มเนื้อบางเบา</td>\n",
       "      <td>เครื่องสำอาง</td>\n",
       "      <td>ถูกใจ 0 ขำกลิ้ง 0 หลงรัก 0 ซึ้ง 0 สยอง 0 ทึ่ง 0</td>\n",
       "      <td>36</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39839097</td>\n",
       "      <td>2020-04-25 13:24:00</td>\n",
       "      <td>https://pantip.com/profile/5798163</td>\n",
       "      <td>สมาชิกหมายเลข 5798163</td>\n",
       "      <td>ไม่อุดตัน แต่ปกปิดแน่นมาก</td>\n",
       "      <td>เครื่องสำอาง</td>\n",
       "      <td>ถูกใจ 0 ขำกลิ้ง 0 หลงรัก 0 ซึ้ง 0 สยอง 0 ทึ่ง 0</td>\n",
       "      <td>36</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39839097</td>\n",
       "      <td>2020-04-25 13:24:00</td>\n",
       "      <td>https://pantip.com/profile/5798163</td>\n",
       "      <td>สมาชิกหมายเลข 5798163</td>\n",
       "      <td>รีวิวแป้ง Lady Audrey Ready All Day จ้า</td>\n",
       "      <td>เครื่องสำอาง</td>\n",
       "      <td>ถูกใจ 0 ขำกลิ้ง 0 หลงรัก 0 ซึ้ง 0 สยอง 0 ทึ่ง 0</td>\n",
       "      <td>36</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39838736</td>\n",
       "      <td>2020-04-25 10:52:00</td>\n",
       "      <td>https://pantip.com/profile/5730006</td>\n",
       "      <td>สมาชิกหมายเลข 5730006</td>\n",
       "      <td>ขอบตาดำมากค่ะ คอร์เล็คเตอร์ก็เอาไม่อยู่</td>\n",
       "      <td>เครื่องสำอาง</td>\n",
       "      <td>ถูกใจ 0 ขำกลิ้ง 0 หลงรัก 0 ซึ้ง 0 สยอง 0 ทึ่ง 0</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>39837384</td>\n",
       "      <td>2020-04-24 20:39:00</td>\n",
       "      <td>https://pantip.com/profile/4975838</td>\n",
       "      <td>สมาชิกหมายเลข 4975838</td>\n",
       "      <td>เอาaloe Vera แช่ตู้เย็น จนกลายเป็นน้ำแข็ง</td>\n",
       "      <td>เครื่องสำอาง</td>\n",
       "      <td>ถูกใจ 0 ขำกลิ้ง 0 หลงรัก 0 ซึ้ง 0 สยอง 0 ทึ่ง 0</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>neu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>39838990</td>\n",
       "      <td>2020-04-25 12:36:00</td>\n",
       "      <td>https://pantip.com/profile/5655853</td>\n",
       "      <td>chdewxx</td>\n",
       "      <td>[SR] ไอเทม #เซรั่มสิว ลดสิว สิวอุดตัน สิวผด บำ...</td>\n",
       "      <td>เครื่องสำอาง</td>\n",
       "      <td>ถูกใจ 0 ขำกลิ้ง 0 หลงรัก 0 ซึ้ง 0 สยอง 0 ทึ่ง 0</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>39838619</td>\n",
       "      <td>2020-04-25 10:01:00</td>\n",
       "      <td>https://pantip.com/profile/5656639</td>\n",
       "      <td>คูจองยอนและวีรยา</td>\n",
       "      <td>รบกวนสาวๆช่วยแนะนำสกินแคร์ ที่ช่วยให้ผิวหน้าขา...</td>\n",
       "      <td>เครื่องสำอาง</td>\n",
       "      <td>ถูกใจ 0 ขำกลิ้ง 0 หลงรัก 0 ซึ้ง 0 สยอง 0 ทึ่ง 0</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>39837266</td>\n",
       "      <td>2020-04-24 19:58:00</td>\n",
       "      <td>https://pantip.com/profile/632132</td>\n",
       "      <td>หมูกลมอารมณ์ดี</td>\n",
       "      <td>ทดลองใช้ แครอทวิตซีหน้าใส</td>\n",
       "      <td>เครื่องสำอาง</td>\n",
       "      <td>ถูกใจ 0 ขำกลิ้ง 0 หลงรัก 0 ซึ้ง 0 สยอง 0 ทึ่ง 0</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>39837266</td>\n",
       "      <td>2020-04-24 19:58:00</td>\n",
       "      <td>https://pantip.com/profile/632132</td>\n",
       "      <td>หมูกลมอารมณ์ดี</td>\n",
       "      <td>ใน 1 สัปดาห์</td>\n",
       "      <td>เครื่องสำอาง</td>\n",
       "      <td>ถูกใจ 0 ขำกลิ้ง 0 หลงรัก 0 ซึ้ง 0 สยอง 0 ทึ่ง 0</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>neu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>39835926</td>\n",
       "      <td>2020-04-24 12:03:00</td>\n",
       "      <td>https://pantip.com/profile/3826851</td>\n",
       "      <td>สมาชิกหมายเลข 3826851</td>\n",
       "      <td>วิธีเลือก \"รองพื้น\" และ \"คอนซีลเลอร์\"</td>\n",
       "      <td>เครื่องสำอาง</td>\n",
       "      <td>ถูกใจ 0 ขำกลิ้ง 0 หลงรัก 0 ซึ้ง 0 สยอง 0 ทึ่ง 0</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    post_id            post_date                             user_id  \\\n",
       "0  39839097  2020-04-25 13:24:00  https://pantip.com/profile/5798163   \n",
       "1  39839097  2020-04-25 13:24:00  https://pantip.com/profile/5798163   \n",
       "2  39839097  2020-04-25 13:24:00  https://pantip.com/profile/5798163   \n",
       "3  39838736  2020-04-25 10:52:00  https://pantip.com/profile/5730006   \n",
       "4  39837384  2020-04-24 20:39:00  https://pantip.com/profile/4975838   \n",
       "5  39838990  2020-04-25 12:36:00  https://pantip.com/profile/5655853   \n",
       "6  39838619  2020-04-25 10:01:00  https://pantip.com/profile/5656639   \n",
       "7  39837266  2020-04-24 19:58:00   https://pantip.com/profile/632132   \n",
       "8  39837266  2020-04-24 19:58:00   https://pantip.com/profile/632132   \n",
       "9  39835926  2020-04-24 12:03:00  https://pantip.com/profile/3826851   \n",
       "\n",
       "               user_name                                               text  \\\n",
       "0  สมาชิกหมายเลข 5798163              [CR] แปังพัฟคุมมัน จัดเต็มเนื้อบางเบา   \n",
       "1  สมาชิกหมายเลข 5798163                          ไม่อุดตัน แต่ปกปิดแน่นมาก   \n",
       "2  สมาชิกหมายเลข 5798163            รีวิวแป้ง Lady Audrey Ready All Day จ้า   \n",
       "3  สมาชิกหมายเลข 5730006            ขอบตาดำมากค่ะ คอร์เล็คเตอร์ก็เอาไม่อยู่   \n",
       "4  สมาชิกหมายเลข 4975838          เอาaloe Vera แช่ตู้เย็น จนกลายเป็นน้ำแข็ง   \n",
       "5                chdewxx  [SR] ไอเทม #เซรั่มสิว ลดสิว สิวอุดตัน สิวผด บำ...   \n",
       "6       คูจองยอนและวีรยา  รบกวนสาวๆช่วยแนะนำสกินแคร์ ที่ช่วยให้ผิวหน้าขา...   \n",
       "7         หมูกลมอารมณ์ดี                          ทดลองใช้ แครอทวิตซีหน้าใส   \n",
       "8         หมูกลมอารมณ์ดี                                       ใน 1 สัปดาห์   \n",
       "9  สมาชิกหมายเลข 3826851              วิธีเลือก \"รองพื้น\" และ \"คอนซีลเลอร์\"   \n",
       "\n",
       "            tag                                          emotion  length  \\\n",
       "0  เครื่องสำอาง  ถูกใจ 0 ขำกลิ้ง 0 หลงรัก 0 ซึ้ง 0 สยอง 0 ทึ่ง 0      36   \n",
       "1  เครื่องสำอาง  ถูกใจ 0 ขำกลิ้ง 0 หลงรัก 0 ซึ้ง 0 สยอง 0 ทึ่ง 0      36   \n",
       "2  เครื่องสำอาง  ถูกใจ 0 ขำกลิ้ง 0 หลงรัก 0 ซึ้ง 0 สยอง 0 ทึ่ง 0      36   \n",
       "3  เครื่องสำอาง  ถูกใจ 0 ขำกลิ้ง 0 หลงรัก 0 ซึ้ง 0 สยอง 0 ทึ่ง 0      15   \n",
       "4  เครื่องสำอาง  ถูกใจ 0 ขำกลิ้ง 0 หลงรัก 0 ซึ้ง 0 สยอง 0 ทึ่ง 0      11   \n",
       "5  เครื่องสำอาง  ถูกใจ 0 ขำกลิ้ง 0 หลงรัก 0 ซึ้ง 0 สยอง 0 ทึ่ง 0      29   \n",
       "6  เครื่องสำอาง  ถูกใจ 0 ขำกลิ้ง 0 หลงรัก 0 ซึ้ง 0 สยอง 0 ทึ่ง 0      23   \n",
       "7  เครื่องสำอาง  ถูกใจ 0 ขำกลิ้ง 0 หลงรัก 0 ซึ้ง 0 สยอง 0 ทึ่ง 0      14   \n",
       "8  เครื่องสำอาง  ถูกใจ 0 ขำกลิ้ง 0 หลงรัก 0 ซึ้ง 0 สยอง 0 ทึ่ง 0      14   \n",
       "9  เครื่องสำอาง  ถูกใจ 0 ขำกลิ้ง 0 หลงรัก 0 ซึ้ง 0 สยอง 0 ทึ่ง 0      14   \n",
       "\n",
       "   num_sent  sent_length  label  label_1  label_2 vote  \n",
       "0         3           14      2        2        2  pos  \n",
       "1         3            8      2        2        2  pos  \n",
       "2         3           14      2        2        1  pos  \n",
       "3         2           13      1        3        3  neg  \n",
       "4         1           11      1        1        3  neu  \n",
       "5         1           29      2        2        2  pos  \n",
       "6         1           23      2        2        1  pos  \n",
       "7         2            9      1        2        2  pos  \n",
       "8         2            5      1        1        1  neu  \n",
       "9         1           14      2        2        2  pos  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_kt.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d84766f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>length</th>\n",
       "      <th>num_sent</th>\n",
       "      <th>sent_length</th>\n",
       "      <th>label</th>\n",
       "      <th>label_1</th>\n",
       "      <th>label_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6.008100e+04</td>\n",
       "      <td>60081.000000</td>\n",
       "      <td>60081.000000</td>\n",
       "      <td>60081.000000</td>\n",
       "      <td>60081.000000</td>\n",
       "      <td>60081.000000</td>\n",
       "      <td>60081.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.964936e+07</td>\n",
       "      <td>116.994574</td>\n",
       "      <td>8.502172</td>\n",
       "      <td>13.978329</td>\n",
       "      <td>1.577304</td>\n",
       "      <td>1.362644</td>\n",
       "      <td>1.662156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.559919e+05</td>\n",
       "      <td>118.647716</td>\n",
       "      <td>7.575442</td>\n",
       "      <td>12.083572</td>\n",
       "      <td>0.777527</td>\n",
       "      <td>0.639271</td>\n",
       "      <td>0.800034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.917283e+07</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.958755e+07</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.968929e+07</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.976947e+07</td>\n",
       "      <td>159.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.983970e+07</td>\n",
       "      <td>499.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>301.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            post_id        length      num_sent   sent_length         label  \\\n",
       "count  6.008100e+04  60081.000000  60081.000000  60081.000000  60081.000000   \n",
       "mean   3.964936e+07    116.994574      8.502172     13.978329      1.577304   \n",
       "std    1.559919e+05    118.647716      7.575442     12.083572      0.777527   \n",
       "min    3.917283e+07      3.000000      1.000000      3.000000      1.000000   \n",
       "25%    3.958755e+07     31.000000      3.000000      6.000000      1.000000   \n",
       "50%    3.968929e+07     72.000000      6.000000     10.000000      1.000000   \n",
       "75%    3.976947e+07    159.000000     11.000000     17.000000      2.000000   \n",
       "max    3.983970e+07    499.000000     44.000000    301.000000      3.000000   \n",
       "\n",
       "            label_1       label_2  \n",
       "count  60081.000000  60081.000000  \n",
       "mean       1.362644      1.662156  \n",
       "std        0.639271      0.800034  \n",
       "min        1.000000      1.000000  \n",
       "25%        1.000000      1.000000  \n",
       "50%        1.000000      1.000000  \n",
       "75%        2.000000      2.000000  \n",
       "max        3.000000      3.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_kt.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5ec5386",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texts</th>\n",
       "      <th>targets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>☹️</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>😔</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>😞</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>😥</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>รำ</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Noๆ</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Rip</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>T_T</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>กาก</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>โกง</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  texts targets\n",
       "0    ☹️     neg\n",
       "1     😔     neg\n",
       "2     😞     neg\n",
       "3     😥     neg\n",
       "4    รำ     neg\n",
       "5   Noๆ     neg\n",
       "6   Rip     neg\n",
       "7   T_T     neg\n",
       "8   กาก     neg\n",
       "9   โกง     neg"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ws.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7077f068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texts</th>\n",
       "      <th>targets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>26737</td>\n",
       "      <td>26737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>26713</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>อุดรมีไหมค่ะ</td>\n",
       "      <td>neu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>2</td>\n",
       "      <td>14561</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               texts targets\n",
       "count          26737   26737\n",
       "unique         26713       4\n",
       "top     อุดรมีไหมค่ะ     neu\n",
       "freq               2   14561"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ws.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75693314",
   "metadata": {},
   "source": [
    "# Train-test split 80/20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c67990d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((48064, 14), (12017, 14))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random stratified split train and test set 80/20\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate\n",
    "all_df_kt, test_df_kt = train_test_split(df_kt, test_size=0.2, random_state=42, shuffle = True)\n",
    "all_df_kt.shape, test_df_kt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c7d3bcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neu    0.632136\n",
       "pos    0.206620\n",
       "neg    0.161243\n",
       "Name: vote, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# class distribution\n",
    "all_df_kt.vote.value_counts() / all_df_kt.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b63ee3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((21389, 2), (5348, 2))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df_ws, test_df_ws = train_test_split(df_ws, test_size=0.2, random_state=42)\n",
    "all_df_ws.shape, test_df_ws.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae548ccd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neu    0.544860\n",
       "neg    0.253588\n",
       "pos    0.179345\n",
       "q      0.022208\n",
       "Name: targets, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# class distribution\n",
    "all_df_ws.targets.value_counts() / all_df_ws.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82ef6037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean and word tokenize\n",
    "all_df_kt['processed'] = all_df_kt['text'].apply(str).apply(process_thai)\n",
    "test_df_kt['processed'] = test_df_kt['text'].apply(str).apply(process_thai)\n",
    "\n",
    "all_df_ws['processed'] = all_df_ws['texts'].apply(str).apply(process_thai)\n",
    "test_df_ws['processed'] = test_df_ws['texts'].apply(str).apply(process_thai)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c6949e",
   "metadata": {},
   "source": [
    "## Feature engineering: dictionary-based, word count, and unique word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e922a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new feature with dict-based approach\n",
    "# load list of our custom positive and negative words\n",
    "with open(os.path.dirname(os.getcwd()) + '\\\\data\\\\' + 'pos_words.txt', encoding='UTF-8') as f:\n",
    "    pos_words = [line.rstrip('\\n') for line in f]\n",
    "\n",
    "with open(os.path.dirname(os.getcwd()) + '\\\\data\\\\' + 'neg_words.txt', encoding='UTF-8') as f:\n",
    "    neg_words = [line.rstrip('\\n') for line in f]\n",
    "pos_words = list(set(pos_words))\n",
    "neg_words = list(set(neg_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ad848e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_sentiment(sentence):\n",
    "    senti = 0\n",
    "    words = [word.lower() for word in sentence]\n",
    "    for word in words:\n",
    "        if word in pos_words:\n",
    "            senti += 1\n",
    "        elif word in neg_words:\n",
    "            senti -= 1\n",
    "    return senti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6fa19d0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>post_date</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_name</th>\n",
       "      <th>text</th>\n",
       "      <th>tag</th>\n",
       "      <th>emotion</th>\n",
       "      <th>length</th>\n",
       "      <th>num_sent</th>\n",
       "      <th>sent_length</th>\n",
       "      <th>label</th>\n",
       "      <th>label_1</th>\n",
       "      <th>label_2</th>\n",
       "      <th>vote</th>\n",
       "      <th>processed</th>\n",
       "      <th>cal_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43487</th>\n",
       "      <td>39420424</td>\n",
       "      <td>2019-11-19 12:36:00</td>\n",
       "      <td>https://pantip.com/profile/3561069</td>\n",
       "      <td>สมาชิกหมายเลข 3561069</td>\n",
       "      <td>ตามหัวข้อ และตามรูปประกอบเลยครับ Zinc Vistra 2...</td>\n",
       "      <td>อาหารเสริม</td>\n",
       "      <td>ถูกใจ 0 ขำกลิ้ง 0 หลงรัก 0 ซึ้ง 0 สยอง 0 ทึ่ง 0</td>\n",
       "      <td>197</td>\n",
       "      <td>11</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>neu</td>\n",
       "      <td>[ตาม, หัวข้อ, และ, ตาม, รูปประกอบ, เลย, ครับ, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31804</th>\n",
       "      <td>39711716</td>\n",
       "      <td>2020-03-13 15:14:00</td>\n",
       "      <td>https://pantip.com/profile/162639</td>\n",
       "      <td>PANTIP CREW</td>\n",
       "      <td>และยังเป็น Expert Account ในนามล็อกอิน pholfoo...</td>\n",
       "      <td>อาหาร</td>\n",
       "      <td>ถูกใจ 5 ขำกลิ้ง 0 หลงรัก 0 ซึ้ง 0 สยอง 0 ทึ่ง 0</td>\n",
       "      <td>154</td>\n",
       "      <td>8</td>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>pos</td>\n",
       "      <td>[และ, ยัง, เป็น, expert, account, ในนาม, ล็อกอ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3000</th>\n",
       "      <td>39646166</td>\n",
       "      <td>2020-02-17 16:39:00</td>\n",
       "      <td>https://pantip.com/profile/5746727</td>\n",
       "      <td>สมาชิกหมายเลข 5746727</td>\n",
       "      <td>อยากผิวขาวใส แต่ขี้เกียจทาครีม</td>\n",
       "      <td>อาหารเสริม</td>\n",
       "      <td>ถูกใจ 0 ขำกลิ้ง 0 หลงรัก 0 ซึ้ง 0 สยอง 0 ทึ่ง 0</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>neu</td>\n",
       "      <td>[อยาก, ผิวขาว, ใส, แต่, ขี้เกียจ, ทา, ครีม]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8760</th>\n",
       "      <td>39763266</td>\n",
       "      <td>2020-03-31 19:43:00</td>\n",
       "      <td>https://pantip.com/profile/567821</td>\n",
       "      <td>sugarsand</td>\n",
       "      <td>แก้ไขข้อความเมื่อ 31 มีนาคม เวลา 19:55 น.</td>\n",
       "      <td>เครื่องสำอาง</td>\n",
       "      <td>ถูกใจ 0 ขำกลิ้ง 0 หลงรัก 0 ซึ้ง 0 สยอง 0 ทึ่ง 0</td>\n",
       "      <td>96</td>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>neu</td>\n",
       "      <td>[แก้ไข, ข้อความ, เมื่อ, 31, มีนาคม, เวลา, 19, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27651</th>\n",
       "      <td>39762185</td>\n",
       "      <td>2020-03-31 20:48:00</td>\n",
       "      <td>https://pantip.com/profile/674738</td>\n",
       "      <td>lantaolhin</td>\n",
       "      <td>เพิ่งรู้เหตุผลที่แท้จริงก็วันนี้เอง</td>\n",
       "      <td>อาหาร</td>\n",
       "      <td>ถูกใจ 0 ขำกลิ้ง 0 หลงรัก 0 ซึ้ง 0 สยอง 0 ทึ่ง 0</td>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>neu</td>\n",
       "      <td>[เพิ่ง, รู้, เหตุผล, ที่, แท้จริง, ก็, วันนี้,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        post_id            post_date                             user_id  \\\n",
       "43487  39420424  2019-11-19 12:36:00  https://pantip.com/profile/3561069   \n",
       "31804  39711716  2020-03-13 15:14:00   https://pantip.com/profile/162639   \n",
       "3000   39646166  2020-02-17 16:39:00  https://pantip.com/profile/5746727   \n",
       "8760   39763266  2020-03-31 19:43:00   https://pantip.com/profile/567821   \n",
       "27651  39762185  2020-03-31 20:48:00   https://pantip.com/profile/674738   \n",
       "\n",
       "                   user_name  \\\n",
       "43487  สมาชิกหมายเลข 3561069   \n",
       "31804            PANTIP CREW   \n",
       "3000   สมาชิกหมายเลข 5746727   \n",
       "8760               sugarsand   \n",
       "27651             lantaolhin   \n",
       "\n",
       "                                                    text           tag  \\\n",
       "43487  ตามหัวข้อ และตามรูปประกอบเลยครับ Zinc Vistra 2...    อาหารเสริม   \n",
       "31804  และยังเป็น Expert Account ในนามล็อกอิน pholfoo...         อาหาร   \n",
       "3000                      อยากผิวขาวใส แต่ขี้เกียจทาครีม    อาหารเสริม   \n",
       "8760           แก้ไขข้อความเมื่อ 31 มีนาคม เวลา 19:55 น.  เครื่องสำอาง   \n",
       "27651                เพิ่งรู้เหตุผลที่แท้จริงก็วันนี้เอง         อาหาร   \n",
       "\n",
       "                                               emotion  length  num_sent  \\\n",
       "43487  ถูกใจ 0 ขำกลิ้ง 0 หลงรัก 0 ซึ้ง 0 สยอง 0 ทึ่ง 0     197        11   \n",
       "31804  ถูกใจ 5 ขำกลิ้ง 0 หลงรัก 0 ซึ้ง 0 สยอง 0 ทึ่ง 0     154         8   \n",
       "3000   ถูกใจ 0 ขำกลิ้ง 0 หลงรัก 0 ซึ้ง 0 สยอง 0 ทึ่ง 0      16         2   \n",
       "8760   ถูกใจ 0 ขำกลิ้ง 0 หลงรัก 0 ซึ้ง 0 สยอง 0 ทึ่ง 0      96         3   \n",
       "27651  ถูกใจ 0 ขำกลิ้ง 0 หลงรัก 0 ซึ้ง 0 สยอง 0 ทึ่ง 0      21         3   \n",
       "\n",
       "       sent_length  label  label_1  label_2 vote  \\\n",
       "43487           18      1        1        1  neu   \n",
       "31804           43      1        2        2  pos   \n",
       "3000             9      1        1        3  neu   \n",
       "8760            16      1        1        1  neu   \n",
       "27651            9      1        1        1  neu   \n",
       "\n",
       "                                               processed  cal_sentiment  \n",
       "43487  [ตาม, หัวข้อ, และ, ตาม, รูปประกอบ, เลย, ครับ, ...              0  \n",
       "31804  [และ, ยัง, เป็น, expert, account, ในนาม, ล็อกอ...              0  \n",
       "3000         [อยาก, ผิวขาว, ใส, แต่, ขี้เกียจ, ทา, ครีม]              0  \n",
       "8760   [แก้ไข, ข้อความ, เมื่อ, 31, มีนาคม, เวลา, 19, ...              0  \n",
       "27651  [เพิ่ง, รู้, เหตุผล, ที่, แท้จริง, ก็, วันนี้,...              0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df_kt['cal_sentiment'] = all_df_kt['processed'].apply(cal_sentiment)\n",
    "all_df_ws['cal_sentiment'] = all_df_ws['processed'].apply(cal_sentiment)\n",
    "\n",
    "test_df_kt['cal_sentiment'] = test_df_kt['processed'].apply(cal_sentiment)\n",
    "test_df_ws['cal_sentiment'] = test_df_ws['processed'].apply(cal_sentiment)\n",
    "all_df_kt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3485a4a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texts</th>\n",
       "      <th>targets</th>\n",
       "      <th>processed</th>\n",
       "      <th>cal_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25232</th>\n",
       "      <td>Nissan Silvia S14 หน้าหมูที่เท่ห์ไม่เหมือนใคร ...</td>\n",
       "      <td>pos</td>\n",
       "      <td>[nissan, silvia, s, 14, หน้า, หมู, ที่, เท่ห์,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11150</th>\n",
       "      <td>ไปดูหนังกันมั้ยจ้ะวิ</td>\n",
       "      <td>neu</td>\n",
       "      <td>[ไปดู, หนัง, กัน, มั้ย, จ้ะ, วิ]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23799</th>\n",
       "      <td>ดัน.....ๆครับ....เอาด้วย</td>\n",
       "      <td>pos</td>\n",
       "      <td>[ดัน, ., xxrep, ๆ, ครับ, ., xxrep, เอา, ด้วย]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19312</th>\n",
       "      <td>คืนนี้ Chang Carnival World of Illusion ที่ลาน...</td>\n",
       "      <td>neu</td>\n",
       "      <td>[คืนนี้, chang, carnival, world, of, illusion,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12291</th>\n",
       "      <td>จัดไห้น้าหน่อยดิตังออกอ่ะ</td>\n",
       "      <td>neu</td>\n",
       "      <td>[จัด, ไห้, น้า, หน่อย, ดิ, ตัง, ออก, อ่ะ]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   texts targets  \\\n",
       "25232  Nissan Silvia S14 หน้าหมูที่เท่ห์ไม่เหมือนใคร ...     pos   \n",
       "11150                               ไปดูหนังกันมั้ยจ้ะวิ     neu   \n",
       "23799                           ดัน.....ๆครับ....เอาด้วย     pos   \n",
       "19312  คืนนี้ Chang Carnival World of Illusion ที่ลาน...     neu   \n",
       "12291                          จัดไห้น้าหน่อยดิตังออกอ่ะ     neu   \n",
       "\n",
       "                                               processed  cal_sentiment  \n",
       "25232  [nissan, silvia, s, 14, หน้า, หมู, ที่, เท่ห์,...              0  \n",
       "11150                   [ไปดู, หนัง, กัน, มั้ย, จ้ะ, วิ]              0  \n",
       "23799      [ดัน, ., xxrep, ๆ, ครับ, ., xxrep, เอา, ด้วย]              0  \n",
       "19312  [คืนนี้, chang, carnival, world, of, illusion,...              0  \n",
       "12291          [จัด, ไห้, น้า, หน่อย, ดิ, ตัง, ออก, อ่ะ]              0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df_ws.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e0bb454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17576646507820368 0.11384882038831483\n"
     ]
    }
   ],
   "source": [
    "# see the correlation between the above computational sentiment and human ratings \n",
    "# for train set\n",
    "all_df_kt['targets_codes'] = all_df_kt['vote'].astype('category').cat.codes\n",
    "all_df_ws['targets_codes'] = all_df_ws['targets'].astype('category').cat.codes\n",
    "\n",
    "# for test set\n",
    "test_df_kt['targets_codes'] = test_df_kt['vote'].astype('category').cat.codes\n",
    "test_df_ws['targets_codes'] = test_df_ws['targets'].astype('category').cat.codes\n",
    "\n",
    "print(all_df_kt['targets_codes'].corr(all_df_kt['cal_sentiment']), \\\n",
    "      all_df_ws['targets_codes'].corr(all_df_ws['cal_sentiment']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9787b7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create word count feature\n",
    "all_df_kt = all_df_kt.rename(columns = {'sent_length':'wc'})\n",
    "all_df_ws['wc'] =  all_df_ws['processed'].map(len)\n",
    "\n",
    "test_df_kt = all_df_kt.rename(columns = {'sent_length':'wc'})\n",
    "test_df_ws['wc'] =  test_df_ws['processed'].map(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77849de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.01440540432656602 -0.0728353149652106\n"
     ]
    }
   ],
   "source": [
    "# see if the a correlation with word count\n",
    "print(all_df_kt['targets_codes'].corr(all_df_kt['wc']), \\\n",
    "      all_df_ws['targets_codes'].corr(all_df_ws['wc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "350cbcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_df_kt.to_csv('all_df_kt.csv',  index=False)\n",
    "#test_df_kt.to_csv('test_df_kt.csv',  index=False)\n",
    "\n",
    "all_df_ws.to_csv('all_df_ws.csv', index=False)\n",
    "test_df_ws.to_csv('test_df_ws.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c6e20f",
   "metadata": {},
   "source": [
    "## Bag of words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b2eb3e4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bow2_fit_ws_kt.pkl']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import joblib\n",
    "\n",
    "# BOW with unigram and bigrams\n",
    "bow1 = CountVectorizer(ngram_range=(1, 1))\n",
    "bow2 = CountVectorizer(ngram_range=(2, 2))\n",
    "\n",
    "bow1_fit_ws = bow1.fit(all_df_ws['processed'].apply(str))\n",
    "bow1_fit_ws_kt = bow1_fit_ws.fit(all_df_kt['processed'].apply(str))\n",
    "\n",
    "bow2_fit_ws = bow2.fit(all_df_ws['processed'].apply(str))\n",
    "bow2_fit_ws_kt = bow2_fit_ws.fit(all_df_kt['processed'].apply(str))\n",
    "\n",
    "joblib.dump(bow1_fit_ws, 'bow1_fit_ws.pkl')\n",
    "joblib.dump(bow1_fit_ws_kt, 'bow1_fit_ws_kt.pkl')\n",
    "joblib.dump(bow2_fit_ws, 'bow2_fit_ws.pkl')\n",
    "joblib.dump(bow2_fit_ws_kt, 'bow2_fit_ws_kt.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d7b552",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f3232d40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tfidf2_fit_ws_kt.pkl']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf1 = TfidfVectorizer(ngram_range=(1, 1), min_df=20, sublinear_tf=True)\n",
    "tfidf2 = TfidfVectorizer(ngram_range=(2, 2), min_df=20, sublinear_tf=True)\n",
    "\n",
    "tfidf1_fit_ws = tfidf1.fit(all_df_ws['processed'].apply(str))\n",
    "tfidf1_fit_ws_kt = tfidf1_fit_ws.fit(all_df_kt['processed'].apply(str))\n",
    "\n",
    "tfidf2_fit_ws = tfidf2.fit(all_df_ws['processed'].apply(str))\n",
    "tfidf2_fit_ws_kt = tfidf2_fit_ws.fit(all_df_kt['processed'].apply(str))\n",
    "\n",
    "joblib.dump(tfidf1_fit_ws, 'tfidf1_fit_ws.pkl')\n",
    "joblib.dump(tfidf1_fit_ws_kt, 'tfidf1_fit_ws_kt.pkl')\n",
    "joblib.dump(tfidf2_fit_ws, 'tfidf2_fit_ws.pkl')\n",
    "joblib.dump(tfidf2_fit_ws_kt, 'tfidf2_fit_ws_kt.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2867680",
   "metadata": {},
   "source": [
    "## Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8551fe6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "\n",
    "class TfidfEmbeddingVectorizer(object):\n",
    "    def __init__(self, model):\n",
    "        w2v = {w: vec for w, vec in zip(model.wv.index_to_key, model.wv.vectors)}\n",
    "        self.word2vec = w2v\n",
    "        self.word2weight = None\n",
    "        self.dim = model.vector_size\n",
    "    \n",
    "    \n",
    "    def fit(self, X):\n",
    "        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "        tfidf.fit(X)\n",
    "        max_idf = max(tfidf.idf_)\n",
    "        self.word2weight = defaultdict(\n",
    "            lambda: max_idf,\n",
    "            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "                np.mean([self.word2vec[w] * self.word2weight[w]\n",
    "                         for w in words if w in self.word2vec] or\n",
    "                        [np.zeros(self.dim)], axis=0)\n",
    "                for words in X\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "856dd1fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21389"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from pythainlp import word_vector\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "\n",
    "# create word2vec models and intersect with the thai wiki pretrained one.\n",
    "\n",
    "w2v_model_ws = Word2Vec(vector_size=300, min_count=1, window = 5, workers=4)\n",
    "w2v_model_ws.build_vocab(all_df_ws['processed'])\n",
    "total_examples = w2v_model_ws.corpus_count\n",
    "total_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ee904ec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51358"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_thwiki = word_vector.get_model()\n",
    "w2v_model_ws.build_vocab(w2v_thwiki.index_to_key, update=True)\n",
    "w2v_model_ws.wv.vectors_lockf = np.ones(len(w2v_model_ws.wv))\n",
    "w2v_model_ws.wv.intersect_word2vec_format('thai2vec.bin', binary=True, lockf=1.0)\n",
    "total_examples = w2v_model_ws.corpus_count\n",
    "total_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "af267111",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42542754, 47151500)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model_ws.train(all_df_ws['processed'], total_examples=total_examples, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "872cbfb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ข้าวมันไก่', 0.39422789216041565),\n",
       " ('กุ้ง', 0.37291258573532104),\n",
       " ('ไ่ข่', 0.36208683252334595),\n",
       " ('ไข่เค็ม', 0.3498747944831848),\n",
       " ('🍣', 0.3467874228954315),\n",
       " ('ลูกชิ้น', 0.34412676095962524),\n",
       " ('แหนม', 0.34077826142311096),\n",
       " ('หมูแดง', 0.3383786678314209),\n",
       " ('ใส้', 0.32756781578063965),\n",
       " ('ยำ', 0.32196688652038574)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model_ws.wv.most_similar(\"บะหมี่\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2a18ee4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill # we use dill instead of joblim because the lambda and dependecie in class TfidfEmbeddingVectorizer\n",
    "# now we have our w2v mmodel, we need to convert single vector\n",
    "w2v_tfidf_emb_ws = TfidfEmbeddingVectorizer(w2v_model_ws)\n",
    "w2v_tifdf_fit_ws = w2v_tfidf_emb_ws.fit(all_df_ws['processed'])\n",
    "dill.dump(w2v_tifdf_fit_ws, open('w2v-tfidf_fit_ws.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "df1f1c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train on new words from kt4.0 corpus\n",
    "w2v_model_ws.build_vocab(all_df_kt['processed'], update = True)\n",
    "total_examples = w2v_model_ws.corpus_count\n",
    "total_examples\n",
    "w2v_model_ws.train(all_df_kt['processed'], total_examples=total_examples, epochs=100)\n",
    "\n",
    "w2v_tfidf_emb_ws_kt = TfidfEmbeddingVectorizer(w2v_model_ws)\n",
    "w2v_tifdf_fit_ws_kt = w2v_tfidf_emb_ws_kt.fit(all_df_ws['processed'])\n",
    "dill.dump(w2v_tifdf_fit_ws_kt, open('w2v-tfidf_fit_ws_kt.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "08d648bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ย่าง', 0.3160748779773712),\n",
       " ('ไส้กรอก', 0.2988615930080414),\n",
       " ('ไก่บ้าน', 0.28489479422569275),\n",
       " ('ชีส', 0.28290531039237976),\n",
       " ('กะทิ', 0.2817407250404358),\n",
       " ('ซุป', 0.2725837826728821),\n",
       " ('🍚', 0.272504061460495),\n",
       " ('แมคโคร', 0.261968731880188),\n",
       " ('หัวไชเท้า', 0.2597872018814087),\n",
       " ('เค้ม', 0.2591487765312195)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model_ws.wv.most_similar(\"บะหมี่\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2102229e",
   "metadata": {},
   "source": [
    "## POS_Tag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0387c5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(x):\n",
    "    l = list(sum(x, ()))\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c257e9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythainlp.tag import pos_tag_sents\n",
    "\n",
    "# we used a POS tag with the orchid_ud feature that represented a type of word in a sentence in one-hot vector form\n",
    "# flatten the list of tuple in series was applied for feature vectors\n",
    "all_df_ws['POSTags'] = pos_tag_sents(all_df_ws['texts'].apply(str).apply(process_thai).tolist(), \\\n",
    "                                     corpus='orchid_ud')\n",
    "all_df_ws['POSTags'] = all_df_ws['POSTags'].apply(flatten)\n",
    "\n",
    "# TODO: concate word with pos (มัน_ADV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9f3755ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pos2_fit_ws_kt.pkl']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos1 = CountVectorizer(ngram_range=(1, 1))\n",
    "pos2 = CountVectorizer(ngram_range=(2, 2))\n",
    "\n",
    "pos1_fit_ws = pos1.fit(all_df_ws['processed'].apply(str))\n",
    "pos1_fit_ws_kt = pos1_fit_ws.fit(all_df_kt['processed'].apply(str))\n",
    "\n",
    "pos2_fit_ws = pos2.fit(all_df_ws['processed'].apply(str))\n",
    "pos2_fit_ws_kt = pos2_fit_ws.fit(all_df_kt['processed'].apply(str))\n",
    "\n",
    "joblib.dump(pos1_fit_ws, 'pos1_fit_ws.pkl')\n",
    "joblib.dump(pos1_fit_ws_kt, 'pos1_fit_ws_kt.pkl')\n",
    "joblib.dump(pos2_fit_ws, 'pos2_fit_ws.pkl')\n",
    "joblib.dump(pos2_fit_ws_kt, 'pos2_fit_ws_kt.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f1e6f2",
   "metadata": {},
   "source": [
    "## Train-valid split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "88c9ea8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_ws, valid_df_ws = train_test_split(all_df_ws, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "abb484ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((18180, 2395), (3209, 2395), (5348, 2395))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_train_ws = tfidf2_fit_ws.transform(train_df_ws['texts'].apply(str))\n",
    "text_valid_ws = tfidf2_fit_ws.transform(valid_df_ws['texts'].apply(str))\n",
    "text_test_ws = tfidf2_fit_ws.transform(test_df_ws['texts'].apply(str))\n",
    "text_train_ws.shape, text_valid_ws.shape, text_test_ws.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a39ac867",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neu    0.548146\n",
       "neg    0.246183\n",
       "pos    0.180118\n",
       "q      0.025553\n",
       "Name: targets, dtype: float64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_df_ws.targets.value_counts() / valid_df_ws.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a2917489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.07882556] [0.75324233]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((18180, 1), (3209, 1))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The calculated sentiment and word count features might be useful, so we concat them to the text feature\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaler_fit = scaler.fit(np.asarray(all_df_ws['cal_sentiment']).reshape(-1, 1))\n",
    "print(scaler_fit.mean_, scaler_fit.var_)\n",
    "\n",
    "cal_sent_train = scaler_fit.transform(np.asarray(train_df_ws['cal_sentiment']).reshape(-1, 1).astype(float))\n",
    "cal_sent_valid = scaler_fit.transform(np.asarray(valid_df_ws['cal_sentiment']).reshape(-1, 1).astype(float))\n",
    "cal_sent_test = scaler_fit.transform(np.asarray(test_df_ws['cal_sentiment']).reshape(-1, 1).astype(float))\n",
    "cal_sent_train.shape, cal_sent_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d3ad41d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22.04474262] [1169.8183263]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((18180, 1), (3209, 1))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler_fit = scaler.fit(np.asarray(all_df_ws['wc']).reshape(-1, 1))\n",
    "print(scaler_fit.mean_, scaler_fit.var_)\n",
    "\n",
    "num_train = scaler_fit.transform(np.asarray(train_df_ws['wc']).reshape(-1, 1).astype(float))\n",
    "num_valid = scaler_fit.transform(np.asarray(valid_df_ws['wc']).reshape(-1, 1).astype(float))\n",
    "num_test = scaler_fit.transform(np.asarray(test_df_ws['wc']).reshape(-1, 1).astype(float))\n",
    "num_train.shape, num_valid.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3159c026",
   "metadata": {},
   "source": [
    "## Test the extracted features with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4a96c00b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((18180, 2397), (3209, 2397))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = train_df_ws['targets']\n",
    "y_valid = valid_df_ws['targets']\n",
    "\n",
    "# concat text vector and generated feature vectors\n",
    "X_train = np.concatenate([text_train_ws.toarray(), cal_sent_train, num_train], axis=1)\n",
    "X_valid = np.concatenate([text_valid_ws.toarray(), cal_sent_valid, num_valid], axis=1)\n",
    "X_test = np.concatenate([text_test_ws.toarray(), cal_sent_test, num_test], axis=1)\n",
    "\n",
    "#X_train = text_train_ws.toarray()\n",
    "#X_valid = text_valid_ws.toarray()\n",
    "#X_test = text_test_ws.toarray()\n",
    "\n",
    "#X_train = text_train_ws\n",
    "#X_valid  = text_valid_ws\n",
    "#X_test = text_test_ws\n",
    "\n",
    "X_train.shape, X_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fc1413dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5687129947023996"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quick test with out cv\n",
    "#fit logistic regression models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(C=2., penalty=\"l2\", solver=\"liblinear\", dual=False, multi_class=\"ovr\")\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_valid, y_valid)\n",
    "#y_pred = model.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f340fa9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.unique(y_pred) \n",
    "# this might cause from label 'q' is not present in the y_pred (due to the severe imbalance class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d6282242",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
    "def build_model(model):\n",
    "    scores = (cross_val_score(model, X_train, y_train, cv = 5).mean())\n",
    "    model = model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_valid)\n",
    "    acc_sc = accuracy_score(y_valid, y_pred)\n",
    "    pre_sc = precision_score(y_valid, y_pred, average='weighted')\n",
    "    rec_sc = recall_score(y_valid, y_pred, average='weighted')\n",
    "    f1_sc = f1_score(y_valid, y_pred, average='weighted')\n",
    "    print('Accuracy :',acc_sc)\n",
    "    print('Confusion Matrix :\\n', confusion_matrix(y_valid, y_pred))\n",
    "    print('Precision :', pre_sc)\n",
    "    print('Recall :', rec_sc)\n",
    "    print('F1-score :', f1_sc)\n",
    "    print('Classification Report :\\n', classification_report(y_valid, y_pred))\n",
    "    print('Average accuracy of k-fold (5-fold) :', scores ,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3e120d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.5687129947023996\n",
      "Confusion Matrix :\n",
      " [[ 112  656   22    0]\n",
      " [  65 1677   17    0]\n",
      " [  20  522   36    0]\n",
      " [   0   80    2    0]]\n",
      "Precision : 0.5373724095592892\n",
      "Recall : 0.5687129947023996\n",
      "F1-score : 0.467336708171298\n",
      "Classification Report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.57      0.14      0.23       790\n",
      "         neu       0.57      0.95      0.71      1759\n",
      "         pos       0.47      0.06      0.11       578\n",
      "           q       0.00      0.00      0.00        82\n",
      "\n",
      "    accuracy                           0.57      3209\n",
      "   macro avg       0.40      0.29      0.26      3209\n",
      "weighted avg       0.54      0.57      0.47      3209\n",
      "\n",
      "Average accuracy of k-fold (5-fold) : 0.5613861386138613 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pree\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Pree\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Pree\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Pree\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "build_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "12a8761b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.unique(y_pred) \n",
    "# this might cause from label 'q' is not present in the y_pred (due to the severe imbalance class)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
