{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "139d4b2f",
   "metadata": {},
   "source": [
    "### Feature extractions\n",
    "\n",
    "This script extracted features from two sentimental corpora, kt4.0 (ours) and wisesight. By training from kt4.0 corpus, we expect to see an improvement in the wisesight corpus' classification performance.\n",
    "\n",
    "For both datasets, random stratify hold-out was performed with 80:20 ratio for train and test set. Next, several feature extraction methods were applied and output as a joblib objects as follows:  \n",
    "\n",
    "* Bag of words for unigram and bigrams\n",
    "* TF-IDF for unigram and bigrams\n",
    "* Word2Vec pretrained from Thai wiki. (100 dimension)\n",
    "* POS_tagging with flatten dataframe for unigram and bigrams\n",
    "* Dictionary-based with Thai positive and negative words  for unigram and bigrams\n",
    "\n",
    "The output vectors will be carried out in the next experiment.  \n",
    "pree.t@cmu.ac.th  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a978d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pythainlp\n",
    "from pythainlp.ulmfit import process_thai\n",
    "\n",
    "# for visualize\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams['font.family'] = 'tahoma'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47e606a",
   "metadata": {},
   "source": [
    "## Load original datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fe523d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60081, 14), (26737, 2))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.path.dirname(os.getcwd())\n",
    "\n",
    "data_path_kt = os.path.dirname(os.getcwd()) + '\\\\data\\kt4.0\\\\'\n",
    "data_path_ws = os.path.dirname(os.getcwd()) + '\\\\data\\wisesight\\\\'\n",
    "model_path = os.path.dirname(os.getcwd()) + '\\\\model\\\\'\n",
    "df_kt = pd.read_csv(data_path_kt + 'pantip_cleaned_1.csv')\n",
    "\n",
    "# we use the original wisesight corpus and reconstruct a new dataframe\n",
    "texts = []\n",
    "targets = []\n",
    "\n",
    "with open(str(data_path_ws) + '\\\\' + 'neg.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        texts.append(line.strip())\n",
    "        targets.append('neg')\n",
    "\n",
    "with open(str(data_path_ws) + '\\\\' + 'neu.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        texts.append(line.strip())\n",
    "        targets.append('neu')\n",
    "\n",
    "with open(str(data_path_ws) + '\\\\' + 'pos.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        texts.append(line.strip())\n",
    "        targets.append('pos')\n",
    "\n",
    "with open(str(data_path_ws) + '\\\\' + 'q.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        texts.append(line.strip())\n",
    "        targets.append('q')\n",
    "        \n",
    "df_ws = pd.DataFrame({'texts': texts, 'targets': targets})\n",
    "df_ws.to_csv(os.path.dirname(os.getcwd()) + '\\\\' + 'wisesight.csv', index=False)\n",
    "df_kt.shape, df_ws.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c128ab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>post_date</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_name</th>\n",
       "      <th>text</th>\n",
       "      <th>tag</th>\n",
       "      <th>emotion</th>\n",
       "      <th>length</th>\n",
       "      <th>num_sent</th>\n",
       "      <th>sent_length</th>\n",
       "      <th>label</th>\n",
       "      <th>label_1</th>\n",
       "      <th>label_2</th>\n",
       "      <th>vote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39839097</td>\n",
       "      <td>2020-04-25 13:24:00</td>\n",
       "      <td>https://pantip.com/profile/5798163</td>\n",
       "      <td>สมาชิกหมายเลข 5798163</td>\n",
       "      <td>[CR] แปังพัฟคุมมัน จัดเต็มเนื้อบางเบา</td>\n",
       "      <td>เครื่องสำอาง</td>\n",
       "      <td>ถูกใจ 0 ขำกลิ้ง 0 หลงรัก 0 ซึ้ง 0 สยอง 0 ทึ่ง 0</td>\n",
       "      <td>36</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39839097</td>\n",
       "      <td>2020-04-25 13:24:00</td>\n",
       "      <td>https://pantip.com/profile/5798163</td>\n",
       "      <td>สมาชิกหมายเลข 5798163</td>\n",
       "      <td>ไม่อุดตัน แต่ปกปิดแน่นมาก</td>\n",
       "      <td>เครื่องสำอาง</td>\n",
       "      <td>ถูกใจ 0 ขำกลิ้ง 0 หลงรัก 0 ซึ้ง 0 สยอง 0 ทึ่ง 0</td>\n",
       "      <td>36</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39839097</td>\n",
       "      <td>2020-04-25 13:24:00</td>\n",
       "      <td>https://pantip.com/profile/5798163</td>\n",
       "      <td>สมาชิกหมายเลข 5798163</td>\n",
       "      <td>รีวิวแป้ง Lady Audrey Ready All Day จ้า</td>\n",
       "      <td>เครื่องสำอาง</td>\n",
       "      <td>ถูกใจ 0 ขำกลิ้ง 0 หลงรัก 0 ซึ้ง 0 สยอง 0 ทึ่ง 0</td>\n",
       "      <td>36</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39838736</td>\n",
       "      <td>2020-04-25 10:52:00</td>\n",
       "      <td>https://pantip.com/profile/5730006</td>\n",
       "      <td>สมาชิกหมายเลข 5730006</td>\n",
       "      <td>ขอบตาดำมากค่ะ คอร์เล็คเตอร์ก็เอาไม่อยู่</td>\n",
       "      <td>เครื่องสำอาง</td>\n",
       "      <td>ถูกใจ 0 ขำกลิ้ง 0 หลงรัก 0 ซึ้ง 0 สยอง 0 ทึ่ง 0</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>39837384</td>\n",
       "      <td>2020-04-24 20:39:00</td>\n",
       "      <td>https://pantip.com/profile/4975838</td>\n",
       "      <td>สมาชิกหมายเลข 4975838</td>\n",
       "      <td>เอาaloe Vera แช่ตู้เย็น จนกลายเป็นน้ำแข็ง</td>\n",
       "      <td>เครื่องสำอาง</td>\n",
       "      <td>ถูกใจ 0 ขำกลิ้ง 0 หลงรัก 0 ซึ้ง 0 สยอง 0 ทึ่ง 0</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>neu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>39838990</td>\n",
       "      <td>2020-04-25 12:36:00</td>\n",
       "      <td>https://pantip.com/profile/5655853</td>\n",
       "      <td>chdewxx</td>\n",
       "      <td>[SR] ไอเทม #เซรั่มสิว ลดสิว สิวอุดตัน สิวผด บำ...</td>\n",
       "      <td>เครื่องสำอาง</td>\n",
       "      <td>ถูกใจ 0 ขำกลิ้ง 0 หลงรัก 0 ซึ้ง 0 สยอง 0 ทึ่ง 0</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>39838619</td>\n",
       "      <td>2020-04-25 10:01:00</td>\n",
       "      <td>https://pantip.com/profile/5656639</td>\n",
       "      <td>คูจองยอนและวีรยา</td>\n",
       "      <td>รบกวนสาวๆช่วยแนะนำสกินแคร์ ที่ช่วยให้ผิวหน้าขา...</td>\n",
       "      <td>เครื่องสำอาง</td>\n",
       "      <td>ถูกใจ 0 ขำกลิ้ง 0 หลงรัก 0 ซึ้ง 0 สยอง 0 ทึ่ง 0</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>39837266</td>\n",
       "      <td>2020-04-24 19:58:00</td>\n",
       "      <td>https://pantip.com/profile/632132</td>\n",
       "      <td>หมูกลมอารมณ์ดี</td>\n",
       "      <td>ทดลองใช้ แครอทวิตซีหน้าใส</td>\n",
       "      <td>เครื่องสำอาง</td>\n",
       "      <td>ถูกใจ 0 ขำกลิ้ง 0 หลงรัก 0 ซึ้ง 0 สยอง 0 ทึ่ง 0</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>39837266</td>\n",
       "      <td>2020-04-24 19:58:00</td>\n",
       "      <td>https://pantip.com/profile/632132</td>\n",
       "      <td>หมูกลมอารมณ์ดี</td>\n",
       "      <td>ใน 1 สัปดาห์</td>\n",
       "      <td>เครื่องสำอาง</td>\n",
       "      <td>ถูกใจ 0 ขำกลิ้ง 0 หลงรัก 0 ซึ้ง 0 สยอง 0 ทึ่ง 0</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>neu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>39835926</td>\n",
       "      <td>2020-04-24 12:03:00</td>\n",
       "      <td>https://pantip.com/profile/3826851</td>\n",
       "      <td>สมาชิกหมายเลข 3826851</td>\n",
       "      <td>วิธีเลือก \"รองพื้น\" และ \"คอนซีลเลอร์\"</td>\n",
       "      <td>เครื่องสำอาง</td>\n",
       "      <td>ถูกใจ 0 ขำกลิ้ง 0 หลงรัก 0 ซึ้ง 0 สยอง 0 ทึ่ง 0</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    post_id            post_date                             user_id  \\\n",
       "0  39839097  2020-04-25 13:24:00  https://pantip.com/profile/5798163   \n",
       "1  39839097  2020-04-25 13:24:00  https://pantip.com/profile/5798163   \n",
       "2  39839097  2020-04-25 13:24:00  https://pantip.com/profile/5798163   \n",
       "3  39838736  2020-04-25 10:52:00  https://pantip.com/profile/5730006   \n",
       "4  39837384  2020-04-24 20:39:00  https://pantip.com/profile/4975838   \n",
       "5  39838990  2020-04-25 12:36:00  https://pantip.com/profile/5655853   \n",
       "6  39838619  2020-04-25 10:01:00  https://pantip.com/profile/5656639   \n",
       "7  39837266  2020-04-24 19:58:00   https://pantip.com/profile/632132   \n",
       "8  39837266  2020-04-24 19:58:00   https://pantip.com/profile/632132   \n",
       "9  39835926  2020-04-24 12:03:00  https://pantip.com/profile/3826851   \n",
       "\n",
       "               user_name                                               text  \\\n",
       "0  สมาชิกหมายเลข 5798163              [CR] แปังพัฟคุมมัน จัดเต็มเนื้อบางเบา   \n",
       "1  สมาชิกหมายเลข 5798163                          ไม่อุดตัน แต่ปกปิดแน่นมาก   \n",
       "2  สมาชิกหมายเลข 5798163            รีวิวแป้ง Lady Audrey Ready All Day จ้า   \n",
       "3  สมาชิกหมายเลข 5730006            ขอบตาดำมากค่ะ คอร์เล็คเตอร์ก็เอาไม่อยู่   \n",
       "4  สมาชิกหมายเลข 4975838          เอาaloe Vera แช่ตู้เย็น จนกลายเป็นน้ำแข็ง   \n",
       "5                chdewxx  [SR] ไอเทม #เซรั่มสิว ลดสิว สิวอุดตัน สิวผด บำ...   \n",
       "6       คูจองยอนและวีรยา  รบกวนสาวๆช่วยแนะนำสกินแคร์ ที่ช่วยให้ผิวหน้าขา...   \n",
       "7         หมูกลมอารมณ์ดี                          ทดลองใช้ แครอทวิตซีหน้าใส   \n",
       "8         หมูกลมอารมณ์ดี                                       ใน 1 สัปดาห์   \n",
       "9  สมาชิกหมายเลข 3826851              วิธีเลือก \"รองพื้น\" และ \"คอนซีลเลอร์\"   \n",
       "\n",
       "            tag                                          emotion  length  \\\n",
       "0  เครื่องสำอาง  ถูกใจ 0 ขำกลิ้ง 0 หลงรัก 0 ซึ้ง 0 สยอง 0 ทึ่ง 0      36   \n",
       "1  เครื่องสำอาง  ถูกใจ 0 ขำกลิ้ง 0 หลงรัก 0 ซึ้ง 0 สยอง 0 ทึ่ง 0      36   \n",
       "2  เครื่องสำอาง  ถูกใจ 0 ขำกลิ้ง 0 หลงรัก 0 ซึ้ง 0 สยอง 0 ทึ่ง 0      36   \n",
       "3  เครื่องสำอาง  ถูกใจ 0 ขำกลิ้ง 0 หลงรัก 0 ซึ้ง 0 สยอง 0 ทึ่ง 0      15   \n",
       "4  เครื่องสำอาง  ถูกใจ 0 ขำกลิ้ง 0 หลงรัก 0 ซึ้ง 0 สยอง 0 ทึ่ง 0      11   \n",
       "5  เครื่องสำอาง  ถูกใจ 0 ขำกลิ้ง 0 หลงรัก 0 ซึ้ง 0 สยอง 0 ทึ่ง 0      29   \n",
       "6  เครื่องสำอาง  ถูกใจ 0 ขำกลิ้ง 0 หลงรัก 0 ซึ้ง 0 สยอง 0 ทึ่ง 0      23   \n",
       "7  เครื่องสำอาง  ถูกใจ 0 ขำกลิ้ง 0 หลงรัก 0 ซึ้ง 0 สยอง 0 ทึ่ง 0      14   \n",
       "8  เครื่องสำอาง  ถูกใจ 0 ขำกลิ้ง 0 หลงรัก 0 ซึ้ง 0 สยอง 0 ทึ่ง 0      14   \n",
       "9  เครื่องสำอาง  ถูกใจ 0 ขำกลิ้ง 0 หลงรัก 0 ซึ้ง 0 สยอง 0 ทึ่ง 0      14   \n",
       "\n",
       "   num_sent  sent_length  label  label_1  label_2 vote  \n",
       "0         3           14      2        2        2  pos  \n",
       "1         3            8      2        2        2  pos  \n",
       "2         3           14      2        2        1  pos  \n",
       "3         2           13      1        3        3  neg  \n",
       "4         1           11      1        1        3  neu  \n",
       "5         1           29      2        2        2  pos  \n",
       "6         1           23      2        2        1  pos  \n",
       "7         2            9      1        2        2  pos  \n",
       "8         2            5      1        1        1  neu  \n",
       "9         1           14      2        2        2  pos  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_kt.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d84766f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>length</th>\n",
       "      <th>num_sent</th>\n",
       "      <th>sent_length</th>\n",
       "      <th>label</th>\n",
       "      <th>label_1</th>\n",
       "      <th>label_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6.008100e+04</td>\n",
       "      <td>60081.000000</td>\n",
       "      <td>60081.000000</td>\n",
       "      <td>60081.000000</td>\n",
       "      <td>60081.000000</td>\n",
       "      <td>60081.000000</td>\n",
       "      <td>60081.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.964936e+07</td>\n",
       "      <td>116.994574</td>\n",
       "      <td>8.502172</td>\n",
       "      <td>13.978329</td>\n",
       "      <td>1.577304</td>\n",
       "      <td>1.362644</td>\n",
       "      <td>1.662156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.559919e+05</td>\n",
       "      <td>118.647716</td>\n",
       "      <td>7.575442</td>\n",
       "      <td>12.083572</td>\n",
       "      <td>0.777527</td>\n",
       "      <td>0.639271</td>\n",
       "      <td>0.800034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.917283e+07</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.958755e+07</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.968929e+07</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.976947e+07</td>\n",
       "      <td>159.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.983970e+07</td>\n",
       "      <td>499.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>301.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            post_id        length      num_sent   sent_length         label  \\\n",
       "count  6.008100e+04  60081.000000  60081.000000  60081.000000  60081.000000   \n",
       "mean   3.964936e+07    116.994574      8.502172     13.978329      1.577304   \n",
       "std    1.559919e+05    118.647716      7.575442     12.083572      0.777527   \n",
       "min    3.917283e+07      3.000000      1.000000      3.000000      1.000000   \n",
       "25%    3.958755e+07     31.000000      3.000000      6.000000      1.000000   \n",
       "50%    3.968929e+07     72.000000      6.000000     10.000000      1.000000   \n",
       "75%    3.976947e+07    159.000000     11.000000     17.000000      2.000000   \n",
       "max    3.983970e+07    499.000000     44.000000    301.000000      3.000000   \n",
       "\n",
       "            label_1       label_2  \n",
       "count  60081.000000  60081.000000  \n",
       "mean       1.362644      1.662156  \n",
       "std        0.639271      0.800034  \n",
       "min        1.000000      1.000000  \n",
       "25%        1.000000      1.000000  \n",
       "50%        1.000000      1.000000  \n",
       "75%        2.000000      2.000000  \n",
       "max        3.000000      3.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_kt.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5ec5386",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texts</th>\n",
       "      <th>targets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>☹️</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>😔</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>😞</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>😥</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>รำ</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Noๆ</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Rip</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>T_T</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>กาก</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>โกง</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  texts targets\n",
       "0    ☹️     neg\n",
       "1     😔     neg\n",
       "2     😞     neg\n",
       "3     😥     neg\n",
       "4    รำ     neg\n",
       "5   Noๆ     neg\n",
       "6   Rip     neg\n",
       "7   T_T     neg\n",
       "8   กาก     neg\n",
       "9   โกง     neg"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ws.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7077f068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texts</th>\n",
       "      <th>targets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>26737</td>\n",
       "      <td>26737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>26713</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>อุดรมีไหมค่ะ</td>\n",
       "      <td>neu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>2</td>\n",
       "      <td>14561</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               texts targets\n",
       "count          26737   26737\n",
       "unique         26713       4\n",
       "top     อุดรมีไหมค่ะ     neu\n",
       "freq               2   14561"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ws.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75693314",
   "metadata": {},
   "source": [
    "# Train-test split 80/20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c67990d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((48064, 14), (12017, 14))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random stratified split train and test set 80/20\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate\n",
    "all_df_kt, test_df_kt = train_test_split(df_kt, test_size=0.2, random_state=42, shuffle = True)\n",
    "all_df_kt.shape, test_df_kt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c7d3bcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neu    0.632136\n",
       "pos    0.206620\n",
       "neg    0.161243\n",
       "Name: vote, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# class distribution\n",
    "all_df_kt.vote.value_counts() / all_df_kt.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b63ee3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((21389, 2), (5348, 2))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df_ws, test_df_ws = train_test_split(df_ws, test_size=0.2, random_state=42, shuffle = True)\n",
    "all_df_ws.shape, test_df_ws.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae548ccd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neu    0.544860\n",
       "neg    0.253588\n",
       "pos    0.179345\n",
       "q      0.022208\n",
       "Name: targets, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# class distribution\n",
    "all_df_ws.targets.value_counts() / all_df_ws.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82ef6037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean and word tokenize\n",
    "all_df_kt['processed'] = all_df_kt['text'].apply(str).apply(process_thai)\n",
    "test_df_kt['processed'] = test_df_kt['text'].apply(str).apply(process_thai)\n",
    "\n",
    "all_df_ws['processed'] = all_df_ws['texts'].apply(str).apply(process_thai)\n",
    "test_df_ws['processed'] = test_df_ws['texts'].apply(str).apply(process_thai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "350cbcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df_kt['vote'].to_csv(os.path.dirname(os.getcwd()) + '\\\\' + 'train_label_kt.csv',  index=False)\n",
    "# test_df_kt['vote'].to_csv(os.path.dirname(os.getcwd()) + '\\\\' + 'test_label_kt.csv',  index=False)\n",
    "\n",
    "all_df_ws['targets'].to_csv(os.path.dirname(os.getcwd()) + '\\\\' + 'train_label_ws.csv', index=False)\n",
    "# test_df_ws['targets'].to_csv(os.path.dirname(os.getcwd()) + '\\\\' + 'test_label_ws.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c6e20f",
   "metadata": {},
   "source": [
    "## Bag of words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2eb3e4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\Pree\\\\Thai_SA_journal\\\\model\\\\text_all_bow2_ws.joblib']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import joblib\n",
    "\n",
    "# BOW with unigram and bigrams\n",
    "bow1 = CountVectorizer(ngram_range=(1, 1))\n",
    "bow2 = CountVectorizer(ngram_range=(2, 2))\n",
    "\n",
    "# fit kt and transfrom on train set\n",
    "bow1_fit_kt = bow1.fit(all_df_kt['processed'].apply(str))\n",
    "text_all_bow1_kt = bow1_fit_kt.transform(all_df_kt['processed'].apply(str))\n",
    "text_all_bow1_ws = bow1_fit_kt.transform(all_df_ws['processed'].apply(str))\n",
    "\n",
    "bow2_fit_kt = bow2.fit(all_df_kt['processed'].apply(str))\n",
    "text_all_bow2_kt = bow2_fit_kt.transform(all_df_kt['processed'].apply(str))\n",
    "text_all_bow2_ws = bow2_fit_kt.transform(all_df_ws['processed'].apply(str))\n",
    "\n",
    "joblib.dump(text_all_bow1_kt, model_path+'text_all_bow1_kt.joblib')\n",
    "joblib.dump(text_all_bow1_ws, model_path+'text_all_bow1_ws.joblib')\n",
    "joblib.dump(text_all_bow2_kt, model_path+'text_all_bow2_kt.joblib')\n",
    "joblib.dump(text_all_bow2_ws, model_path+'text_all_bow2_ws.joblib')\n",
    "\n",
    "# bow1 = CountVectorizer(ngram_range=(1, 1))\n",
    "# bow2 = CountVectorizer(ngram_range=(2, 2))\n",
    "\n",
    "# # fit kt and transfrom on test set\n",
    "# bow1_fit_kt = bow1.fit(test_df_kt['processed'].apply(str))\n",
    "# text_test_bow1_kt = bow1_fit_kt.transform(test_df_kt['processed'].apply(str))\n",
    "# text_test_bow1_ws = bow1_fit_kt.transform(test_df_ws['processed'].apply(str))\n",
    "\n",
    "# bow2_fit_kt = bow2.fit(all_df_kt['processed'].apply(str))\n",
    "# text_test_bow2_kt = bow2_fit_kt.transform(test_df_kt['processed'].apply(str))\n",
    "# text_test_bow2_ws = bow2_fit_kt.transform(test_df_ws['processed'].apply(str))\n",
    "\n",
    "# joblib.dump(text_test_bow1_kt, model_path+'text_test_bow1_kt.joblib')\n",
    "# joblib.dump(text_test_bow1_ws, model_path+'text_test_bow1_ws.joblib')\n",
    "# joblib.dump(text_test_bow2_kt, model_path+'text_test_bow2_kt.joblib')\n",
    "# joblib.dump(text_test_bow2_ws, model_path+'text_test_bow2_ws.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d7b552",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3232d40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\Pree\\\\Thai_SA_journal\\\\model\\\\text_all_tfidf2_ws.joblib']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF-IDF with unigram and bigrams\n",
    "tfidf1 = TfidfVectorizer(ngram_range=(1, 1))\n",
    "tfidf2 = TfidfVectorizer(ngram_range=(2, 2))\n",
    "\n",
    "# fit kt and transfrom on train set\n",
    "tfidf1_fit_kt = tfidf1.fit(all_df_kt['processed'].apply(str))\n",
    "text_all_tfidf1_kt = tfidf1_fit_kt.transform(all_df_kt['processed'].apply(str))\n",
    "text_all_tfidf1_ws = tfidf1_fit_kt.transform(all_df_ws['processed'].apply(str))\n",
    "\n",
    "tfidf2_fit_kt = tfidf2.fit(all_df_kt['processed'].apply(str))\n",
    "text_all_tfidf2_kt = tfidf2_fit_kt.transform(all_df_kt['processed'].apply(str))\n",
    "text_all_tfidf2_ws = tfidf2_fit_kt.transform(all_df_ws['processed'].apply(str))\n",
    "\n",
    "joblib.dump(text_all_tfidf1_kt, model_path+'text_all_tfidf1_kt.joblib')\n",
    "joblib.dump(text_all_tfidf1_ws, model_path+'text_all_tfidf1_ws.joblib')\n",
    "joblib.dump(text_all_tfidf2_kt, model_path+'text_all_tfidf2_kt.joblib')\n",
    "joblib.dump(text_all_tfidf2_ws, model_path+'text_all_tfidf2_ws.joblib')\n",
    "\n",
    "# tfidf1 = TfidfVectorizer(ngram_range=(1, 1))\n",
    "# tfidf2 = TfidfVectorizer(ngram_range=(2, 2))\n",
    "\n",
    "# # fit kt and transfrom on test set\n",
    "# tfidf1_fit_kt = tfidf1.fit(test_df_kt['processed'].apply(str))\n",
    "# text_test_tfidf1_kt = tfidf1_fit_kt.transform(test_df_kt['processed'].apply(str))\n",
    "# text_test_tfidf1_ws = tfidf1_fit_kt.transform(test_df_ws['processed'].apply(str))\n",
    "\n",
    "# tfidf2_fit_kt = tfidf2.fit(all_df_kt['processed'].apply(str))\n",
    "# text_test_tfidf2_kt = tfidf2_fit_kt.transform(test_df_kt['processed'].apply(str))\n",
    "# text_test_tfidf2_ws = tfidf2_fit_kt.transform(test_df_ws['processed'].apply(str))\n",
    "\n",
    "# joblib.dump(text_test_tfidf1_kt, model_path+'text_test_tfidf1_kt.joblib')\n",
    "# joblib.dump(text_test_tfidf1_ws, model_path+'text_test_tfidf1_ws.joblib')\n",
    "# joblib.dump(text_test_tfidf2_kt, model_path+'text_test_tfidf2_kt.joblib')\n",
    "# joblib.dump(text_test_tfidf2_ws, model_path+'text_test_tfidf2_ws.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2867680",
   "metadata": {},
   "source": [
    "## Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8551fe6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "\n",
    "class TfidfEmbeddingVectorizer(object):\n",
    "    def __init__(self, model):\n",
    "        w2v = {w: vec for w, vec in zip(model.wv.index_to_key, model.wv.vectors)}\n",
    "        self.word2vec = w2v\n",
    "        self.word2weight = None\n",
    "        self.dim = model.vector_size\n",
    "    \n",
    "    \n",
    "    def fit(self, X):\n",
    "        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "        tfidf.fit(X)\n",
    "        max_idf = max(tfidf.idf_)\n",
    "        self.word2weight = defaultdict(\n",
    "            lambda: max_idf,\n",
    "            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "                np.mean([self.word2vec[w] * self.word2weight[w]\n",
    "                         for w in words if w in self.word2vec] or\n",
    "                        [np.zeros(self.dim)], axis=0)\n",
    "                for words in X\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "856dd1fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('สำเร็จรูป', 0.6994114518165588),\n",
       " ('กึ่ง', 0.6770331263542175),\n",
       " ('หัวไชเท้า', 0.6058409214019775),\n",
       " ('ไส้กรอก', 0.586573600769043),\n",
       " ('ดอง', 0.5776714086532593),\n",
       " ('ต้มยำ', 0.5749776363372803),\n",
       " ('คะน้า', 0.5620478391647339),\n",
       " ('ยำ', 0.5589373111724854),\n",
       " ('ขนม', 0.556253969669342),\n",
       " ('หมูสับ', 0.5471652150154114)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from pythainlp import word_vector\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "\n",
    "# create word2vec for kt corpus\n",
    "w2v_kt = Word2Vec(vector_size=100, min_count=1, window = 5, workers=4)\n",
    "w2v_kt.build_vocab(all_df_kt['processed'])\n",
    "total_examples = w2v_kt.corpus_count\n",
    "\n",
    "w2v_kt.train(all_df_kt['processed'], total_examples=total_examples, epochs=50)\n",
    "w2v_kt.wv.most_similar(\"บะหมี่\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "542624be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill # we use dill instead of joblim because the lambda and dependecie in class TfidfEmbeddingVectorizer\n",
    "# create embbed vector\n",
    "w2v_tfidf_emb_kt = TfidfEmbeddingVectorizer(w2v_kt)\n",
    "w2v_tifdf_fit_kt = w2v_tfidf_emb_kt.fit(all_df_kt['processed'])\n",
    "\n",
    "# transfrom on the trai and test set for both corpuses\n",
    "text_all_w2v_tifdf_kt = w2v_tifdf_fit_kt.transform(all_df_kt['processed'])\n",
    "text_all_w2v_tifdf_ws = w2v_tifdf_fit_kt.transform(all_df_ws['processed'])\n",
    "\n",
    "# text_test_w2v_tifdf_kt = w2v_tifdf_fit_kt.transform(test_df_kt['processed'])\n",
    "# text_test_w2v_tifdf_ws = w2v_tifdf_fit_kt.transform(test_df_ws['processed'])\n",
    "\n",
    "dill.dump(text_all_w2v_tifdf_kt, open(model_path+'text_all_w2v_tifdf_kt.dill', 'wb'))\n",
    "dill.dump(text_all_w2v_tifdf_ws, open(model_path+'text_all_w2v_tifdf_ws.dill', 'wb'))\n",
    "\n",
    "# dill.dump(text_test_w2v_tifdf_kt, open(model_path+'text_test_w2v_tifdf_kt.dill', 'wb'))\n",
    "# dill.dump(text_test_w2v_tifdf_ws, open(model_path+'text_test_w2v_tifdf_ws.dill', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee904ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: use pretrained from thai2fit\n",
    "#w2v_thwiki = word_vector.get_model()\n",
    "#w2v_model_ws.build_vocab(w2v_thwiki.index_to_key, update=True)\n",
    "#w2v_model_ws.wv.vectors_lockf = np.ones(len(w2v_model_ws.wv))\n",
    "#w2v_model_ws.wv.intersect_word2vec_format('thai2vec.bin', binary=True, lockf=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2102229e",
   "metadata": {},
   "source": [
    "## POS Tagging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0387c5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(x):\n",
    "    l = list(sum(x, ()))\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c257e9d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nissan',\n",
       " 'NOUN',\n",
       " 'silvia',\n",
       " 'NOUN',\n",
       " 's',\n",
       " 'NOUN',\n",
       " '14',\n",
       " 'NUM',\n",
       " 'หน้า',\n",
       " 'NOUN',\n",
       " 'หมู',\n",
       " 'NOUN',\n",
       " 'ที่',\n",
       " 'SCONJ',\n",
       " 'เท่ห์',\n",
       " 'VERB',\n",
       " 'ไม่',\n",
       " 'PART',\n",
       " 'เหมือน',\n",
       " 'VERB',\n",
       " 'ใคร',\n",
       " 'PRON',\n",
       " 'ความ',\n",
       " 'NOUN',\n",
       " 'เปรี้ยว',\n",
       " 'VERB',\n",
       " 'ของ',\n",
       " 'ADP',\n",
       " 'สปอร์ต',\n",
       " 'NOUN',\n",
       " 'คาร์',\n",
       " 'NOUN',\n",
       " 'ยุค',\n",
       " 'VERB',\n",
       " '90',\n",
       " 'NUM']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pythainlp.tag import pos_tag_sents\n",
    "# we used a POS tag with the orchid_ud feature that represented a type of word in a sentence in one-hot vector form\n",
    "# flatten the list of tuple in series was applied for feature vectors\n",
    "all_df_kt['POSTags'] = pos_tag_sents(all_df_kt['processed'].tolist(), corpus='orchid_ud')\n",
    "all_df_kt['POSTags'] = all_df_kt['POSTags'].apply(flatten)\n",
    "\n",
    "# test_df_kt['POSTags'] = pos_tag_sents(test_df_kt['processed'].tolist(), corpus='orchid_ud')\n",
    "# test_df_kt['POSTags'] = test_df_kt['POSTags'].apply(flatten)\n",
    "\n",
    "all_df_ws['POSTags'] = pos_tag_sents(all_df_ws['processed'].tolist(), corpus='orchid_ud')\n",
    "all_df_ws['POSTags'] = all_df_ws['POSTags'].apply(flatten)\n",
    "\n",
    "# test_df_ws['POSTags'] = pos_tag_sents(test_df_ws['processed'].tolist(), corpus='orchid_ud')\n",
    "# test_df_ws['POSTags'] = test_df_ws['POSTags'].apply(flatten)\n",
    "\n",
    "# TODO: 1. concate word with pos via underscore (มัน_ADV)\n",
    "#       2. use only tagging \n",
    "all_df_ws['POSTags'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9f3755ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\Pree\\\\Thai_SA_journal\\\\model\\\\text_all_pos_bow2_ws.joblib']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create bow vectors\n",
    "bow1 = CountVectorizer(ngram_range=(1, 1))\n",
    "bow2 = CountVectorizer(ngram_range=(2, 2))\n",
    "\n",
    "text_all_pos_bow1_fit_kt = bow1.fit(all_df_kt['POSTags'].apply(str))\n",
    "text_all_pos_bow1_kt = text_all_pos_bow1_fit_kt.transform(all_df_kt['POSTags'].apply(str))\n",
    "text_all_pos_bow1_ws = text_all_pos_bow1_fit_kt.transform(all_df_ws['POSTags'].apply(str))\n",
    "\n",
    "text_all_pos_bow2_fit_kt = bow2.fit(all_df_kt['POSTags'].apply(str))\n",
    "text_all_pos_bow2_kt = text_all_pos_bow2_fit_kt.transform(all_df_kt['POSTags'].apply(str))\n",
    "text_all_pos_bow2_ws = text_all_pos_bow2_fit_kt.transform(all_df_ws['POSTags'].apply(str))\n",
    "\n",
    "joblib.dump(text_all_pos_bow1_kt, model_path+'text_all_pos_bow1_kt.joblib')\n",
    "joblib.dump(text_all_pos_bow1_ws, model_path+'text_all_pos_bow1_ws.joblib')\n",
    "joblib.dump(text_all_pos_bow2_kt, model_path+'text_all_pos_bow2_kt.joblib')\n",
    "joblib.dump(text_all_pos_bow2_ws, model_path+'text_all_pos_bow2_ws.joblib')\n",
    "\n",
    "# bow1 = CountVectorizer(ngram_range=(1, 1))\n",
    "# bow2 = CountVectorizer(ngram_range=(2, 2))\n",
    "\n",
    "# text_test_pos_bow1_fit_kt = bow1.fit(test_df_kt['POSTags'].apply(str))\n",
    "# text_test_pos_bow1_kt = text_test_pos_bow1_fit_kt.transform(test_df_kt['POSTags'].apply(str))\n",
    "# text_test_pos_bow1_ws = text_test_pos_bow1_fit_kt.transform(test_df_ws['POSTags'].apply(str))\n",
    "\n",
    "# text_test_pos_bow2_fit_kt = bow2.fit(test_df_kt['POSTags'].apply(str))\n",
    "# text_test_pos_bow2_kt = text_test_pos_bow2_fit_kt.transform(test_df_kt['POSTags'].apply(str))\n",
    "# text_test_pos_bow2_ws = text_test_pos_bow2_fit_kt.transform(test_df_ws['POSTags'].apply(str))\n",
    "\n",
    "# joblib.dump(text_test_pos_bow1_kt, model_path+'text_test_pos_bow1_kt.joblib')\n",
    "# joblib.dump(text_test_pos_bow1_ws, model_path+'text_test_pos_bow1_ws.joblib')\n",
    "# joblib.dump(text_test_pos_bow2_kt, model_path+'text_test_pos_bow2_kt.joblib')\n",
    "# joblib.dump(text_test_pos_bow2_ws, model_path+'text_test_pos_bow2_ws.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "568848ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\Pree\\\\Thai_SA_journal\\\\model\\\\text_all_pos_tfidf2_ws.joblib']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create tfidf vectors\n",
    "tfidf1 = TfidfVectorizer(ngram_range=(1, 1))\n",
    "tfidf2 = TfidfVectorizer(ngram_range=(2, 2))\n",
    "\n",
    "text_all_pos_tfidf1_fit_kt = tfidf1.fit(all_df_kt['POSTags'].apply(str))\n",
    "text_all_pos_tfidf1_kt = text_all_pos_tfidf1_fit_kt.transform(all_df_kt['POSTags'].apply(str))\n",
    "text_all_pos_tfidf1_ws = text_all_pos_tfidf1_fit_kt.transform(all_df_ws['POSTags'].apply(str))\n",
    "\n",
    "text_all_pos_tfidf2_fit_kt = tfidf2.fit(all_df_kt['POSTags'].apply(str))\n",
    "text_all_pos_tfidf2_kt = text_all_pos_tfidf2_fit_kt.transform(all_df_kt['POSTags'].apply(str))\n",
    "text_all_pos_tfidf2_ws = text_all_pos_tfidf2_fit_kt.transform(all_df_ws['POSTags'].apply(str))\n",
    "\n",
    "joblib.dump(text_all_pos_tfidf1_kt, model_path+'text_all_pos_tfidf1_kt.joblib')\n",
    "joblib.dump(text_all_pos_tfidf1_ws, model_path+'text_all_pos_tfidf1_ws.joblib')\n",
    "joblib.dump(text_all_pos_tfidf2_kt, model_path+'text_all_pos_tfidf2_kt.joblib')\n",
    "joblib.dump(text_all_pos_tfidf2_ws, model_path+'text_all_pos_tfidf2_ws.joblib')\n",
    "\n",
    "# tfidf1 = TfidfVectorizer(ngram_range=(1, 1))\n",
    "# tfidf2 = TfidfVectorizer(ngram_range=(2, 2))\n",
    "\n",
    "# text_test_pos_tfidf1_fit_kt = tfidf1.fit(test_df_kt['POSTags'].apply(str))\n",
    "# text_test_pos_tfidf1_kt = text_test_pos_tfidf1_fit_kt.transform(test_df_kt['POSTags'].apply(str))\n",
    "# text_test_pos_tfidf1_ws = text_test_pos_tfidf1_fit_kt.transform(test_df_ws['POSTags'].apply(str))\n",
    "\n",
    "# text_test_pos_tfidf2_fit_kt = tfidf2.fit(test_df_kt['POSTags'].apply(str))\n",
    "# text_test_pos_tfidf2_kt = text_test_pos_tfidf2_fit_kt.transform(test_df_kt['POSTags'].apply(str))\n",
    "# text_test_pos_tfidf2_ws = text_test_pos_tfidf2_fit_kt.transform(test_df_ws['POSTags'].apply(str))\n",
    "\n",
    "# joblib.dump(text_test_pos_tfidf1_kt, model_path+'text_test_pos_tfidf1_kt.joblib')\n",
    "# joblib.dump(text_test_pos_tfidf1_ws, model_path+'text_test_pos_tfidf1_ws.joblib')\n",
    "# joblib.dump(text_test_pos_tfidf2_kt, model_path+'text_test_pos_tfidf2_kt.joblib')\n",
    "# joblib.dump(text_test_pos_tfidf2_ws, model_path+'text_test_pos_tfidf2_ws.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c6949e",
   "metadata": {},
   "source": [
    "## Dictionary-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5e922a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load list of our custom positive and negative words\n",
    "with open(os.path.dirname(os.getcwd()) + '\\\\data\\\\' + 'pos_words.txt', encoding='UTF-8') as f:\n",
    "    pos_words = [line.rstrip('\\n') for line in f]\n",
    "\n",
    "with open(os.path.dirname(os.getcwd()) + '\\\\data\\\\' + 'neg_words.txt', encoding='UTF-8') as f:\n",
    "    neg_words = [line.rstrip('\\n') for line in f]\n",
    "pos_words = list(set(pos_words))\n",
    "neg_words = list(set(neg_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6ad848e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict size:  91\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\Pree\\\\Thai_SA_journal\\\\model\\\\text_all_dict_bow2_ws.joblib']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use bow and tfidf vectorizer based on our custom dict\n",
    "bow1 = CountVectorizer(ngram_range=(1, 1))\n",
    "bow2 = CountVectorizer(ngram_range=(2, 2))\n",
    "\n",
    "my_vocabs = pos_words + neg_words\n",
    "print('dict size: ', len(my_vocabs))\n",
    "\n",
    "text_all_dict_bow1_fit = bow1.fit(my_vocabs)\n",
    "text_all_dict_bow1_kt = text_all_dict_bow1_fit.transform(all_df_kt['processed'].apply(str))\n",
    "text_all_dict_bow1_ws = text_all_dict_bow1_fit.transform(all_df_ws['processed'].apply(str))\n",
    "\n",
    "text_all_dict_bow2_fit = bow2.fit(my_vocabs)\n",
    "text_all_dict_bow2_kt = text_all_dict_bow2_fit.transform(all_df_kt['processed'].apply(str))\n",
    "text_all_dict_bow2_ws = text_all_dict_bow2_fit.transform(all_df_ws['processed'].apply(str))\n",
    "\n",
    "joblib.dump(text_all_dict_bow1_kt, model_path+'text_all_dict_bow1_kt.joblib')\n",
    "joblib.dump(text_all_dict_bow1_ws, model_path+'text_all_dict_bow1_ws.joblib')\n",
    "joblib.dump(text_all_dict_bow2_kt, model_path+'text_all_dict_bow2_kt.joblib')\n",
    "joblib.dump(text_all_dict_bow2_ws, model_path+'text_all_dict_bow2_ws.joblib')\n",
    "\n",
    "# bow1 = CountVectorizer(ngram_range=(1, 1))\n",
    "# bow2 = CountVectorizer(ngram_range=(2, 2))\n",
    "\n",
    "# text_test_dict_bow1_fit = bow1.fit(my_vocabs)\n",
    "# text_test_dict_bow1_kt = text_test_dict_bow1_fit.transform(test_df_kt['processed'].apply(str))\n",
    "# text_test_dict_bow1_ws = text_test_dict_bow1_fit.transform(test_df_ws['processed'].apply(str))\n",
    "\n",
    "# text_test_dict_bow2_fit = bow2.fit(my_vocabs)\n",
    "# text_test_dict_bow2_kt = text_test_dict_bow2_fit.transform(test_df_kt['processed'].apply(str))\n",
    "# text_test_dict_bow2_ws = text_test_dict_bow2_fit.transform(test_df_ws['processed'].apply(str))\n",
    "\n",
    "# joblib.dump(text_test_dict_bow1_kt, model_path+'text_test_dict_bow1_kt.joblib')\n",
    "# joblib.dump(text_test_dict_bow1_ws, model_path+'text_test_dict_bow1_ws.joblib')\n",
    "# joblib.dump(text_test_dict_bow2_kt, model_path+'text_test_dict_bow2_kt.joblib')\n",
    "# joblib.dump(text_test_dict_bow2_ws, model_path+'text_test_dict_bow2_ws.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b616abd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\Pree\\\\Thai_SA_journal\\\\model\\\\text_all_dict_tfidf2_ws.joblib']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use bow and tfidf vectorizer based on our custom dict\n",
    "tfidf1 = TfidfVectorizer(ngram_range=(1, 1))\n",
    "tfidf2 = TfidfVectorizer(ngram_range=(2, 2))\n",
    "\n",
    "text_all_dict_tfidf1_fit = tfidf1.fit(my_vocabs)\n",
    "text_all_dict_tfidf1_kt = text_all_dict_tfidf1_fit.transform(all_df_kt['processed'].apply(str))\n",
    "text_all_dict_tfidf1_ws = text_all_dict_tfidf1_fit.transform(all_df_ws['processed'].apply(str))\n",
    "\n",
    "text_all_dict_tfidf2_fit = bow2.fit(my_vocabs)\n",
    "text_all_dict_tfidf2_kt = text_all_dict_tfidf2_fit.transform(all_df_kt['processed'].apply(str))\n",
    "text_all_dict_tfidf2_ws = text_all_dict_tfidf2_fit.transform(all_df_ws['processed'].apply(str))\n",
    "\n",
    "joblib.dump(text_all_dict_tfidf1_kt, model_path+'text_all_dict_tfidf1_kt.joblib')\n",
    "joblib.dump(text_all_dict_tfidf1_ws, model_path+'text_all_dict_tfidf1_ws.joblib')\n",
    "joblib.dump(text_all_dict_tfidf2_kt, model_path+'text_all_dict_tfidf2_kt.joblib')\n",
    "joblib.dump(text_all_dict_tfidf2_ws, model_path+'text_all_dict_tfidf2_ws.joblib')\n",
    "\n",
    "# tfidf1 = TfidfVectorizer(ngram_range=(1, 1))\n",
    "# tfidf2 = TfidfVectorizer(ngram_range=(2, 2))\n",
    "\n",
    "# text_test_dict_tfidf1_fit = tfidf1.fit(my_vocabs)\n",
    "# text_test_dict_tfidf1_kt = text_test_dict_tfidf1_fit.transform(test_df_kt['processed'].apply(str))\n",
    "# text_test_dict_tfidf1_ws = text_test_dict_tfidf1_fit.transform(test_df_ws['processed'].apply(str))\n",
    "\n",
    "# text_test_dict_tfidf2_fit = tfidf2.fit(my_vocabs)\n",
    "# text_test_dict_tfidf2_kt = text_test_dict_tfidf2_fit.transform(test_df_kt['processed'].apply(str))\n",
    "# text_test_dict_tfidf2_ws = text_test_dict_tfidf2_fit.transform(test_df_ws['processed'].apply(str))\n",
    "\n",
    "# joblib.dump(text_test_dict_tfidf1_kt, model_path+'text_test_dict_tfidf1_kt.joblib')\n",
    "# joblib.dump(text_test_dict_tfidf1_ws, model_path+'text_test_dict_tfidf1_ws.joblib')\n",
    "# joblib.dump(text_test_dict_tfidf2_kt, model_path+'text_test_dict_tfidf2_kt.joblib')\n",
    "# joblib.dump(text_test_dict_tfidf2_ws, model_path+'text_test_dict_tfidf2_ws.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f1e6f2",
   "metadata": {},
   "source": [
    "## Demonstrate usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e15f9a2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21389, 134146)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#text_all_kt = joblib.load(model_path+'text_all_tfidf2_kt.joblib')\n",
    "#text_test_kt = joblib.load(model_path+'text_all_tfidf2_kt.joblib')\n",
    "\n",
    "text_all_ws = joblib.load(model_path+'text_all_tfidf2_ws.joblib')\n",
    "#text_test_ws = joblib.load(model_path+'text_test_tfidf2_ws.joblib')\n",
    "\n",
    "text_all_ws.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ad81ac9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load target class\n",
    "y_train = pd.read_csv(os.path.dirname(os.getcwd()) + '\\\\' + 'train_label_ws.csv')['targets']\n",
    "#y_test = pd.read_csv(os.path.dirname(os.getcwd()) + '\\\\' + 'test_label_ws.csv')['targets']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520d359d",
   "metadata": {},
   "source": [
    "## Train-valid split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "87519fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(text_all_ws, y_train, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3159c026",
   "metadata": {},
   "source": [
    "## Test the extracted features with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fc1413dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6344655655967592"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quick test with out cv\n",
    "#fit logistic regression models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(C=2., penalty=\"l2\", solver=\"liblinear\", dual=False, multi_class=\"ovr\")\n",
    "model.fit(X_train, y_train.values.ravel())\n",
    "model.score(X_valid, y_valid)\n",
    "#y_pred = model.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d6282242",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
    "def build_model(model):\n",
    "    scores = (cross_val_score(model, X_train, y_train, cv = 5).mean())\n",
    "    model = model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_valid)\n",
    "    acc_sc = accuracy_score(y_valid, y_pred)\n",
    "    pre_sc = precision_score(y_valid, y_pred, average='weighted')\n",
    "    rec_sc = recall_score(y_valid, y_pred, average='weighted')\n",
    "    f1_sc = f1_score(y_valid, y_pred, average='weighted')\n",
    "    print('Accuracy :',acc_sc)\n",
    "    print('Confusion Matrix :\\n', confusion_matrix(y_valid, y_pred))\n",
    "    print('Precision :', pre_sc)\n",
    "    print('Recall :', rec_sc)\n",
    "    print('F1-score :', f1_sc)\n",
    "    print('Classification Report :\\n', classification_report(y_valid, y_pred))\n",
    "    print('Average accuracy of k-fold (5-fold) :', scores ,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3e120d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.6344655655967592\n",
      "Confusion Matrix :\n",
      " [[ 339  432   19    0]\n",
      " [ 110 1608   39    2]\n",
      " [  41  448   89    0]\n",
      " [   5   73    4    0]]\n",
      "Precision : 0.6189299634363078\n",
      "Recall : 0.6344655655967592\n",
      "F1-score : 0.5819361159246491\n",
      "Classification Report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.68      0.43      0.53       790\n",
      "         neu       0.63      0.91      0.74      1759\n",
      "         pos       0.59      0.15      0.24       578\n",
      "           q       0.00      0.00      0.00        82\n",
      "\n",
      "    accuracy                           0.63      3209\n",
      "   macro avg       0.48      0.37      0.38      3209\n",
      "weighted avg       0.62      0.63      0.58      3209\n",
      "\n",
      "Average accuracy of k-fold (5-fold) : 0.6265676567656766 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "build_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "18ed0983",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_all_kt = joblib.load(model_path+'text_all_tfidf2_kt.joblib')\n",
    "text_all_ws.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e9288c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pd.read_csv(os.path.dirname(os.getcwd()) + '\\\\' + 'train_label_kt.csv')['vote']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6d73174f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(text_all_kt, y_train, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "640f33fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.6963938973647712\n",
      "Confusion Matrix :\n",
      " [[ 224  926   31]\n",
      " [  82 4343   85]\n",
      " [  19 1046  454]]\n",
      "Precision : 0.7108881188654218\n",
      "Recall : 0.6963938973647712\n",
      "F1-score : 0.6422187904341828\n",
      "Classification Report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.69      0.19      0.30      1181\n",
      "         neu       0.69      0.96      0.80      4510\n",
      "         pos       0.80      0.30      0.43      1519\n",
      "\n",
      "    accuracy                           0.70      7210\n",
      "   macro avg       0.72      0.48      0.51      7210\n",
      "weighted avg       0.71      0.70      0.64      7210\n",
      "\n",
      "Average accuracy of k-fold (5-fold) : 0.6992214936934771 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "build_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fa3504",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
