{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "139d4b2f",
   "metadata": {},
   "source": [
    "### Feature extractions\n",
    "\n",
    "This script extracted features from two sentimental corpora, kt4.0 (ours) and wisesight. By training from kt4.0 corpus, we expect to see an improvement in the wisesight corpus' classification performance.\n",
    "\n",
    "For both datasets, random stratify hold-out was performed with 80:20 ratio for train and test set. Next, several feature extraction methods were applied and output as a joblib objects as follows:  \n",
    "\n",
    "* BOW1, BOW2\n",
    "* TF-IDF1, TF-IDF2\n",
    "* Word2Vec pretrained from Thai wiki. (300 dimension)\n",
    "* POS_tagging with flatten dataframe\n",
    "* Dictionary-based with Thai positive and negative words  \n",
    "\n",
    "The output vectors will be carried out in the next experiment.  \n",
    "pree.t@cmu.ac.th  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a978d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pythainlp\n",
    "from pythainlp.ulmfit import process_thai\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.rcParams['font.family'] = 'tahoma'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47e606a",
   "metadata": {},
   "source": [
    "## Load original datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fe523d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60081, 14), (26737, 2))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.path.dirname(os.getcwd())\n",
    "\n",
    "data_path_kt = os.path.dirname(os.getcwd()) + '\\\\data\\kt4.0\\\\'\n",
    "data_path_ws = os.path.dirname(os.getcwd()) + '\\\\data\\wisesight\\\\'\n",
    "model_path = os.path.dirname(os.getcwd()) + '\\\\model\\\\'\n",
    "df_kt = pd.read_csv(data_path_kt + 'pantip_cleaned_1.csv')\n",
    "\n",
    "# we use the original wisesight corpus and reconstruct a new dataframe\n",
    "texts = []\n",
    "targets = []\n",
    "\n",
    "with open(str(data_path_ws) + '\\\\' + 'neg.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        texts.append(line.strip())\n",
    "        targets.append('neg')\n",
    "\n",
    "with open(str(data_path_ws) + '\\\\' + 'neu.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        texts.append(line.strip())\n",
    "        targets.append('neu')\n",
    "\n",
    "with open(str(data_path_ws) + '\\\\' + 'pos.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        texts.append(line.strip())\n",
    "        targets.append('pos')\n",
    "\n",
    "with open(str(data_path_ws) + '\\\\' + 'q.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        texts.append(line.strip())\n",
    "        targets.append('q')\n",
    "        \n",
    "df_ws = pd.DataFrame({'texts': texts, 'targets': targets})\n",
    "df_ws.to_csv(os.path.dirname(os.getcwd()) + '\\\\' + 'wisesight.csv', index=False)\n",
    "df_kt.shape, df_ws.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c128ab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>post_date</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_name</th>\n",
       "      <th>text</th>\n",
       "      <th>tag</th>\n",
       "      <th>emotion</th>\n",
       "      <th>length</th>\n",
       "      <th>num_sent</th>\n",
       "      <th>sent_length</th>\n",
       "      <th>label</th>\n",
       "      <th>label_1</th>\n",
       "      <th>label_2</th>\n",
       "      <th>vote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39839097</td>\n",
       "      <td>2020-04-25 13:24:00</td>\n",
       "      <td>https://pantip.com/profile/5798163</td>\n",
       "      <td>สมาชิกหมายเลข 5798163</td>\n",
       "      <td>[CR] แปังพัฟคุมมัน จัดเต็มเนื้อบางเบา</td>\n",
       "      <td>เครื่องสำอาง</td>\n",
       "      <td>ถูกใจ 0 ขำกลิ้ง 0 หลงรัก 0 ซึ้ง 0 สยอง 0 ทึ่ง 0</td>\n",
       "      <td>36</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39839097</td>\n",
       "      <td>2020-04-25 13:24:00</td>\n",
       "      <td>https://pantip.com/profile/5798163</td>\n",
       "      <td>สมาชิกหมายเลข 5798163</td>\n",
       "      <td>ไม่อุดตัน แต่ปกปิดแน่นมาก</td>\n",
       "      <td>เครื่องสำอาง</td>\n",
       "      <td>ถูกใจ 0 ขำกลิ้ง 0 หลงรัก 0 ซึ้ง 0 สยอง 0 ทึ่ง 0</td>\n",
       "      <td>36</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39839097</td>\n",
       "      <td>2020-04-25 13:24:00</td>\n",
       "      <td>https://pantip.com/profile/5798163</td>\n",
       "      <td>สมาชิกหมายเลข 5798163</td>\n",
       "      <td>รีวิวแป้ง Lady Audrey Ready All Day จ้า</td>\n",
       "      <td>เครื่องสำอาง</td>\n",
       "      <td>ถูกใจ 0 ขำกลิ้ง 0 หลงรัก 0 ซึ้ง 0 สยอง 0 ทึ่ง 0</td>\n",
       "      <td>36</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39838736</td>\n",
       "      <td>2020-04-25 10:52:00</td>\n",
       "      <td>https://pantip.com/profile/5730006</td>\n",
       "      <td>สมาชิกหมายเลข 5730006</td>\n",
       "      <td>ขอบตาดำมากค่ะ คอร์เล็คเตอร์ก็เอาไม่อยู่</td>\n",
       "      <td>เครื่องสำอาง</td>\n",
       "      <td>ถูกใจ 0 ขำกลิ้ง 0 หลงรัก 0 ซึ้ง 0 สยอง 0 ทึ่ง 0</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>39837384</td>\n",
       "      <td>2020-04-24 20:39:00</td>\n",
       "      <td>https://pantip.com/profile/4975838</td>\n",
       "      <td>สมาชิกหมายเลข 4975838</td>\n",
       "      <td>เอาaloe Vera แช่ตู้เย็น จนกลายเป็นน้ำแข็ง</td>\n",
       "      <td>เครื่องสำอาง</td>\n",
       "      <td>ถูกใจ 0 ขำกลิ้ง 0 หลงรัก 0 ซึ้ง 0 สยอง 0 ทึ่ง 0</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>neu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>39838990</td>\n",
       "      <td>2020-04-25 12:36:00</td>\n",
       "      <td>https://pantip.com/profile/5655853</td>\n",
       "      <td>chdewxx</td>\n",
       "      <td>[SR] ไอเทม #เซรั่มสิว ลดสิว สิวอุดตัน สิวผด บำ...</td>\n",
       "      <td>เครื่องสำอาง</td>\n",
       "      <td>ถูกใจ 0 ขำกลิ้ง 0 หลงรัก 0 ซึ้ง 0 สยอง 0 ทึ่ง 0</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>39838619</td>\n",
       "      <td>2020-04-25 10:01:00</td>\n",
       "      <td>https://pantip.com/profile/5656639</td>\n",
       "      <td>คูจองยอนและวีรยา</td>\n",
       "      <td>รบกวนสาวๆช่วยแนะนำสกินแคร์ ที่ช่วยให้ผิวหน้าขา...</td>\n",
       "      <td>เครื่องสำอาง</td>\n",
       "      <td>ถูกใจ 0 ขำกลิ้ง 0 หลงรัก 0 ซึ้ง 0 สยอง 0 ทึ่ง 0</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>39837266</td>\n",
       "      <td>2020-04-24 19:58:00</td>\n",
       "      <td>https://pantip.com/profile/632132</td>\n",
       "      <td>หมูกลมอารมณ์ดี</td>\n",
       "      <td>ทดลองใช้ แครอทวิตซีหน้าใส</td>\n",
       "      <td>เครื่องสำอาง</td>\n",
       "      <td>ถูกใจ 0 ขำกลิ้ง 0 หลงรัก 0 ซึ้ง 0 สยอง 0 ทึ่ง 0</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>39837266</td>\n",
       "      <td>2020-04-24 19:58:00</td>\n",
       "      <td>https://pantip.com/profile/632132</td>\n",
       "      <td>หมูกลมอารมณ์ดี</td>\n",
       "      <td>ใน 1 สัปดาห์</td>\n",
       "      <td>เครื่องสำอาง</td>\n",
       "      <td>ถูกใจ 0 ขำกลิ้ง 0 หลงรัก 0 ซึ้ง 0 สยอง 0 ทึ่ง 0</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>neu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>39835926</td>\n",
       "      <td>2020-04-24 12:03:00</td>\n",
       "      <td>https://pantip.com/profile/3826851</td>\n",
       "      <td>สมาชิกหมายเลข 3826851</td>\n",
       "      <td>วิธีเลือก \"รองพื้น\" และ \"คอนซีลเลอร์\"</td>\n",
       "      <td>เครื่องสำอาง</td>\n",
       "      <td>ถูกใจ 0 ขำกลิ้ง 0 หลงรัก 0 ซึ้ง 0 สยอง 0 ทึ่ง 0</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    post_id            post_date                             user_id  \\\n",
       "0  39839097  2020-04-25 13:24:00  https://pantip.com/profile/5798163   \n",
       "1  39839097  2020-04-25 13:24:00  https://pantip.com/profile/5798163   \n",
       "2  39839097  2020-04-25 13:24:00  https://pantip.com/profile/5798163   \n",
       "3  39838736  2020-04-25 10:52:00  https://pantip.com/profile/5730006   \n",
       "4  39837384  2020-04-24 20:39:00  https://pantip.com/profile/4975838   \n",
       "5  39838990  2020-04-25 12:36:00  https://pantip.com/profile/5655853   \n",
       "6  39838619  2020-04-25 10:01:00  https://pantip.com/profile/5656639   \n",
       "7  39837266  2020-04-24 19:58:00   https://pantip.com/profile/632132   \n",
       "8  39837266  2020-04-24 19:58:00   https://pantip.com/profile/632132   \n",
       "9  39835926  2020-04-24 12:03:00  https://pantip.com/profile/3826851   \n",
       "\n",
       "               user_name                                               text  \\\n",
       "0  สมาชิกหมายเลข 5798163              [CR] แปังพัฟคุมมัน จัดเต็มเนื้อบางเบา   \n",
       "1  สมาชิกหมายเลข 5798163                          ไม่อุดตัน แต่ปกปิดแน่นมาก   \n",
       "2  สมาชิกหมายเลข 5798163            รีวิวแป้ง Lady Audrey Ready All Day จ้า   \n",
       "3  สมาชิกหมายเลข 5730006            ขอบตาดำมากค่ะ คอร์เล็คเตอร์ก็เอาไม่อยู่   \n",
       "4  สมาชิกหมายเลข 4975838          เอาaloe Vera แช่ตู้เย็น จนกลายเป็นน้ำแข็ง   \n",
       "5                chdewxx  [SR] ไอเทม #เซรั่มสิว ลดสิว สิวอุดตัน สิวผด บำ...   \n",
       "6       คูจองยอนและวีรยา  รบกวนสาวๆช่วยแนะนำสกินแคร์ ที่ช่วยให้ผิวหน้าขา...   \n",
       "7         หมูกลมอารมณ์ดี                          ทดลองใช้ แครอทวิตซีหน้าใส   \n",
       "8         หมูกลมอารมณ์ดี                                       ใน 1 สัปดาห์   \n",
       "9  สมาชิกหมายเลข 3826851              วิธีเลือก \"รองพื้น\" และ \"คอนซีลเลอร์\"   \n",
       "\n",
       "            tag                                          emotion  length  \\\n",
       "0  เครื่องสำอาง  ถูกใจ 0 ขำกลิ้ง 0 หลงรัก 0 ซึ้ง 0 สยอง 0 ทึ่ง 0      36   \n",
       "1  เครื่องสำอาง  ถูกใจ 0 ขำกลิ้ง 0 หลงรัก 0 ซึ้ง 0 สยอง 0 ทึ่ง 0      36   \n",
       "2  เครื่องสำอาง  ถูกใจ 0 ขำกลิ้ง 0 หลงรัก 0 ซึ้ง 0 สยอง 0 ทึ่ง 0      36   \n",
       "3  เครื่องสำอาง  ถูกใจ 0 ขำกลิ้ง 0 หลงรัก 0 ซึ้ง 0 สยอง 0 ทึ่ง 0      15   \n",
       "4  เครื่องสำอาง  ถูกใจ 0 ขำกลิ้ง 0 หลงรัก 0 ซึ้ง 0 สยอง 0 ทึ่ง 0      11   \n",
       "5  เครื่องสำอาง  ถูกใจ 0 ขำกลิ้ง 0 หลงรัก 0 ซึ้ง 0 สยอง 0 ทึ่ง 0      29   \n",
       "6  เครื่องสำอาง  ถูกใจ 0 ขำกลิ้ง 0 หลงรัก 0 ซึ้ง 0 สยอง 0 ทึ่ง 0      23   \n",
       "7  เครื่องสำอาง  ถูกใจ 0 ขำกลิ้ง 0 หลงรัก 0 ซึ้ง 0 สยอง 0 ทึ่ง 0      14   \n",
       "8  เครื่องสำอาง  ถูกใจ 0 ขำกลิ้ง 0 หลงรัก 0 ซึ้ง 0 สยอง 0 ทึ่ง 0      14   \n",
       "9  เครื่องสำอาง  ถูกใจ 0 ขำกลิ้ง 0 หลงรัก 0 ซึ้ง 0 สยอง 0 ทึ่ง 0      14   \n",
       "\n",
       "   num_sent  sent_length  label  label_1  label_2 vote  \n",
       "0         3           14      2        2        2  pos  \n",
       "1         3            8      2        2        2  pos  \n",
       "2         3           14      2        2        1  pos  \n",
       "3         2           13      1        3        3  neg  \n",
       "4         1           11      1        1        3  neu  \n",
       "5         1           29      2        2        2  pos  \n",
       "6         1           23      2        2        1  pos  \n",
       "7         2            9      1        2        2  pos  \n",
       "8         2            5      1        1        1  neu  \n",
       "9         1           14      2        2        2  pos  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_kt.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d84766f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>length</th>\n",
       "      <th>num_sent</th>\n",
       "      <th>sent_length</th>\n",
       "      <th>label</th>\n",
       "      <th>label_1</th>\n",
       "      <th>label_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6.008100e+04</td>\n",
       "      <td>60081.000000</td>\n",
       "      <td>60081.000000</td>\n",
       "      <td>60081.000000</td>\n",
       "      <td>60081.000000</td>\n",
       "      <td>60081.000000</td>\n",
       "      <td>60081.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.964936e+07</td>\n",
       "      <td>116.994574</td>\n",
       "      <td>8.502172</td>\n",
       "      <td>13.978329</td>\n",
       "      <td>1.577304</td>\n",
       "      <td>1.362644</td>\n",
       "      <td>1.662156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.559919e+05</td>\n",
       "      <td>118.647716</td>\n",
       "      <td>7.575442</td>\n",
       "      <td>12.083572</td>\n",
       "      <td>0.777527</td>\n",
       "      <td>0.639271</td>\n",
       "      <td>0.800034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.917283e+07</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.958755e+07</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.968929e+07</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.976947e+07</td>\n",
       "      <td>159.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.983970e+07</td>\n",
       "      <td>499.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>301.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            post_id        length      num_sent   sent_length         label  \\\n",
       "count  6.008100e+04  60081.000000  60081.000000  60081.000000  60081.000000   \n",
       "mean   3.964936e+07    116.994574      8.502172     13.978329      1.577304   \n",
       "std    1.559919e+05    118.647716      7.575442     12.083572      0.777527   \n",
       "min    3.917283e+07      3.000000      1.000000      3.000000      1.000000   \n",
       "25%    3.958755e+07     31.000000      3.000000      6.000000      1.000000   \n",
       "50%    3.968929e+07     72.000000      6.000000     10.000000      1.000000   \n",
       "75%    3.976947e+07    159.000000     11.000000     17.000000      2.000000   \n",
       "max    3.983970e+07    499.000000     44.000000    301.000000      3.000000   \n",
       "\n",
       "            label_1       label_2  \n",
       "count  60081.000000  60081.000000  \n",
       "mean       1.362644      1.662156  \n",
       "std        0.639271      0.800034  \n",
       "min        1.000000      1.000000  \n",
       "25%        1.000000      1.000000  \n",
       "50%        1.000000      1.000000  \n",
       "75%        2.000000      2.000000  \n",
       "max        3.000000      3.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_kt.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5ec5386",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texts</th>\n",
       "      <th>targets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>☹️</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>😔</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>😞</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>😥</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>รำ</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Noๆ</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Rip</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>T_T</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>กาก</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>โกง</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  texts targets\n",
       "0    ☹️     neg\n",
       "1     😔     neg\n",
       "2     😞     neg\n",
       "3     😥     neg\n",
       "4    รำ     neg\n",
       "5   Noๆ     neg\n",
       "6   Rip     neg\n",
       "7   T_T     neg\n",
       "8   กาก     neg\n",
       "9   โกง     neg"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ws.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7077f068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texts</th>\n",
       "      <th>targets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>26737</td>\n",
       "      <td>26737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>26713</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>อุดรมีไหมค่ะ</td>\n",
       "      <td>neu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>2</td>\n",
       "      <td>14561</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               texts targets\n",
       "count          26737   26737\n",
       "unique         26713       4\n",
       "top     อุดรมีไหมค่ะ     neu\n",
       "freq               2   14561"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ws.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75693314",
   "metadata": {},
   "source": [
    "# Train-test split 80/20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c67990d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((48064, 14), (12017, 14))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random stratified split train and test set 80/20\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate\n",
    "all_df_kt, test_df_kt = train_test_split(df_kt, test_size=0.2, random_state=42, shuffle = True)\n",
    "all_df_kt.shape, test_df_kt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c7d3bcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neu    0.632136\n",
       "pos    0.206620\n",
       "neg    0.161243\n",
       "Name: vote, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# class distribution\n",
    "all_df_kt.vote.value_counts() / all_df_kt.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b63ee3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((21389, 2), (5348, 2))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df_ws, test_df_ws = train_test_split(df_ws, test_size=0.2, random_state=42, shuffle = True)\n",
    "all_df_ws.shape, test_df_ws.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae548ccd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neu    0.544860\n",
       "neg    0.253588\n",
       "pos    0.179345\n",
       "q      0.022208\n",
       "Name: targets, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# class distribution\n",
    "all_df_ws.targets.value_counts() / all_df_ws.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82ef6037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean and word tokenize\n",
    "all_df_kt['processed'] = all_df_kt['text'].apply(str).apply(process_thai)\n",
    "test_df_kt['processed'] = test_df_kt['text'].apply(str).apply(process_thai)\n",
    "\n",
    "all_df_ws['processed'] = all_df_ws['texts'].apply(str).apply(process_thai)\n",
    "test_df_ws['processed'] = test_df_ws['texts'].apply(str).apply(process_thai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "350cbcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df_kt.to_csv(os.path.dirname(os.getcwd())+'all_df_kt.csv',  index=False)\n",
    "test_df_kt.to_csv(os.path.dirname(os.getcwd())+'test_df_kt.csv',  index=False)\n",
    "\n",
    "all_df_ws.to_csv(os.path.dirname(os.getcwd())+'all_df_ws.csv', index=False)\n",
    "test_df_ws.to_csv(os.path.dirname(os.getcwd())+'test_df_ws.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c6e20f",
   "metadata": {},
   "source": [
    "## Bag of words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2eb3e4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\Pree\\\\Thai_SA_journal\\\\model\\\\bow2_ws.joblib']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import joblib\n",
    "\n",
    "# BOW with unigram and bigrams\n",
    "bow1 = CountVectorizer(ngram_range=(1, 1))\n",
    "bow2 = CountVectorizer(ngram_range=(2, 2))\n",
    "\n",
    "bow1_fit_kt = bow1.fit(all_df_kt['processed'].apply(str))\n",
    "bow1_kt = bow1_fit_kt.transform(all_df_kt['processed'].apply(str))\n",
    "bow1_ws = bow1_fit_kt.transform(all_df_ws['processed'].apply(str))\n",
    "\n",
    "bow2_fit_kt = bow2.fit(all_df_kt['processed'].apply(str))\n",
    "bow2_kt = bow2_fit_kt.transform(all_df_kt['processed'].apply(str))\n",
    "bow2_ws = bow2_fit_kt.transform(all_df_ws['processed'].apply(str))\n",
    "\n",
    "joblib.dump(bow1_kt, model_path+'bow1_kt.joblib')\n",
    "joblib.dump(bow1_ws, model_path+'bow1_ws.joblib')\n",
    "joblib.dump(bow2_kt, model_path+'bow2_kt.joblib')\n",
    "joblib.dump(bow2_ws, model_path+'bow2_ws.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d7b552",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3232d40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\Pree\\\\Thai_SA_journal\\\\model\\\\tfidf2_ws.joblib']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf1 = TfidfVectorizer(ngram_range=(1, 1), min_df=20, sublinear_tf=True)\n",
    "tfidf2 = TfidfVectorizer(ngram_range=(2, 2), min_df=20, sublinear_tf=True)\n",
    "\n",
    "tfidf1_fit_kt = tfidf1.fit(all_df_kt['processed'].apply(str))\n",
    "tfidf1_kt = tfidf1_fit_kt.transform(all_df_kt['processed'].apply(str))\n",
    "tfidf1_ws = tfidf1_fit_kt.transform(all_df_ws['processed'].apply(str))\n",
    "\n",
    "tfidf2_fit_kt = tfidf2.fit(all_df_kt['processed'].apply(str))\n",
    "tfidf2_kt = tfidf2_fit_kt.transform(all_df_kt['processed'].apply(str))\n",
    "tfidf2_ws = tfidf2_fit_kt.transform(all_df_ws['processed'].apply(str))\n",
    "\n",
    "joblib.dump(tfidf1_kt, model_path+'tfidf1_kt.joblib')\n",
    "joblib.dump(tfidf1_ws, model_path+'tfidf1_ws.joblib')\n",
    "joblib.dump(tfidf2_kt, model_path+'tfidf2_kt.joblib')\n",
    "joblib.dump(tfidf2_ws, model_path+'tfidf2_ws.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2867680",
   "metadata": {},
   "source": [
    "## Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8551fe6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "\n",
    "class TfidfEmbeddingVectorizer(object):\n",
    "    def __init__(self, model):\n",
    "        w2v = {w: vec for w, vec in zip(model.wv.index_to_key, model.wv.vectors)}\n",
    "        self.word2vec = w2v\n",
    "        self.word2weight = None\n",
    "        self.dim = model.vector_size\n",
    "    \n",
    "    \n",
    "    def fit(self, X):\n",
    "        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "        tfidf.fit(X)\n",
    "        max_idf = max(tfidf.idf_)\n",
    "        self.word2weight = defaultdict(\n",
    "            lambda: max_idf,\n",
    "            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "                np.mean([self.word2vec[w] * self.word2weight[w]\n",
    "                         for w in words if w in self.word2vec] or\n",
    "                        [np.zeros(self.dim)], axis=0)\n",
    "                for words in X\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "856dd1fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('หยก', 0.8109732866287231),\n",
       " ('ต้มยำ', 0.7100409269332886),\n",
       " ('ไข่เค็ม', 0.6316604018211365),\n",
       " ('น้ำพริก', 0.6190825700759888),\n",
       " ('ยำ', 0.6129943132400513),\n",
       " ('กุ้ง', 0.6098511815071106),\n",
       " ('น้ำปลา', 0.6052413582801819),\n",
       " ('ผัด', 0.6024342775344849),\n",
       " ('สลัด', 0.5912983417510986),\n",
       " ('ใส้', 0.5805458426475525)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from pythainlp import word_vector\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "\n",
    "# create word2vec for kt corpus\n",
    "w2v_kt = Word2Vec(vector_size=300, min_count=1, window = 5, workers=4)\n",
    "w2v_kt.build_vocab(all_df_kt['processed'])\n",
    "total_examples = w2v_kt.corpus_count\n",
    "w2v_kt.train(all_df_ws['processed'], total_examples=total_examples, epochs=50)\n",
    "w2v_kt.wv.most_similar(\"บะหมี่\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "542624be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill # we use dill instead of joblim because the lambda and dependecie in class TfidfEmbeddingVectorizer\n",
    "# create embbed vector\n",
    "w2v_tfidf_emb_kt = TfidfEmbeddingVectorizer(w2v_kt)\n",
    "w2v_tifdf_fit_kt = w2v_tfidf_emb_kt.fit(all_df_kt['processed'])\n",
    "\n",
    "w2v_tifdf_kt = w2v_tifdf_fit_kt.transform(all_df_kt['processed'])\n",
    "w2v_tifdf_ws = w2v_tifdf_fit_kt.transform(all_df_ws['processed'])\n",
    "\n",
    "dill.dump(w2v_tifdf_kt, open(model_path+'w2v-tfidf_kt.joblib', 'wb'))\n",
    "dill.dump(w2v_tifdf_ws, open(model_path+'w2v_tifdf_ws.joblib', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee904ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: use pretrained from thai2fit\n",
    "#w2v_thwiki = word_vector.get_model()\n",
    "#w2v_model_ws.build_vocab(w2v_thwiki.index_to_key, update=True)\n",
    "#w2v_model_ws.wv.vectors_lockf = np.ones(len(w2v_model_ws.wv))\n",
    "#w2v_model_ws.wv.intersect_word2vec_format('thai2vec.bin', binary=True, lockf=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2102229e",
   "metadata": {},
   "source": [
    "## POS_Tag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0387c5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(x):\n",
    "    l = list(sum(x, ()))\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c257e9d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[',\n",
       " 'NOUN',\n",
       " 'cr',\n",
       " 'NOUN',\n",
       " ']',\n",
       " 'NOUN',\n",
       " 'แป',\n",
       " 'NOUN',\n",
       " 'ังพัฟ',\n",
       " 'NOUN',\n",
       " 'คุม',\n",
       " 'NOUN',\n",
       " 'มัน',\n",
       " 'PRON',\n",
       " 'จัด',\n",
       " 'VERB',\n",
       " 'เต็ม',\n",
       " 'VERB',\n",
       " 'เนื้อ',\n",
       " 'NOUN',\n",
       " 'บางเบา',\n",
       " 'NOUN']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pythainlp.tag import pos_tag_sents\n",
    "# we used a POS tag with the orchid_ud feature that represented a type of word in a sentence in one-hot vector form\n",
    "# flatten the list of tuple in series was applied for feature vectors\n",
    "all_df_kt['POSTags'] = pos_tag_sents(all_df_kt['processed'].tolist(), corpus='orchid_ud')\n",
    "all_df_kt['POSTags'] = all_df_kt['POSTags'].apply(flatten)\n",
    "\n",
    "all_df_ws['POSTags'] = pos_tag_sents(all_df_ws['processed'].tolist(), corpus='orchid_ud')\n",
    "all_df_ws['POSTags'] = all_df_ws['POSTags'].apply(flatten)\n",
    "# TODO: 1. concate word with pos via underscore (มัน_ADV)\n",
    "#       2. use only tagging \n",
    "all_df_kt['POSTags'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9f3755ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\Pree\\\\Thai_SA_journal\\\\model\\\\pos_bow2_ws.joblib']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create bow vectors\n",
    "bow1 = CountVectorizer(ngram_range=(1, 1))\n",
    "bow2 = CountVectorizer(ngram_range=(2, 2))\n",
    "\n",
    "pos_bow1_fit_kt = bow1.fit(all_df_kt['POSTags'].apply(str))\n",
    "pos_bow1_kt = pos_bow1_fit_kt.transform(all_df_kt['POSTags'].apply(str))\n",
    "pos_bow1_ws = pos_bow1_fit_kt.transform(all_df_ws['POSTags'].apply(str))\n",
    "\n",
    "pos_bow2_fit_kt = bow2.fit(all_df_kt['POSTags'].apply(str))\n",
    "pos_bow2_kt = pos_bow2_fit_kt.transform(all_df_kt['POSTags'].apply(str))\n",
    "pos_bow2_ws = pos_bow2_fit_kt.transform(all_df_ws['POSTags'].apply(str))\n",
    "\n",
    "joblib.dump(pos_bow1_kt, model_path+'pos_bow1_kt.joblib')\n",
    "joblib.dump(pos_bow1_ws, model_path+'pos_bow1_ws.joblib')\n",
    "joblib.dump(pos_bow2_kt, model_path+'pos_bow2_kt.joblib')\n",
    "joblib.dump(pos_bow2_ws, model_path+'pos_bow2_ws.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a11d2926",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\Pree\\\\Thai_SA_journal\\\\model\\\\pos_bow2_ws.joblib']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow1 = CountVectorizer(ngram_range=(1, 1))\n",
    "bow2 = CountVectorizer(ngram_range=(2, 2))\n",
    "\n",
    "pos_bow1_fit_kt = bow1.fit(all_df_kt['POSTags'].apply(str))\n",
    "pos_bow1_kt = pos_bow1_fit_kt.transform(all_df_kt['POSTags'].apply(str))\n",
    "pos_bow1_ws = pos_bow1_fit_kt.transform(all_df_ws['POSTags'].apply(str))\n",
    "\n",
    "pos_bow2_fit_kt = bow2.fit(all_df_kt['POSTags'].apply(str))\n",
    "pos_bow2_kt = pos_bow2_fit_kt.transform(all_df_kt['POSTags'].apply(str))\n",
    "pos_bow2_ws = pos_bow2_fit_kt.transform(all_df_ws['POSTags'].apply(str))\n",
    "\n",
    "joblib.dump(pos_bow1_kt, model_path+'pos_bow1_kt.joblib')\n",
    "joblib.dump(pos_bow1_ws, model_path+'pos_bow1_ws.joblib')\n",
    "joblib.dump(pos_bow2_kt, model_path+'pos_bow2_kt.joblib')\n",
    "joblib.dump(pos_bow2_ws, model_path+'pos_bow2_ws.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f1f5775d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\Pree\\\\Thai_SA_journal\\\\model\\\\pos_tfidf2_ws.joblib']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create tfidf vectors\n",
    "tfidf1 = TfidfVectorizer(ngram_range=(1, 1), min_df=20, sublinear_tf=True)\n",
    "tfidf2 = TfidfVectorizer(ngram_range=(2, 2), min_df=20, sublinear_tf=True)\n",
    "\n",
    "pos_tfidf1_fit_kt = tfidf1.fit(all_df_kt['POSTags'].apply(str))\n",
    "pos_tfidf1_kt = pos_tfidf1_fit_kt.transform(all_df_kt['POSTags'].apply(str))\n",
    "pos_tfidf1_ws = pos_tfidf1_fit_kt.transform(all_df_ws['POSTags'].apply(str))\n",
    "\n",
    "pos_tfidf2_fit_kt = tfidf2.fit(all_df_kt['POSTags'].apply(str))\n",
    "pos_tfidf2_kt = pos_tfidf2_fit_kt.transform(all_df_kt['POSTags'].apply(str))\n",
    "pos_tfidf2_ws = pos_tfidf2_fit_kt.transform(all_df_ws['POSTags'].apply(str))\n",
    "\n",
    "joblib.dump(pos_tfidf1_kt, model_path+'pos_tfidf1_kt.joblib')\n",
    "joblib.dump(pos_tfidf1_ws, model_path+'pos_tfidf1_ws.joblib')\n",
    "joblib.dump(pos_tfidf2_kt, model_path+'pos_tfidf2_kt.joblib')\n",
    "joblib.dump(pos_tfidf2_ws, model_path+'pos_tfidf2_ws.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c6949e",
   "metadata": {},
   "source": [
    "## Dictionary-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e922a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load list of our custom positive and negative words\n",
    "with open(os.path.dirname(os.getcwd()) + '\\\\data\\\\' + 'pos_words.txt', encoding='UTF-8') as f:\n",
    "    pos_words = [line.rstrip('\\n') for line in f]\n",
    "\n",
    "with open(os.path.dirname(os.getcwd()) + '\\\\data\\\\' + 'neg_words.txt', encoding='UTF-8') as f:\n",
    "    neg_words = [line.rstrip('\\n') for line in f]\n",
    "pos_words = list(set(pos_words))\n",
    "neg_words = list(set(neg_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad848e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_sentiment(sentence):\n",
    "    senti = 0\n",
    "    words = [word.lower() for word in sentence]\n",
    "    for word in words:\n",
    "        if word in pos_words:\n",
    "            senti += 1\n",
    "        elif word in neg_words:\n",
    "            senti -= 1\n",
    "    return senti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0bb454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see the correlation between the above computational sentiment and human ratings \n",
    "# for train set\n",
    "all_df_kt['targets_codes'] = all_df_kt['vote'].astype('category').cat.codes\n",
    "all_df_ws['targets_codes'] = all_df_ws['targets'].astype('category').cat.codes\n",
    "\n",
    "# for test set\n",
    "test_df_kt['targets_codes'] = test_df_kt['vote'].astype('category').cat.codes\n",
    "test_df_ws['targets_codes'] = test_df_ws['targets'].astype('category').cat.codes\n",
    "\n",
    "print(all_df_kt['targets_codes'].corr(all_df_kt['cal_sentiment']), \\\n",
    "      all_df_ws['targets_codes'].corr(all_df_ws['cal_sentiment']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa19d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df_kt['cal_sentiment'] = all_df_kt['processed'].apply(cal_sentiment)\n",
    "all_df_ws['cal_sentiment'] = all_df_ws['processed'].apply(cal_sentiment)\n",
    "\n",
    "test_df_kt['cal_sentiment'] = test_df_kt['processed'].apply(cal_sentiment)\n",
    "test_df_ws['cal_sentiment'] = test_df_ws['processed'].apply(cal_sentiment)\n",
    "all_df_kt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9787b7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create word count feature\n",
    "all_df_kt = all_df_kt.rename(columns = {'sent_length':'wc'})\n",
    "all_df_ws['wc'] =  all_df_ws['processed'].map(len)\n",
    "\n",
    "test_df_kt = all_df_kt.rename(columns = {'sent_length':'wc'})\n",
    "test_df_ws['wc'] =  test_df_ws['processed'].map(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77849de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see if the a correlation with word count\n",
    "print(all_df_kt['targets_codes'].corr(all_df_kt['wc']), \\\n",
    "      all_df_ws['targets_codes'].corr(all_df_ws['wc']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f1e6f2",
   "metadata": {},
   "source": [
    "## Train-valid split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c9ea8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_ws, valid_df_ws = train_test_split(all_df_ws, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb484ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train_ws = tfidf2_fit_kt.transform(train_df_ws['texts'].apply(str))\n",
    "text_valid_ws = tfidf2_fit_kt.transform(valid_df_ws['texts'].apply(str))\n",
    "text_test_ws = tfidf2_fit_kt.transform(test_df_ws['texts'].apply(str))\n",
    "text_train_ws.shape, text_valid_ws.shape, text_test_ws.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39ac867",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df_ws['targets'].value_counts() / valid_df_ws.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2917489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The calculated sentiment and word count features might be useful, so we concat them to the text feature\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaler_fit = scaler.fit(np.asarray(all_df_ws['cal_sentiment']).reshape(-1, 1))\n",
    "print(scaler_fit.mean_, scaler_fit.var_)\n",
    "\n",
    "cal_sent_train = scaler_fit.transform(np.asarray(train_df_ws['cal_sentiment']).reshape(-1, 1).astype(float))\n",
    "cal_sent_valid = scaler_fit.transform(np.asarray(valid_df_ws['cal_sentiment']).reshape(-1, 1).astype(float))\n",
    "cal_sent_test = scaler_fit.transform(np.asarray(test_df_ws['cal_sentiment']).reshape(-1, 1).astype(float))\n",
    "cal_sent_train.shape, cal_sent_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ad41d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_fit = scaler.fit(np.asarray(all_df_ws['wc']).reshape(-1, 1))\n",
    "print(scaler_fit.mean_, scaler_fit.var_)\n",
    "\n",
    "num_train = scaler_fit.transform(np.asarray(train_df_ws['wc']).reshape(-1, 1).astype(float))\n",
    "num_valid = scaler_fit.transform(np.asarray(valid_df_ws['wc']).reshape(-1, 1).astype(float))\n",
    "num_test = scaler_fit.transform(np.asarray(test_df_ws['wc']).reshape(-1, 1).astype(float))\n",
    "num_train.shape, num_valid.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3159c026",
   "metadata": {},
   "source": [
    "## Test the extracted features with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a96c00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_df_ws['targets']\n",
    "y_valid = valid_df_ws['targets']\n",
    "\n",
    "# concat text vector and generated feature vectors\n",
    "#X_train = np.concatenate([text_train_ws.toarray(), cal_sent_train, num_train], axis=1)\n",
    "#X_valid = np.concatenate([text_valid_ws.toarray(), cal_sent_valid, num_valid], axis=1)\n",
    "#X_test = np.concatenate([text_test_ws.toarray(), cal_sent_test, num_test], axis=1)\n",
    "\n",
    "#X_train = text_train_ws.toarray()\n",
    "#X_valid = text_valid_ws.toarray()\n",
    "#X_test = text_test_ws.toarray()\n",
    "\n",
    "X_train = text_train_ws\n",
    "X_valid  = text_valid_ws\n",
    "X_test = text_test_ws\n",
    "\n",
    "X_train.shape, X_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1413dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick test with out cv\n",
    "#fit logistic regression models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(C=2., penalty=\"l2\", solver=\"liblinear\", dual=False, multi_class=\"ovr\")\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_valid, y_valid)\n",
    "#y_pred = model.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f340fa9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.unique(y_pred) \n",
    "# this might cause from label 'q' is not present in the y_pred (due to the severe imbalance class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6282242",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
    "def build_model(model):\n",
    "    scores = (cross_val_score(model, X_train, y_train, cv = 5).mean())\n",
    "    model = model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_valid)\n",
    "    acc_sc = accuracy_score(y_valid, y_pred)\n",
    "    pre_sc = precision_score(y_valid, y_pred, average='weighted')\n",
    "    rec_sc = recall_score(y_valid, y_pred, average='weighted')\n",
    "    f1_sc = f1_score(y_valid, y_pred, average='weighted')\n",
    "    print('Accuracy :',acc_sc)\n",
    "    print('Confusion Matrix :\\n', confusion_matrix(y_valid, y_pred))\n",
    "    print('Precision :', pre_sc)\n",
    "    print('Recall :', rec_sc)\n",
    "    print('F1-score :', f1_sc)\n",
    "    print('Classification Report :\\n', classification_report(y_valid, y_pred))\n",
    "    print('Average accuracy of k-fold (5-fold) :', scores ,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e120d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "build_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a8761b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.unique(y_pred) \n",
    "# this might cause from label 'q' is not present in the y_pred (due to the severe imbalance class)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
