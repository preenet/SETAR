{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA of four Thai SA datasets\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import src.utilities as utils\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from src.feature.thai_tokenizer import ThaiTokenizer\n",
    "from src.feature.process_thai_text import process_text, process_text_old\n",
    "\n",
    "root = utils.get_project_root()\n",
    "config = utils.read_config()\n",
    " \n",
    "df_to = pd.read_csv(Path.joinpath(root, config['data']['processed_to']))\n",
    "#df_ws = pd.read_csv(Path.joinpath(root, config['data']['processed_ws']))\n",
    "# df_kt = pd.read_csv(Path.joinpath(root, config['data']['processed_kt']))\n",
    "# df_tt = pd.read_csv(Path.joinpath(root, config['data']['processed_tt']))\n",
    "\n",
    "#X_aa, y, Xt_aa, yt = joblib.load(Path.joinpath(root, 'data/processed/toxic_tweet_icdamt.sav'))\n",
    "\n",
    "#Xo, yo = joblib.load(Path.joinpath(root, config['data']['processed_kt']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the old Thai tokenizer from Bert-base thai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file ='th.wiki.bpe.op25000.vocab'\n",
    "spm_file ='th.wiki.bpe.op25000.model'\n",
    "tokenizer = ThaiTokenizer( vocab_file, spm_file)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Pree\\Thai_SA_journal\\notebooks\\corpus_explore.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Pree/Thai_SA_journal/notebooks/corpus_explore.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m Xo \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(process_text_old(item))  \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m df_ws[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mstr\u001b[39m)]\n",
      "\u001b[1;32mc:\\Users\\Pree\\Thai_SA_journal\\notebooks\\corpus_explore.ipynb Cell 4\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Pree/Thai_SA_journal/notebooks/corpus_explore.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m Xo \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(process_text_old(item))  \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m df_ws[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mstr\u001b[39m)]\n",
      "File \u001b[1;32m~\\Thai_SA_journal\\src\\feature\\process_thai_text.py:84\u001b[0m, in \u001b[0;36mprocess_text_old\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     81\u001b[0m vocab_file \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(Path\u001b[39m.\u001b[39mjoinpath(root, config[\u001b[39m'\u001b[39m\u001b[39mmodels\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39mth.wiki.bpe.op25000.vocab\u001b[39m\u001b[39m'\u001b[39m )) \n\u001b[0;32m     82\u001b[0m spm_file \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(Path\u001b[39m.\u001b[39mjoinpath(root, config[\u001b[39m'\u001b[39m\u001b[39mmodels\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39mth.wiki.bpe.op25000.model\u001b[39m\u001b[39m'\u001b[39m )) \n\u001b[1;32m---> 84\u001b[0m tokenizer \u001b[39m=\u001b[39m ThaiTokenizer(vocab_file\u001b[39m=\u001b[39;49mvocab_file, spm_file\u001b[39m=\u001b[39;49mspm_file)\n\u001b[0;32m     87\u001b[0m split_sentences \u001b[39m=\u001b[39m sent_tokenize(res)\n\u001b[0;32m     88\u001b[0m res \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(tokenizer\u001b[39m.\u001b[39mtokenize(\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(split_sentences)))\n",
      "File \u001b[1;32m~\\Thai_SA_journal\\src\\feature\\thai_tokenizer.py:60\u001b[0m, in \u001b[0;36mThaiTokenizer.__init__\u001b[1;34m(self, vocab_file, spm_file)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab \u001b[39m=\u001b[39m load_vocab(vocab_file)\n\u001b[0;32m     58\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minv_vocab \u001b[39m=\u001b[39m {v: k \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab\u001b[39m.\u001b[39mitems()}\n\u001b[1;32m---> 60\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbpe \u001b[39m=\u001b[39m BPE(vocab_file)    \n\u001b[0;32m     61\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ms \u001b[39m=\u001b[39m spm\u001b[39m.\u001b[39mSentencePieceProcessor()\n\u001b[0;32m     62\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ms\u001b[39m.\u001b[39mLoad(spm_file)\n",
      "File \u001b[1;32m~\\Thai_SA_journal\\bert\\bpe_helper.py:7\u001b[0m, in \u001b[0;36mBPE.__init__\u001b[1;34m(self, vocab_file)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, vocab_file):\n\u001b[1;32m----> 7\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(vocab_file, encoding\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mutf8\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m      8\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwords \u001b[39m=\u001b[39m [l\u001b[39m.\u001b[39msplit()[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m f]\n\u001b[0;32m      9\u001b[0m         log_len \u001b[39m=\u001b[39m log(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwords))\n",
      "File \u001b[1;32mc:\\Users\\Pree\\anaconda3\\envs\\sciGPU\\lib\\codecs.py:309\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.__init__\u001b[1;34m(self, errors)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mBufferedIncrementalDecoder\u001b[39;00m(IncrementalDecoder):\n\u001b[0;32m    304\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    305\u001b[0m \u001b[39m    This subclass of IncrementalDecoder can be used as the baseclass for an\u001b[39;00m\n\u001b[0;32m    306\u001b[0m \u001b[39m    incremental decoder if the decoder must be able to handle incomplete\u001b[39;00m\n\u001b[0;32m    307\u001b[0m \u001b[39m    byte sequences.\u001b[39;00m\n\u001b[0;32m    308\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 309\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, errors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mstrict\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m    310\u001b[0m         IncrementalDecoder\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, errors)\n\u001b[0;32m    311\u001b[0m         \u001b[39m# undecoded input that is kept between calls to decode()\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Xo = [' '.join(process_text_old(item))  for item in df_ws['text'].apply(str)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "random.shuffle(Xo)\n",
    "print(Xo[:10])\n",
    "print(type(Xo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Xo = np.vstack( (X_aa, Xt_aa))\n",
    "yo = np.hstack ( (y, yt))\n",
    "yo.shape\n",
    "yo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to['num_words'] = df_to['processed'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to['num_words'].plot.box()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to['num_words'].plot.box()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_to[df_to.num_words > 256]\n",
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df_to['processed'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(tokenizer=lambda x: x.split())\n",
    "v = vectorizer.fit(X_)\n",
    "# res = v.transform(df_to['processed'].iloc[0])\n",
    "# print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(lower=True)\n",
    "tokenizer.fit_on_texts(Xo)\n",
    "sequences_train_num = tokenizer.texts_to_sequences(Xo)\n",
    "max_len = max([len(w) for w in sequences_train_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long = max(sequences_train_num, key=len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_train_num = tokenizer.texts_to_sequences(Xo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_ds = df_ws['target'].astype('category').cat.codes\n",
    "y_ds = y_ds.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ds.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 ('sciGPU')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "67278d683e4e13f52f107db243d2a5105cc533d4ade030a7c7d7c3c729872230"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
