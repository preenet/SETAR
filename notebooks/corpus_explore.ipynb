{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA of four Thai SA datasets\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import src.utilities as utils\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from src.feature.thai_tokenizer import ThaiTokenizer\n",
    "from src.feature.process_thai_text import process_text, process_text_old\n",
    "import tensorflow as tf\n",
    "\n",
    "root = utils.get_project_root()\n",
    "config = utils.read_config()\n",
    " \n",
    "# df_to = pd.read_csv(Path.joinpath(root, config['data']['processed_to']))\n",
    "df_ws = pd.read_csv(Path.joinpath(root, config['data']['processed_ws']))\n",
    "# df_kt = pd.read_csv(Path.joinpath(root, config['data']['processed_kt']))\n",
    "#df_tt = pd.read_csv(Path.joinpath(root, config['data']['processed_tt']))\n",
    "\n",
    "#X_aa, y, Xt_aa, yt = joblib.load(Path.joinpath(root, 'data/processed/toxic_tweet_icdamt.sav'))\n",
    "\n",
    "#Xo, yo = joblib.load(Path.joinpath(root, config['data']['processed_kt']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xo = [' '.join(process_text(item))  for item in df_tt['text'].apply(str)]\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(lower=True)\n",
    "tokenizer.fit_on_texts(Xo)\n",
    "sequences_train_num = tokenizer.texts_to_sequences(Xo)\n",
    "max_len = max([len(w) for w in sequences_train_num])\n",
    "max_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the old Thai tokenizer from Bert-base thai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file ='th.wiki.bpe.op25000.vocab'\n",
    "spm_file ='th.wiki.bpe.op25000.model'\n",
    "tokenizer = ThaiTokenizer( vocab_file, spm_file)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pro_test is: ▁วัน ๆ ▁นี่ คุ ยก ะ หมา ▁แมว ▁หมู ▁ไก่ ▁ม้า ▁คว าย ▁มากกว่า คุย กับ คน ไป ละ\n"
     ]
    }
   ],
   "source": [
    "og_text = \"วันๆ นี่คุยกะหมา แมว หมู ไก่ ม้า ควาย มากกว่าคุยกับคนไปละ\"\n",
    "print (\"pro_test is:\", process_text_old(og_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_to['text'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xo = [process_text_old(item) for item in df_to['text'].apply(str)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁วัน ๆ ▁นี่ คุ ยก ะ หมา ▁แมว ▁หมู ▁ไก่ ▁ม้า ▁คว าย ▁มากกว่า คุย กับ คน ไป ละ', '▁หล ่อ มาก x x re p ▁หล ่อ วัว ตาย ควาย ล้ม กัน เลย ทีเดียว x x re p', '▁ส ิว เห ี้ย ไร ขึ้น หลัง หู ▁เสีย ชาติ เกิด ม ั้ ย ▁เกิด ม ามัน ต้อง ต้อง โดดเด่น เด้ ง ด ึ ๋ง อย่า เช่น ขึ้นที่ หน้า ไร ง ี้ ▁อี ควาย', '▁อ ่ะ ▁ป ่วยก ็ ป่วย ▁งาน จ้าง ก็ต้อง ทํา ▁งาน เรียน ก็ต้อง ทํา ▁วิชา ดีไซน์ ตัวร้าย กับ ควาย อย่าง กู เอง', '▁นี่ ก็ เพิ่ง รู้ว่า ▁เกิด ชาติน ี้ ชาติ เดียว ▁เป็นทั้ง ▁เห ี้ย ▁เป็นทั้ง ▁คว าย ▁เลย ▁คุ้ม แท้ ๆ ▁ <unk> gr inn ing - squ int ing - face <unk>', 'โว ้ย อี ควาย ▁หนัก กว่ าก ูก ็ม ึง อะ', '▁ขอ โทษ ▁นะ ครับ ▁อยาก ได้ ควาย ให้ ไป คบ กับ คนอื่น นะ พอดี กู คน ไม่ใช่ ควาย ▁ <unk>', '▁ปล ั๊ก ไฟ อยู่ พื้น ▁เลย ลง มาน อน บนพื้น ▁อี ควาย ▁ไฟ จะ ดู ดก ู ม ั้ ย ▁เส ียบ ชาร์ จ ไฟ ก็ แล บ ออกมา ▁อี เห ี้ย x x re p', '▁กู มัน โง ่ เอง ที่ยัง เชื่อ คํา สัญญา บ้า บอ ของคน คน นึง อยู่ . x x re p กู นี้ มัน ควาย จริงๆ ▁# <unk> sm ir king - face <unk>', '▁คว าย มัน อยู่บน ดิน ไง ▁มัน บิน ไม่ได้']\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(Xo[:10])\n",
    "print(type(Xo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Xo = np.vstack( (X_aa, Xt_aa))\n",
    "yo = np.hstack ( (y, yt))\n",
    "yo.shape\n",
    "yo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to['num_words'] = df_to['processed'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to['num_words'].plot.box()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to['num_words'].plot.box()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_to[df_to.num_words > 256]\n",
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df_to['processed'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(tokenizer=lambda x: x.split())\n",
    "v = vectorizer.fit(X_)\n",
    "# res = v.transform(df_to['processed'].iloc[0])\n",
    "# print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(lower=True)\n",
    "tokenizer.fit_on_texts(Xo)\n",
    "sequences_train_num = tokenizer.texts_to_sequences(Xo)\n",
    "max_len = max([len(w) for w in sequences_train_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long = max(sequences_train_num, key=len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_train_num = tokenizer.texts_to_sequences(Xo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_ds = df_ws['target'].astype('category').cat.codes\n",
    "y_ds = y_ds.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ds.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 ('sciGPU')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "67278d683e4e13f52f107db243d2a5105cc533d4ade030a7c7d7c3c729872230"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
