{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "139d4b2f",
   "metadata": {},
   "source": [
    "### Feature extractions\n",
    "\n",
    "This script extracted features from two sentimental corpora, kt4.0 (ours) and wisesight. By training from kt4.0 corpus, we expect to see an improvement in the wisesight corpus' classification performance.\n",
    "\n",
    "Several feature extraction methods were applied on text feature to both corpuses as follows:  \n",
    "\n",
    "* Bag of words for unigram and bigrams\n",
    "* TF-IDF for unigram and bigrams\n",
    "* Word2Vec with TF-IDF vector (300 dimension)\n",
    "* POS_tagging with flatten dataframe for unigram and bigrams\n",
    "* Dictionary-based with list of Thai positive and negative words for unigram and bigrams\n",
    "\n",
    "Output:  \n",
    "for all the feature extraction methods above, Joblib objects as numpy array and sparse matrix on text feature were dumped.     \n",
    "pree.t@cmu.ac.th  \n",
    "\n",
    "Todo:   \n",
    "* add thaitale corpus  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a978d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pythainlp\n",
    "import re\n",
    "import emoji\n",
    "import emot\n",
    "from pythainlp import word_tokenize\n",
    "\n",
    "# for visualize\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams['font.family'] = 'tahoma'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47e606a",
   "metadata": {},
   "source": [
    "## Load original datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fe523d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60081, 14), (26737, 2))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from emot.emo_unicode import UNICODE_EMOJI\n",
    "    \n",
    "os.path.dirname(os.getcwd())\n",
    "\n",
    "data_path_kt = os.path.dirname(os.getcwd()) + '\\\\data\\kt4.0\\\\'\n",
    "data_path_ws = os.path.dirname(os.getcwd()) + '\\\\data\\wisesight\\\\'\n",
    "model_path = os.path.dirname(os.getcwd()) + '\\\\model\\\\'\n",
    "lexicon_path = os.path.dirname(os.getcwd()) + '\\\\lexicon\\\\'\n",
    "\n",
    "df_kt = pd.read_csv(data_path_kt + 'pantip_cleaned_1.csv')\n",
    "\n",
    "# we use the original wisesight corpus and reconstruct a new dataframe\n",
    "texts = []\n",
    "targets = []\n",
    "\n",
    "with open(str(data_path_ws) + '\\\\' + 'neg.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        texts.append(line.strip())\n",
    "        targets.append('neg')\n",
    "\n",
    "with open(str(data_path_ws) + '\\\\' + 'neu.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        texts.append(line.strip())\n",
    "        targets.append('neu')\n",
    "\n",
    "with open(str(data_path_ws) + '\\\\' + 'pos.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        texts.append(line.strip())\n",
    "        targets.append('pos')\n",
    "\n",
    "with open(str(data_path_ws) + '\\\\' + 'q.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        texts.append(line.strip())\n",
    "        targets.append('q')\n",
    "        \n",
    "df_ws = pd.DataFrame({'texts': texts, 'targets': targets})\n",
    "df_ws.to_csv(os.path.dirname(os.getcwd()) + '\\\\' + 'wisesight.csv', index=False)\n",
    "df_kt.shape, df_ws.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57eaffe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2, 0, 1], dtype=int8), array([0, 1, 2, 3], dtype=int8))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_kt = df_kt['vote'].astype('category').cat.codes\n",
    "y_ws = df_ws['targets'].astype('category').cat.codes\n",
    "y_kt.unique(), y_ws.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b8219fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_url(text):\n",
    "    URL_PATTERN = r\"\"\"(?i)\\b((?:https?:(?:/{1,3}|[a-z0-9%])|[a-z0-9.\\-]+[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)/)(?:[^\\s()<>{}\\[\\]]+|\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\))+(?:\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\)|[^\\s`!()\\[\\]{};:'\".,<>?¬´¬ª‚Äú‚Äù‚Äò‚Äô])|(?:(?<!@)[a-z0-9]+(?:[.\\-][a-z0-9]+)*[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)\\b/?(?!@)))\"\"\"\n",
    "    return re.sub(URL_PATTERN, 'xxurl', text)\n",
    "\n",
    "def replace_rep(text):\n",
    "    def _replace_rep(m):\n",
    "        c,cc = m.groups()\n",
    "        return f'{c}xxrep'\n",
    "    re_rep = re.compile(r'(\\S)(\\1{2,})')\n",
    "    return re_rep.sub(_replace_rep, text)\n",
    "    \n",
    "\n",
    "def convert_emojis(text):\n",
    "    for emot in UNICODE_EMOJI:\n",
    "        text = text.replace(emot, \"_\".join(UNICODE_EMOJI[emot].replace(\"_\", \"-\").replace(\",\",\"\").replace(\":\",\"EMJ\").split()))\n",
    "    return text  \n",
    "\n",
    "def ungroup_emoji(toks):\n",
    "    res = []\n",
    "    for tok in toks:\n",
    "        if emoji.emoji_count(tok) == len(tok):\n",
    "            for char in tok:\n",
    "                res.append(char)\n",
    "        else:\n",
    "            res.append(tok)\n",
    "    return res\n",
    "\n",
    "def remove_all_extra_spaces(text):\n",
    "    return \" \".join(text.split())\n",
    "\n",
    "def remove_word_len(text):\n",
    "    new_list = [ word for word in text if len(word) >= 2 ]\n",
    "    return new_list\n",
    "\n",
    "def process_text(text):\n",
    "    #pre rules\n",
    "    res = text.lower().strip()\n",
    "    res = replace_url(res)\n",
    "    res = convert_emojis(res)\n",
    "    res = replace_rep(res)\n",
    "    \n",
    "    #tokenize with the newmm algo.\n",
    "    res = [word for word in word_tokenize(res, engine='newmm', keep_whitespace=False)if word and not re.search(pattern=r\"\\s+\", string=word)]\n",
    "    \n",
    "    #post rules\n",
    "    res = ungroup_emoji(res)\n",
    "    res = remove_word_len(res)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82ef6037",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kt['processed'] = df_kt['text'].apply(str).apply(process_text)\n",
    "df_ws['processed'] = df_ws['texts'].apply(str).apply(process_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "558ceee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texts</th>\n",
       "      <th>targets</th>\n",
       "      <th>processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>‚òπÔ∏è</td>\n",
       "      <td>neg</td>\n",
       "      <td>[EMJfrowning-faceEMJ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>üòî</td>\n",
       "      <td>neg</td>\n",
       "      <td>[EMJpensive-faceEMJ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>üòû</td>\n",
       "      <td>neg</td>\n",
       "      <td>[EMJdisappointed-faceEMJ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>üò•</td>\n",
       "      <td>neg</td>\n",
       "      <td>[EMJsad-but-relieved-faceEMJ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>‡∏£‡∏≥</td>\n",
       "      <td>neg</td>\n",
       "      <td>[‡∏£‡∏≥]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>No‡πÜ</td>\n",
       "      <td>neg</td>\n",
       "      <td>[no]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Rip</td>\n",
       "      <td>neg</td>\n",
       "      <td>[rip]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>T_T</td>\n",
       "      <td>neg</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>‡∏Å‡∏≤‡∏Å</td>\n",
       "      <td>neg</td>\n",
       "      <td>[‡∏Å‡∏≤‡∏Å]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>‡πÇ‡∏Å‡∏á</td>\n",
       "      <td>neg</td>\n",
       "      <td>[‡πÇ‡∏Å‡∏á]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  texts targets                      processed\n",
       "0    ‚òπÔ∏è     neg          [EMJfrowning-faceEMJ]\n",
       "1     üòî     neg           [EMJpensive-faceEMJ]\n",
       "2     üòû     neg      [EMJdisappointed-faceEMJ]\n",
       "3     üò•     neg  [EMJsad-but-relieved-faceEMJ]\n",
       "4    ‡∏£‡∏≥     neg                           [‡∏£‡∏≥]\n",
       "5   No‡πÜ     neg                           [no]\n",
       "6   Rip     neg                          [rip]\n",
       "7   T_T     neg                             []\n",
       "8   ‡∏Å‡∏≤‡∏Å     neg                          [‡∏Å‡∏≤‡∏Å]\n",
       "9   ‡πÇ‡∏Å‡∏á     neg                          [‡πÇ‡∏Å‡∏á]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ws[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c128ab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>post_date</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_name</th>\n",
       "      <th>text</th>\n",
       "      <th>tag</th>\n",
       "      <th>emotion</th>\n",
       "      <th>length</th>\n",
       "      <th>num_sent</th>\n",
       "      <th>sent_length</th>\n",
       "      <th>label</th>\n",
       "      <th>label_1</th>\n",
       "      <th>label_2</th>\n",
       "      <th>vote</th>\n",
       "      <th>processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39839097</td>\n",
       "      <td>2020-04-25 13:24:00</td>\n",
       "      <td>https://pantip.com/profile/5798163</td>\n",
       "      <td>‡∏™‡∏°‡∏≤‡∏ä‡∏¥‡∏Å‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏•‡∏Ç 5798163</td>\n",
       "      <td>[CR] ‡πÅ‡∏õ‡∏±‡∏á‡∏û‡∏±‡∏ü‡∏Ñ‡∏∏‡∏°‡∏°‡∏±‡∏ô ‡∏à‡∏±‡∏î‡πÄ‡∏ï‡πá‡∏°‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏ö‡∏≤‡∏á‡πÄ‡∏ö‡∏≤</td>\n",
       "      <td>‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏™‡∏≥‡∏≠‡∏≤‡∏á</td>\n",
       "      <td>‡∏ñ‡∏π‡∏Å‡πÉ‡∏à 0 ‡∏Ç‡∏≥‡∏Å‡∏•‡∏¥‡πâ‡∏á 0 ‡∏´‡∏•‡∏á‡∏£‡∏±‡∏Å 0 ‡∏ã‡∏∂‡πâ‡∏á 0 ‡∏™‡∏¢‡∏≠‡∏á 0 ‡∏ó‡∏∂‡πà‡∏á 0</td>\n",
       "      <td>36</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>pos</td>\n",
       "      <td>[cr, ‡πÅ‡∏õ, ‡∏±‡∏á‡∏û‡∏±‡∏ü, ‡∏Ñ‡∏∏‡∏°, ‡∏°‡∏±‡∏ô, ‡∏à‡∏±‡∏î, ‡πÄ‡∏ï‡πá‡∏°, ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠, ‡∏ö‡∏≤...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39839097</td>\n",
       "      <td>2020-04-25 13:24:00</td>\n",
       "      <td>https://pantip.com/profile/5798163</td>\n",
       "      <td>‡∏™‡∏°‡∏≤‡∏ä‡∏¥‡∏Å‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏•‡∏Ç 5798163</td>\n",
       "      <td>‡πÑ‡∏°‡πà‡∏≠‡∏∏‡∏î‡∏ï‡∏±‡∏ô ‡πÅ‡∏ï‡πà‡∏õ‡∏Å‡∏õ‡∏¥‡∏î‡πÅ‡∏ô‡πà‡∏ô‡∏°‡∏≤‡∏Å</td>\n",
       "      <td>‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏™‡∏≥‡∏≠‡∏≤‡∏á</td>\n",
       "      <td>‡∏ñ‡∏π‡∏Å‡πÉ‡∏à 0 ‡∏Ç‡∏≥‡∏Å‡∏•‡∏¥‡πâ‡∏á 0 ‡∏´‡∏•‡∏á‡∏£‡∏±‡∏Å 0 ‡∏ã‡∏∂‡πâ‡∏á 0 ‡∏™‡∏¢‡∏≠‡∏á 0 ‡∏ó‡∏∂‡πà‡∏á 0</td>\n",
       "      <td>36</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>pos</td>\n",
       "      <td>[‡πÑ‡∏°‡πà, ‡∏≠‡∏∏‡∏î‡∏ï‡∏±‡∏ô, ‡πÅ‡∏ï‡πà, ‡∏õ‡∏Å‡∏õ‡∏¥‡∏î, ‡πÅ‡∏ô‡πà‡∏ô, ‡∏°‡∏≤‡∏Å]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39839097</td>\n",
       "      <td>2020-04-25 13:24:00</td>\n",
       "      <td>https://pantip.com/profile/5798163</td>\n",
       "      <td>‡∏™‡∏°‡∏≤‡∏ä‡∏¥‡∏Å‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏•‡∏Ç 5798163</td>\n",
       "      <td>‡∏£‡∏µ‡∏ß‡∏¥‡∏ß‡πÅ‡∏õ‡πâ‡∏á Lady Audrey Ready All Day ‡∏à‡πâ‡∏≤</td>\n",
       "      <td>‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏™‡∏≥‡∏≠‡∏≤‡∏á</td>\n",
       "      <td>‡∏ñ‡∏π‡∏Å‡πÉ‡∏à 0 ‡∏Ç‡∏≥‡∏Å‡∏•‡∏¥‡πâ‡∏á 0 ‡∏´‡∏•‡∏á‡∏£‡∏±‡∏Å 0 ‡∏ã‡∏∂‡πâ‡∏á 0 ‡∏™‡∏¢‡∏≠‡∏á 0 ‡∏ó‡∏∂‡πà‡∏á 0</td>\n",
       "      <td>36</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>pos</td>\n",
       "      <td>[‡∏£‡∏µ‡∏ß‡∏¥‡∏ß, ‡πÅ‡∏õ‡πâ‡∏á, lady, audrey, ready, all, day, ‡∏à‡πâ‡∏≤]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39838736</td>\n",
       "      <td>2020-04-25 10:52:00</td>\n",
       "      <td>https://pantip.com/profile/5730006</td>\n",
       "      <td>‡∏™‡∏°‡∏≤‡∏ä‡∏¥‡∏Å‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏•‡∏Ç 5730006</td>\n",
       "      <td>‡∏Ç‡∏≠‡∏ö‡∏ï‡∏≤‡∏î‡∏≥‡∏°‡∏≤‡∏Å‡∏Ñ‡πà‡∏∞ ‡∏Ñ‡∏≠‡∏£‡πå‡πÄ‡∏•‡πá‡∏Ñ‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏Å‡πá‡πÄ‡∏≠‡∏≤‡πÑ‡∏°‡πà‡∏≠‡∏¢‡∏π‡πà</td>\n",
       "      <td>‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏™‡∏≥‡∏≠‡∏≤‡∏á</td>\n",
       "      <td>‡∏ñ‡∏π‡∏Å‡πÉ‡∏à 0 ‡∏Ç‡∏≥‡∏Å‡∏•‡∏¥‡πâ‡∏á 0 ‡∏´‡∏•‡∏á‡∏£‡∏±‡∏Å 0 ‡∏ã‡∏∂‡πâ‡∏á 0 ‡∏™‡∏¢‡∏≠‡∏á 0 ‡∏ó‡∏∂‡πà‡∏á 0</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>neg</td>\n",
       "      <td>[‡∏Ç‡∏≠‡∏ö, ‡∏ï‡∏≤‡∏î‡∏≥, ‡∏°‡∏≤‡∏Å, ‡∏Ñ‡πà‡∏∞, ‡∏Ñ‡∏≠‡∏£‡πå, ‡πÄ‡∏•‡πá‡∏Ñ, ‡πÄ‡∏ï, ‡∏≠‡∏£, ‡∏Å‡πá, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>39837384</td>\n",
       "      <td>2020-04-24 20:39:00</td>\n",
       "      <td>https://pantip.com/profile/4975838</td>\n",
       "      <td>‡∏™‡∏°‡∏≤‡∏ä‡∏¥‡∏Å‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏•‡∏Ç 4975838</td>\n",
       "      <td>‡πÄ‡∏≠‡∏≤aloe Vera ‡πÅ‡∏ä‡πà‡∏ï‡∏π‡πâ‡πÄ‡∏¢‡πá‡∏ô ‡∏à‡∏ô‡∏Å‡∏•‡∏≤‡∏¢‡πÄ‡∏õ‡πá‡∏ô‡∏ô‡πâ‡∏≥‡πÅ‡∏Ç‡πá‡∏á</td>\n",
       "      <td>‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏™‡∏≥‡∏≠‡∏≤‡∏á</td>\n",
       "      <td>‡∏ñ‡∏π‡∏Å‡πÉ‡∏à 0 ‡∏Ç‡∏≥‡∏Å‡∏•‡∏¥‡πâ‡∏á 0 ‡∏´‡∏•‡∏á‡∏£‡∏±‡∏Å 0 ‡∏ã‡∏∂‡πâ‡∏á 0 ‡∏™‡∏¢‡∏≠‡∏á 0 ‡∏ó‡∏∂‡πà‡∏á 0</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>neu</td>\n",
       "      <td>[‡πÄ‡∏≠‡∏≤, aloe, vera, ‡πÅ‡∏ä‡πà, ‡∏ï‡∏π‡πâ‡πÄ‡∏¢‡πá‡∏ô, ‡∏à‡∏ô, ‡∏Å‡∏•‡∏≤‡∏¢‡πÄ‡∏õ‡πá‡∏ô, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>39838990</td>\n",
       "      <td>2020-04-25 12:36:00</td>\n",
       "      <td>https://pantip.com/profile/5655853</td>\n",
       "      <td>chdewxx</td>\n",
       "      <td>[SR] ‡πÑ‡∏≠‡πÄ‡∏ó‡∏° #‡πÄ‡∏ã‡∏£‡∏±‡πà‡∏°‡∏™‡∏¥‡∏ß ‡∏•‡∏î‡∏™‡∏¥‡∏ß ‡∏™‡∏¥‡∏ß‡∏≠‡∏∏‡∏î‡∏ï‡∏±‡∏ô ‡∏™‡∏¥‡∏ß‡∏ú‡∏î ‡∏ö‡∏≥...</td>\n",
       "      <td>‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏™‡∏≥‡∏≠‡∏≤‡∏á</td>\n",
       "      <td>‡∏ñ‡∏π‡∏Å‡πÉ‡∏à 0 ‡∏Ç‡∏≥‡∏Å‡∏•‡∏¥‡πâ‡∏á 0 ‡∏´‡∏•‡∏á‡∏£‡∏±‡∏Å 0 ‡∏ã‡∏∂‡πâ‡∏á 0 ‡∏™‡∏¢‡∏≠‡∏á 0 ‡∏ó‡∏∂‡πà‡∏á 0</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>pos</td>\n",
       "      <td>[sr, ‡πÑ‡∏≠‡πÄ‡∏ó‡∏°, ‡πÄ‡∏ã, ‡∏£‡∏±‡πà‡∏°, ‡∏™‡∏¥‡∏ß, ‡∏•‡∏î, ‡∏™‡∏¥‡∏ß, ‡∏™‡∏¥‡∏ß, ‡∏≠‡∏∏‡∏î‡∏ï‡∏±...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>39838619</td>\n",
       "      <td>2020-04-25 10:01:00</td>\n",
       "      <td>https://pantip.com/profile/5656639</td>\n",
       "      <td>‡∏Ñ‡∏π‡∏à‡∏≠‡∏á‡∏¢‡∏≠‡∏ô‡πÅ‡∏•‡∏∞‡∏ß‡∏µ‡∏£‡∏¢‡∏≤</td>\n",
       "      <td>‡∏£‡∏ö‡∏Å‡∏ß‡∏ô‡∏™‡∏≤‡∏ß‡πÜ‡∏ä‡πà‡∏ß‡∏¢‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏™‡∏Å‡∏¥‡∏ô‡πÅ‡∏Ñ‡∏£‡πå ‡∏ó‡∏µ‡πà‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡∏ú‡∏¥‡∏ß‡∏´‡∏ô‡πâ‡∏≤‡∏Ç‡∏≤...</td>\n",
       "      <td>‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏™‡∏≥‡∏≠‡∏≤‡∏á</td>\n",
       "      <td>‡∏ñ‡∏π‡∏Å‡πÉ‡∏à 0 ‡∏Ç‡∏≥‡∏Å‡∏•‡∏¥‡πâ‡∏á 0 ‡∏´‡∏•‡∏á‡∏£‡∏±‡∏Å 0 ‡∏ã‡∏∂‡πâ‡∏á 0 ‡∏™‡∏¢‡∏≠‡∏á 0 ‡∏ó‡∏∂‡πà‡∏á 0</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>pos</td>\n",
       "      <td>[‡∏£‡∏ö‡∏Å‡∏ß‡∏ô, ‡∏™‡∏≤‡∏ß, ‡∏ä‡πà‡∏ß‡∏¢, ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥, ‡∏Å‡∏¥‡∏ô, ‡πÅ‡∏Ñ‡∏£‡πå, ‡∏ó‡∏µ‡πà, ‡∏ä‡πà‡∏ß‡∏¢...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>39837266</td>\n",
       "      <td>2020-04-24 19:58:00</td>\n",
       "      <td>https://pantip.com/profile/632132</td>\n",
       "      <td>‡∏´‡∏°‡∏π‡∏Å‡∏•‡∏°‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå‡∏î‡∏µ</td>\n",
       "      <td>‡∏ó‡∏î‡∏•‡∏≠‡∏á‡πÉ‡∏ä‡πâ ‡πÅ‡∏Ñ‡∏£‡∏≠‡∏ó‡∏ß‡∏¥‡∏ï‡∏ã‡∏µ‡∏´‡∏ô‡πâ‡∏≤‡πÉ‡∏™</td>\n",
       "      <td>‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏™‡∏≥‡∏≠‡∏≤‡∏á</td>\n",
       "      <td>‡∏ñ‡∏π‡∏Å‡πÉ‡∏à 0 ‡∏Ç‡∏≥‡∏Å‡∏•‡∏¥‡πâ‡∏á 0 ‡∏´‡∏•‡∏á‡∏£‡∏±‡∏Å 0 ‡∏ã‡∏∂‡πâ‡∏á 0 ‡∏™‡∏¢‡∏≠‡∏á 0 ‡∏ó‡∏∂‡πà‡∏á 0</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>pos</td>\n",
       "      <td>[‡∏ó‡∏î‡∏•‡∏≠‡∏á‡πÉ‡∏ä‡πâ, ‡πÅ‡∏Ñ‡∏£‡∏≠‡∏ó, ‡∏ß‡∏¥, ‡∏ã‡∏µ, ‡∏´‡∏ô‡πâ‡∏≤, ‡πÉ‡∏™]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>39837266</td>\n",
       "      <td>2020-04-24 19:58:00</td>\n",
       "      <td>https://pantip.com/profile/632132</td>\n",
       "      <td>‡∏´‡∏°‡∏π‡∏Å‡∏•‡∏°‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå‡∏î‡∏µ</td>\n",
       "      <td>‡πÉ‡∏ô 1 ‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå</td>\n",
       "      <td>‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏™‡∏≥‡∏≠‡∏≤‡∏á</td>\n",
       "      <td>‡∏ñ‡∏π‡∏Å‡πÉ‡∏à 0 ‡∏Ç‡∏≥‡∏Å‡∏•‡∏¥‡πâ‡∏á 0 ‡∏´‡∏•‡∏á‡∏£‡∏±‡∏Å 0 ‡∏ã‡∏∂‡πâ‡∏á 0 ‡∏™‡∏¢‡∏≠‡∏á 0 ‡∏ó‡∏∂‡πà‡∏á 0</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>neu</td>\n",
       "      <td>[‡πÉ‡∏ô, ‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>39835926</td>\n",
       "      <td>2020-04-24 12:03:00</td>\n",
       "      <td>https://pantip.com/profile/3826851</td>\n",
       "      <td>‡∏™‡∏°‡∏≤‡∏ä‡∏¥‡∏Å‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏•‡∏Ç 3826851</td>\n",
       "      <td>‡∏ß‡∏¥‡∏ò‡∏µ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å \"‡∏£‡∏≠‡∏á‡∏û‡∏∑‡πâ‡∏ô\" ‡πÅ‡∏•‡∏∞ \"‡∏Ñ‡∏≠‡∏ô‡∏ã‡∏µ‡∏•‡πÄ‡∏•‡∏≠‡∏£‡πå\"</td>\n",
       "      <td>‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏™‡∏≥‡∏≠‡∏≤‡∏á</td>\n",
       "      <td>‡∏ñ‡∏π‡∏Å‡πÉ‡∏à 0 ‡∏Ç‡∏≥‡∏Å‡∏•‡∏¥‡πâ‡∏á 0 ‡∏´‡∏•‡∏á‡∏£‡∏±‡∏Å 0 ‡∏ã‡∏∂‡πâ‡∏á 0 ‡∏™‡∏¢‡∏≠‡∏á 0 ‡∏ó‡∏∂‡πà‡∏á 0</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>pos</td>\n",
       "      <td>[‡∏ß‡∏¥‡∏ò‡∏µ, ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å, ‡∏£‡∏≠‡∏á‡∏û‡∏∑‡πâ‡∏ô, ‡πÅ‡∏•‡∏∞, ‡∏Ñ‡∏≠‡∏ô, ‡∏ã‡∏µ‡∏•, ‡πÄ‡∏•‡∏≠, ‡∏£‡πå\"]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    post_id            post_date                             user_id  \\\n",
       "0  39839097  2020-04-25 13:24:00  https://pantip.com/profile/5798163   \n",
       "1  39839097  2020-04-25 13:24:00  https://pantip.com/profile/5798163   \n",
       "2  39839097  2020-04-25 13:24:00  https://pantip.com/profile/5798163   \n",
       "3  39838736  2020-04-25 10:52:00  https://pantip.com/profile/5730006   \n",
       "4  39837384  2020-04-24 20:39:00  https://pantip.com/profile/4975838   \n",
       "5  39838990  2020-04-25 12:36:00  https://pantip.com/profile/5655853   \n",
       "6  39838619  2020-04-25 10:01:00  https://pantip.com/profile/5656639   \n",
       "7  39837266  2020-04-24 19:58:00   https://pantip.com/profile/632132   \n",
       "8  39837266  2020-04-24 19:58:00   https://pantip.com/profile/632132   \n",
       "9  39835926  2020-04-24 12:03:00  https://pantip.com/profile/3826851   \n",
       "\n",
       "               user_name                                               text  \\\n",
       "0  ‡∏™‡∏°‡∏≤‡∏ä‡∏¥‡∏Å‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏•‡∏Ç 5798163              [CR] ‡πÅ‡∏õ‡∏±‡∏á‡∏û‡∏±‡∏ü‡∏Ñ‡∏∏‡∏°‡∏°‡∏±‡∏ô ‡∏à‡∏±‡∏î‡πÄ‡∏ï‡πá‡∏°‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏ö‡∏≤‡∏á‡πÄ‡∏ö‡∏≤   \n",
       "1  ‡∏™‡∏°‡∏≤‡∏ä‡∏¥‡∏Å‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏•‡∏Ç 5798163                          ‡πÑ‡∏°‡πà‡∏≠‡∏∏‡∏î‡∏ï‡∏±‡∏ô ‡πÅ‡∏ï‡πà‡∏õ‡∏Å‡∏õ‡∏¥‡∏î‡πÅ‡∏ô‡πà‡∏ô‡∏°‡∏≤‡∏Å   \n",
       "2  ‡∏™‡∏°‡∏≤‡∏ä‡∏¥‡∏Å‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏•‡∏Ç 5798163            ‡∏£‡∏µ‡∏ß‡∏¥‡∏ß‡πÅ‡∏õ‡πâ‡∏á Lady Audrey Ready All Day ‡∏à‡πâ‡∏≤   \n",
       "3  ‡∏™‡∏°‡∏≤‡∏ä‡∏¥‡∏Å‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏•‡∏Ç 5730006            ‡∏Ç‡∏≠‡∏ö‡∏ï‡∏≤‡∏î‡∏≥‡∏°‡∏≤‡∏Å‡∏Ñ‡πà‡∏∞ ‡∏Ñ‡∏≠‡∏£‡πå‡πÄ‡∏•‡πá‡∏Ñ‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏Å‡πá‡πÄ‡∏≠‡∏≤‡πÑ‡∏°‡πà‡∏≠‡∏¢‡∏π‡πà   \n",
       "4  ‡∏™‡∏°‡∏≤‡∏ä‡∏¥‡∏Å‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏•‡∏Ç 4975838          ‡πÄ‡∏≠‡∏≤aloe Vera ‡πÅ‡∏ä‡πà‡∏ï‡∏π‡πâ‡πÄ‡∏¢‡πá‡∏ô ‡∏à‡∏ô‡∏Å‡∏•‡∏≤‡∏¢‡πÄ‡∏õ‡πá‡∏ô‡∏ô‡πâ‡∏≥‡πÅ‡∏Ç‡πá‡∏á   \n",
       "5                chdewxx  [SR] ‡πÑ‡∏≠‡πÄ‡∏ó‡∏° #‡πÄ‡∏ã‡∏£‡∏±‡πà‡∏°‡∏™‡∏¥‡∏ß ‡∏•‡∏î‡∏™‡∏¥‡∏ß ‡∏™‡∏¥‡∏ß‡∏≠‡∏∏‡∏î‡∏ï‡∏±‡∏ô ‡∏™‡∏¥‡∏ß‡∏ú‡∏î ‡∏ö‡∏≥...   \n",
       "6       ‡∏Ñ‡∏π‡∏à‡∏≠‡∏á‡∏¢‡∏≠‡∏ô‡πÅ‡∏•‡∏∞‡∏ß‡∏µ‡∏£‡∏¢‡∏≤  ‡∏£‡∏ö‡∏Å‡∏ß‡∏ô‡∏™‡∏≤‡∏ß‡πÜ‡∏ä‡πà‡∏ß‡∏¢‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏™‡∏Å‡∏¥‡∏ô‡πÅ‡∏Ñ‡∏£‡πå ‡∏ó‡∏µ‡πà‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡∏ú‡∏¥‡∏ß‡∏´‡∏ô‡πâ‡∏≤‡∏Ç‡∏≤...   \n",
       "7         ‡∏´‡∏°‡∏π‡∏Å‡∏•‡∏°‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå‡∏î‡∏µ                          ‡∏ó‡∏î‡∏•‡∏≠‡∏á‡πÉ‡∏ä‡πâ ‡πÅ‡∏Ñ‡∏£‡∏≠‡∏ó‡∏ß‡∏¥‡∏ï‡∏ã‡∏µ‡∏´‡∏ô‡πâ‡∏≤‡πÉ‡∏™   \n",
       "8         ‡∏´‡∏°‡∏π‡∏Å‡∏•‡∏°‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå‡∏î‡∏µ                                       ‡πÉ‡∏ô 1 ‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå   \n",
       "9  ‡∏™‡∏°‡∏≤‡∏ä‡∏¥‡∏Å‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏•‡∏Ç 3826851              ‡∏ß‡∏¥‡∏ò‡∏µ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å \"‡∏£‡∏≠‡∏á‡∏û‡∏∑‡πâ‡∏ô\" ‡πÅ‡∏•‡∏∞ \"‡∏Ñ‡∏≠‡∏ô‡∏ã‡∏µ‡∏•‡πÄ‡∏•‡∏≠‡∏£‡πå\"   \n",
       "\n",
       "            tag                                          emotion  length  \\\n",
       "0  ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏™‡∏≥‡∏≠‡∏≤‡∏á  ‡∏ñ‡∏π‡∏Å‡πÉ‡∏à 0 ‡∏Ç‡∏≥‡∏Å‡∏•‡∏¥‡πâ‡∏á 0 ‡∏´‡∏•‡∏á‡∏£‡∏±‡∏Å 0 ‡∏ã‡∏∂‡πâ‡∏á 0 ‡∏™‡∏¢‡∏≠‡∏á 0 ‡∏ó‡∏∂‡πà‡∏á 0      36   \n",
       "1  ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏™‡∏≥‡∏≠‡∏≤‡∏á  ‡∏ñ‡∏π‡∏Å‡πÉ‡∏à 0 ‡∏Ç‡∏≥‡∏Å‡∏•‡∏¥‡πâ‡∏á 0 ‡∏´‡∏•‡∏á‡∏£‡∏±‡∏Å 0 ‡∏ã‡∏∂‡πâ‡∏á 0 ‡∏™‡∏¢‡∏≠‡∏á 0 ‡∏ó‡∏∂‡πà‡∏á 0      36   \n",
       "2  ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏™‡∏≥‡∏≠‡∏≤‡∏á  ‡∏ñ‡∏π‡∏Å‡πÉ‡∏à 0 ‡∏Ç‡∏≥‡∏Å‡∏•‡∏¥‡πâ‡∏á 0 ‡∏´‡∏•‡∏á‡∏£‡∏±‡∏Å 0 ‡∏ã‡∏∂‡πâ‡∏á 0 ‡∏™‡∏¢‡∏≠‡∏á 0 ‡∏ó‡∏∂‡πà‡∏á 0      36   \n",
       "3  ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏™‡∏≥‡∏≠‡∏≤‡∏á  ‡∏ñ‡∏π‡∏Å‡πÉ‡∏à 0 ‡∏Ç‡∏≥‡∏Å‡∏•‡∏¥‡πâ‡∏á 0 ‡∏´‡∏•‡∏á‡∏£‡∏±‡∏Å 0 ‡∏ã‡∏∂‡πâ‡∏á 0 ‡∏™‡∏¢‡∏≠‡∏á 0 ‡∏ó‡∏∂‡πà‡∏á 0      15   \n",
       "4  ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏™‡∏≥‡∏≠‡∏≤‡∏á  ‡∏ñ‡∏π‡∏Å‡πÉ‡∏à 0 ‡∏Ç‡∏≥‡∏Å‡∏•‡∏¥‡πâ‡∏á 0 ‡∏´‡∏•‡∏á‡∏£‡∏±‡∏Å 0 ‡∏ã‡∏∂‡πâ‡∏á 0 ‡∏™‡∏¢‡∏≠‡∏á 0 ‡∏ó‡∏∂‡πà‡∏á 0      11   \n",
       "5  ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏™‡∏≥‡∏≠‡∏≤‡∏á  ‡∏ñ‡∏π‡∏Å‡πÉ‡∏à 0 ‡∏Ç‡∏≥‡∏Å‡∏•‡∏¥‡πâ‡∏á 0 ‡∏´‡∏•‡∏á‡∏£‡∏±‡∏Å 0 ‡∏ã‡∏∂‡πâ‡∏á 0 ‡∏™‡∏¢‡∏≠‡∏á 0 ‡∏ó‡∏∂‡πà‡∏á 0      29   \n",
       "6  ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏™‡∏≥‡∏≠‡∏≤‡∏á  ‡∏ñ‡∏π‡∏Å‡πÉ‡∏à 0 ‡∏Ç‡∏≥‡∏Å‡∏•‡∏¥‡πâ‡∏á 0 ‡∏´‡∏•‡∏á‡∏£‡∏±‡∏Å 0 ‡∏ã‡∏∂‡πâ‡∏á 0 ‡∏™‡∏¢‡∏≠‡∏á 0 ‡∏ó‡∏∂‡πà‡∏á 0      23   \n",
       "7  ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏™‡∏≥‡∏≠‡∏≤‡∏á  ‡∏ñ‡∏π‡∏Å‡πÉ‡∏à 0 ‡∏Ç‡∏≥‡∏Å‡∏•‡∏¥‡πâ‡∏á 0 ‡∏´‡∏•‡∏á‡∏£‡∏±‡∏Å 0 ‡∏ã‡∏∂‡πâ‡∏á 0 ‡∏™‡∏¢‡∏≠‡∏á 0 ‡∏ó‡∏∂‡πà‡∏á 0      14   \n",
       "8  ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏™‡∏≥‡∏≠‡∏≤‡∏á  ‡∏ñ‡∏π‡∏Å‡πÉ‡∏à 0 ‡∏Ç‡∏≥‡∏Å‡∏•‡∏¥‡πâ‡∏á 0 ‡∏´‡∏•‡∏á‡∏£‡∏±‡∏Å 0 ‡∏ã‡∏∂‡πâ‡∏á 0 ‡∏™‡∏¢‡∏≠‡∏á 0 ‡∏ó‡∏∂‡πà‡∏á 0      14   \n",
       "9  ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏™‡∏≥‡∏≠‡∏≤‡∏á  ‡∏ñ‡∏π‡∏Å‡πÉ‡∏à 0 ‡∏Ç‡∏≥‡∏Å‡∏•‡∏¥‡πâ‡∏á 0 ‡∏´‡∏•‡∏á‡∏£‡∏±‡∏Å 0 ‡∏ã‡∏∂‡πâ‡∏á 0 ‡∏™‡∏¢‡∏≠‡∏á 0 ‡∏ó‡∏∂‡πà‡∏á 0      14   \n",
       "\n",
       "   num_sent  sent_length  label  label_1  label_2 vote  \\\n",
       "0         3           14      2        2        2  pos   \n",
       "1         3            8      2        2        2  pos   \n",
       "2         3           14      2        2        1  pos   \n",
       "3         2           13      1        3        3  neg   \n",
       "4         1           11      1        1        3  neu   \n",
       "5         1           29      2        2        2  pos   \n",
       "6         1           23      2        2        1  pos   \n",
       "7         2            9      1        2        2  pos   \n",
       "8         2            5      1        1        1  neu   \n",
       "9         1           14      2        2        2  pos   \n",
       "\n",
       "                                           processed  \n",
       "0  [cr, ‡πÅ‡∏õ, ‡∏±‡∏á‡∏û‡∏±‡∏ü, ‡∏Ñ‡∏∏‡∏°, ‡∏°‡∏±‡∏ô, ‡∏à‡∏±‡∏î, ‡πÄ‡∏ï‡πá‡∏°, ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠, ‡∏ö‡∏≤...  \n",
       "1               [‡πÑ‡∏°‡πà, ‡∏≠‡∏∏‡∏î‡∏ï‡∏±‡∏ô, ‡πÅ‡∏ï‡πà, ‡∏õ‡∏Å‡∏õ‡∏¥‡∏î, ‡πÅ‡∏ô‡πà‡∏ô, ‡∏°‡∏≤‡∏Å]  \n",
       "2  [‡∏£‡∏µ‡∏ß‡∏¥‡∏ß, ‡πÅ‡∏õ‡πâ‡∏á, lady, audrey, ready, all, day, ‡∏à‡πâ‡∏≤]  \n",
       "3  [‡∏Ç‡∏≠‡∏ö, ‡∏ï‡∏≤‡∏î‡∏≥, ‡∏°‡∏≤‡∏Å, ‡∏Ñ‡πà‡∏∞, ‡∏Ñ‡∏≠‡∏£‡πå, ‡πÄ‡∏•‡πá‡∏Ñ, ‡πÄ‡∏ï, ‡∏≠‡∏£, ‡∏Å‡πá, ...  \n",
       "4  [‡πÄ‡∏≠‡∏≤, aloe, vera, ‡πÅ‡∏ä‡πà, ‡∏ï‡∏π‡πâ‡πÄ‡∏¢‡πá‡∏ô, ‡∏à‡∏ô, ‡∏Å‡∏•‡∏≤‡∏¢‡πÄ‡∏õ‡πá‡∏ô, ...  \n",
       "5  [sr, ‡πÑ‡∏≠‡πÄ‡∏ó‡∏°, ‡πÄ‡∏ã, ‡∏£‡∏±‡πà‡∏°, ‡∏™‡∏¥‡∏ß, ‡∏•‡∏î, ‡∏™‡∏¥‡∏ß, ‡∏™‡∏¥‡∏ß, ‡∏≠‡∏∏‡∏î‡∏ï‡∏±...  \n",
       "6  [‡∏£‡∏ö‡∏Å‡∏ß‡∏ô, ‡∏™‡∏≤‡∏ß, ‡∏ä‡πà‡∏ß‡∏¢, ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥, ‡∏Å‡∏¥‡∏ô, ‡πÅ‡∏Ñ‡∏£‡πå, ‡∏ó‡∏µ‡πà, ‡∏ä‡πà‡∏ß‡∏¢...  \n",
       "7                [‡∏ó‡∏î‡∏•‡∏≠‡∏á‡πÉ‡∏ä‡πâ, ‡πÅ‡∏Ñ‡∏£‡∏≠‡∏ó, ‡∏ß‡∏¥, ‡∏ã‡∏µ, ‡∏´‡∏ô‡πâ‡∏≤, ‡πÉ‡∏™]  \n",
       "8                                      [‡πÉ‡∏ô, ‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå]  \n",
       "9    [‡∏ß‡∏¥‡∏ò‡∏µ, ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å, ‡∏£‡∏≠‡∏á‡∏û‡∏∑‡πâ‡∏ô, ‡πÅ‡∏•‡∏∞, ‡∏Ñ‡∏≠‡∏ô, ‡∏ã‡∏µ‡∏•, ‡πÄ‡∏•‡∏≠, ‡∏£‡πå\"]  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_kt.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d84766f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>length</th>\n",
       "      <th>num_sent</th>\n",
       "      <th>sent_length</th>\n",
       "      <th>label</th>\n",
       "      <th>label_1</th>\n",
       "      <th>label_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6.008100e+04</td>\n",
       "      <td>60081.000000</td>\n",
       "      <td>60081.000000</td>\n",
       "      <td>60081.000000</td>\n",
       "      <td>60081.000000</td>\n",
       "      <td>60081.000000</td>\n",
       "      <td>60081.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.964936e+07</td>\n",
       "      <td>116.994574</td>\n",
       "      <td>8.502172</td>\n",
       "      <td>13.978329</td>\n",
       "      <td>1.577304</td>\n",
       "      <td>1.362644</td>\n",
       "      <td>1.662156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.559919e+05</td>\n",
       "      <td>118.647716</td>\n",
       "      <td>7.575442</td>\n",
       "      <td>12.083572</td>\n",
       "      <td>0.777527</td>\n",
       "      <td>0.639271</td>\n",
       "      <td>0.800034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.917283e+07</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.958755e+07</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.968929e+07</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.976947e+07</td>\n",
       "      <td>159.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.983970e+07</td>\n",
       "      <td>499.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>301.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            post_id        length      num_sent   sent_length         label  \\\n",
       "count  6.008100e+04  60081.000000  60081.000000  60081.000000  60081.000000   \n",
       "mean   3.964936e+07    116.994574      8.502172     13.978329      1.577304   \n",
       "std    1.559919e+05    118.647716      7.575442     12.083572      0.777527   \n",
       "min    3.917283e+07      3.000000      1.000000      3.000000      1.000000   \n",
       "25%    3.958755e+07     31.000000      3.000000      6.000000      1.000000   \n",
       "50%    3.968929e+07     72.000000      6.000000     10.000000      1.000000   \n",
       "75%    3.976947e+07    159.000000     11.000000     17.000000      2.000000   \n",
       "max    3.983970e+07    499.000000     44.000000    301.000000      3.000000   \n",
       "\n",
       "            label_1       label_2  \n",
       "count  60081.000000  60081.000000  \n",
       "mean       1.362644      1.662156  \n",
       "std        0.639271      0.800034  \n",
       "min        1.000000      1.000000  \n",
       "25%        1.000000      1.000000  \n",
       "50%        1.000000      1.000000  \n",
       "75%        2.000000      2.000000  \n",
       "max        3.000000      3.000000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_kt.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5ec5386",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texts</th>\n",
       "      <th>targets</th>\n",
       "      <th>processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26727</th>\n",
       "      <td>‡∏û‡∏µ‡πà‡∏Ç‡∏ß‡∏±‡∏ç‡∏Ñ‡∏∞ ‡∏ñ‡πâ‡∏≤‡πÄ‡∏£‡∏≤‡∏°‡∏µ ‡∏ã‡∏∏‡∏•‡∏ß‡∏≤‡∏ã‡∏∏‡∏•‡πÄ‡∏ü‡∏¥‡∏£‡πå‡∏™‡πÅ‡∏Ñ‡∏£‡πå ‡∏Å‡∏±‡∏ö vich...</td>\n",
       "      <td>q</td>\n",
       "      <td>[‡∏û‡∏µ‡πà, ‡∏Ç‡∏ß‡∏±‡∏ç, ‡∏Ñ‡∏∞, ‡∏ñ‡πâ‡∏≤, ‡πÄ‡∏£‡∏≤, ‡∏°‡∏µ, ‡∏ã‡∏∏‡∏•, ‡∏ß‡∏≤, ‡∏ã‡∏∏‡∏•‡πÄ‡∏ü‡∏¥‡∏£...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26728</th>\n",
       "      <td>‡∏°‡∏µ‡∏™‡∏≤‡∏ß‡∏ú‡∏¥‡∏ß‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏á‡∏Ñ‡∏ô‡πÑ‡∏´‡∏ô‡∏•‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡∏£‡∏≠‡∏á‡∏û‡∏∑‡πâ‡∏ô‡∏£‡∏∏‡πà‡∏ô‡πÉ‡∏´‡∏°‡πà‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î...</td>\n",
       "      <td>q</td>\n",
       "      <td>[‡∏°‡∏µ, ‡∏™‡∏≤‡∏ß, ‡∏ú‡∏¥‡∏ß‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏á, ‡∏Ñ‡∏ô, ‡πÑ‡∏´‡∏ô, ‡∏•‡∏≠‡∏á, ‡πÉ‡∏ä‡πâ, ‡∏£‡∏≠‡∏á‡∏û‡∏∑‡πâ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26729</th>\n",
       "      <td>‡πÅ‡∏≠‡∏î‡∏°‡∏¥‡∏ô‡∏Ñ‡∏∞ ‡∏ó‡∏≥‡πÑ‡∏° True Money Wallet ‡∏´‡∏±‡∏Å‡πÄ‡∏á‡∏¥‡∏ô‡∏Ñ‡∏á‡πÄ‡∏´‡∏•‡∏∑‡∏≠...</td>\n",
       "      <td>q</td>\n",
       "      <td>[‡πÅ‡∏≠‡∏î‡∏°‡∏¥‡∏ô, ‡∏Ñ‡∏∞, ‡∏ó‡∏≥‡πÑ‡∏°, true, money, wallet, ‡∏´‡∏±‡∏Å, ‡πÄ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26730</th>\n",
       "      <td>‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏•‡∏¥‡∏õ‡∏Ç‡∏≠‡∏á cute press ‡∏™‡∏µ‡∏™‡πâ‡∏°‡∏≠‡∏¥‡∏ê‡∏´‡∏ô‡πà‡∏≠‡∏¢‡∏Ñ‡∏£‡∏±‡∏ö ‡∏≠‡∏±‡∏ô‡πÑ‡∏´...</td>\n",
       "      <td>q</td>\n",
       "      <td>[‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥, ‡∏•‡∏¥‡∏õ, ‡∏Ç‡∏≠‡∏á, cute, press, ‡∏™‡∏µ‡∏™‡πâ‡∏°, ‡∏≠‡∏¥‡∏ê, ‡∏´‡∏ô‡πà...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26731</th>\n",
       "      <td>‡πÄ‡∏ß‡∏•‡∏≤‡∏ß‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß 80 ‡∏Ç‡∏∂‡πâ‡∏ô‡πÑ‡∏õ‡∏û‡∏ß‡∏á‡∏°‡∏≤‡∏•‡∏±‡∏¢‡∏à‡∏∞‡∏™‡∏±‡πà‡∏ô‡∏Ñ‡∏£‡∏±...</td>\n",
       "      <td>q</td>\n",
       "      <td>[‡πÄ‡∏ß‡∏•‡∏≤, ‡∏ß‡∏¥‡πà‡∏á, ‡∏ó‡∏µ‡πà, ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß, 80, ‡∏Ç‡∏∂‡πâ‡∏ô‡πÑ‡∏õ, ‡∏û‡∏ß‡∏á‡∏°‡∏≤‡∏•...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26732</th>\n",
       "      <td>‡∏ß‡∏¥‡∏ò‡∏µ‡∏ã‡∏∑‡πâ‡∏≠‡∏£‡∏ñ‡∏°‡∏∑‡∏≠‡∏™‡∏≠‡∏á‡∏ï‡πâ‡∏≠‡∏á‡∏î‡∏π‡∏¢‡∏±‡∏á‡πÑ‡∏á‡∏î‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö‡∏ï‡∏≠‡∏ô‡∏ô‡∏±‡πâ‡∏ô‡∏û‡∏µ‡∏à‡∏¥‡∏°‡πÅ...</td>\n",
       "      <td>q</td>\n",
       "      <td>[‡∏ß‡∏¥‡∏ò‡∏µ, ‡∏ã‡∏∑‡πâ‡∏≠, ‡∏£‡∏ñ, ‡∏°‡∏∑‡∏≠‡∏™‡∏≠‡∏á, ‡∏ï‡πâ‡∏≠‡∏á, ‡∏î‡∏π, ‡∏¢‡∏±‡∏á‡πÑ‡∏á, ‡∏î‡∏µ, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26733</th>\n",
       "      <td>- ‡∏ô‡πâ‡∏≥‡∏°‡∏±‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á ‡∏ó‡∏±‡πâ‡∏á Fully / Semi ‡πÄ‡∏ö‡∏ô‡∏ã‡∏¥‡∏ô - ‡∏ô‡πâ‡∏≥...</td>\n",
       "      <td>q</td>\n",
       "      <td>[‡∏ô‡πâ‡∏≥‡∏°‡∏±‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á, ‡∏ó‡∏±‡πâ‡∏á, fully, semi, ‡πÄ‡∏ö‡∏ô‡∏ã‡∏¥‡∏ô, ‡∏ô‡πâ‡∏≥...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26734</th>\n",
       "      <td>Honda Civic Hatchback ‡∏ó‡∏µ‡πà‡∏°‡∏≤‡∏Å‡∏±‡∏ö‡∏™‡∏µ‡πÅ‡∏î‡∏á ‡∏£‡∏≤‡∏Ñ‡∏≤‡πÄ‡∏ó‡πà‡∏≤‡πÄ‡∏î...</td>\n",
       "      <td>q</td>\n",
       "      <td>[honda, civic, hatchback, ‡∏ó‡∏µ‡πà‡∏°‡∏≤, ‡∏Å‡∏±‡∏ö, ‡∏™‡∏µ‡πÅ‡∏î‡∏á, ‡∏£...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26735</th>\n",
       "      <td>‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏à‡∏∞‡πÄ‡∏£‡∏¥‡πà‡∏° skincare 2 ‡∏ï‡∏±‡∏ß‡∏Ñ‡∏∑‡∏≠ stemfactor ‡∏Å‡∏±‡∏ö ...</td>\n",
       "      <td>q</td>\n",
       "      <td>[‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏à‡∏∞, ‡πÄ‡∏£‡∏¥‡πà‡∏°, skincare, ‡∏ï‡∏±‡∏ß, ‡∏Ñ‡∏∑‡∏≠, stemfacto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26736</th>\n",
       "      <td>‡∏û‡∏µ‡πà‡∏Ñ‡∏∞ ‡∏´‡∏ô‡∏π‡∏≠‡∏¢‡∏≤‡∏Å‡∏™‡∏≠‡∏ö‡∏ñ‡∏≤‡∏°‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ô‡∏∂‡∏á‡∏Ñ‡πà‡∏∞ ‡∏≠‡∏≤‡∏à‡∏∞‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å...</td>\n",
       "      <td>q</td>\n",
       "      <td>[‡∏û‡∏µ‡πà, ‡∏Ñ‡∏∞, ‡∏´‡∏ô‡∏π, ‡∏≠‡∏¢‡∏≤‡∏Å, ‡∏™‡∏≠‡∏ö‡∏ñ‡∏≤‡∏°, ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á, ‡∏ô‡∏∂‡∏á, ‡∏Ñ‡πà‡∏∞,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   texts targets  \\\n",
       "26727  ‡∏û‡∏µ‡πà‡∏Ç‡∏ß‡∏±‡∏ç‡∏Ñ‡∏∞ ‡∏ñ‡πâ‡∏≤‡πÄ‡∏£‡∏≤‡∏°‡∏µ ‡∏ã‡∏∏‡∏•‡∏ß‡∏≤‡∏ã‡∏∏‡∏•‡πÄ‡∏ü‡∏¥‡∏£‡πå‡∏™‡πÅ‡∏Ñ‡∏£‡πå ‡∏Å‡∏±‡∏ö vich...       q   \n",
       "26728  ‡∏°‡∏µ‡∏™‡∏≤‡∏ß‡∏ú‡∏¥‡∏ß‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏á‡∏Ñ‡∏ô‡πÑ‡∏´‡∏ô‡∏•‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡∏£‡∏≠‡∏á‡∏û‡∏∑‡πâ‡∏ô‡∏£‡∏∏‡πà‡∏ô‡πÉ‡∏´‡∏°‡πà‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î...       q   \n",
       "26729  ‡πÅ‡∏≠‡∏î‡∏°‡∏¥‡∏ô‡∏Ñ‡∏∞ ‡∏ó‡∏≥‡πÑ‡∏° True Money Wallet ‡∏´‡∏±‡∏Å‡πÄ‡∏á‡∏¥‡∏ô‡∏Ñ‡∏á‡πÄ‡∏´‡∏•‡∏∑‡∏≠...       q   \n",
       "26730  ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏•‡∏¥‡∏õ‡∏Ç‡∏≠‡∏á cute press ‡∏™‡∏µ‡∏™‡πâ‡∏°‡∏≠‡∏¥‡∏ê‡∏´‡∏ô‡πà‡∏≠‡∏¢‡∏Ñ‡∏£‡∏±‡∏ö ‡∏≠‡∏±‡∏ô‡πÑ‡∏´...       q   \n",
       "26731  ‡πÄ‡∏ß‡∏•‡∏≤‡∏ß‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß 80 ‡∏Ç‡∏∂‡πâ‡∏ô‡πÑ‡∏õ‡∏û‡∏ß‡∏á‡∏°‡∏≤‡∏•‡∏±‡∏¢‡∏à‡∏∞‡∏™‡∏±‡πà‡∏ô‡∏Ñ‡∏£‡∏±...       q   \n",
       "26732  ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ã‡∏∑‡πâ‡∏≠‡∏£‡∏ñ‡∏°‡∏∑‡∏≠‡∏™‡∏≠‡∏á‡∏ï‡πâ‡∏≠‡∏á‡∏î‡∏π‡∏¢‡∏±‡∏á‡πÑ‡∏á‡∏î‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö‡∏ï‡∏≠‡∏ô‡∏ô‡∏±‡πâ‡∏ô‡∏û‡∏µ‡∏à‡∏¥‡∏°‡πÅ...       q   \n",
       "26733  - ‡∏ô‡πâ‡∏≥‡∏°‡∏±‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á ‡∏ó‡∏±‡πâ‡∏á Fully / Semi ‡πÄ‡∏ö‡∏ô‡∏ã‡∏¥‡∏ô - ‡∏ô‡πâ‡∏≥...       q   \n",
       "26734  Honda Civic Hatchback ‡∏ó‡∏µ‡πà‡∏°‡∏≤‡∏Å‡∏±‡∏ö‡∏™‡∏µ‡πÅ‡∏î‡∏á ‡∏£‡∏≤‡∏Ñ‡∏≤‡πÄ‡∏ó‡πà‡∏≤‡πÄ‡∏î...       q   \n",
       "26735  ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏à‡∏∞‡πÄ‡∏£‡∏¥‡πà‡∏° skincare 2 ‡∏ï‡∏±‡∏ß‡∏Ñ‡∏∑‡∏≠ stemfactor ‡∏Å‡∏±‡∏ö ...       q   \n",
       "26736  ‡∏û‡∏µ‡πà‡∏Ñ‡∏∞ ‡∏´‡∏ô‡∏π‡∏≠‡∏¢‡∏≤‡∏Å‡∏™‡∏≠‡∏ö‡∏ñ‡∏≤‡∏°‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ô‡∏∂‡∏á‡∏Ñ‡πà‡∏∞ ‡∏≠‡∏≤‡∏à‡∏∞‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å...       q   \n",
       "\n",
       "                                               processed  \n",
       "26727  [‡∏û‡∏µ‡πà, ‡∏Ç‡∏ß‡∏±‡∏ç, ‡∏Ñ‡∏∞, ‡∏ñ‡πâ‡∏≤, ‡πÄ‡∏£‡∏≤, ‡∏°‡∏µ, ‡∏ã‡∏∏‡∏•, ‡∏ß‡∏≤, ‡∏ã‡∏∏‡∏•‡πÄ‡∏ü‡∏¥‡∏£...  \n",
       "26728  [‡∏°‡∏µ, ‡∏™‡∏≤‡∏ß, ‡∏ú‡∏¥‡∏ß‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏á, ‡∏Ñ‡∏ô, ‡πÑ‡∏´‡∏ô, ‡∏•‡∏≠‡∏á, ‡πÉ‡∏ä‡πâ, ‡∏£‡∏≠‡∏á‡∏û‡∏∑‡πâ...  \n",
       "26729  [‡πÅ‡∏≠‡∏î‡∏°‡∏¥‡∏ô, ‡∏Ñ‡∏∞, ‡∏ó‡∏≥‡πÑ‡∏°, true, money, wallet, ‡∏´‡∏±‡∏Å, ‡πÄ...  \n",
       "26730  [‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥, ‡∏•‡∏¥‡∏õ, ‡∏Ç‡∏≠‡∏á, cute, press, ‡∏™‡∏µ‡∏™‡πâ‡∏°, ‡∏≠‡∏¥‡∏ê, ‡∏´‡∏ô‡πà...  \n",
       "26731  [‡πÄ‡∏ß‡∏•‡∏≤, ‡∏ß‡∏¥‡πà‡∏á, ‡∏ó‡∏µ‡πà, ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß, 80, ‡∏Ç‡∏∂‡πâ‡∏ô‡πÑ‡∏õ, ‡∏û‡∏ß‡∏á‡∏°‡∏≤‡∏•...  \n",
       "26732  [‡∏ß‡∏¥‡∏ò‡∏µ, ‡∏ã‡∏∑‡πâ‡∏≠, ‡∏£‡∏ñ, ‡∏°‡∏∑‡∏≠‡∏™‡∏≠‡∏á, ‡∏ï‡πâ‡∏≠‡∏á, ‡∏î‡∏π, ‡∏¢‡∏±‡∏á‡πÑ‡∏á, ‡∏î‡∏µ, ...  \n",
       "26733  [‡∏ô‡πâ‡∏≥‡∏°‡∏±‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á, ‡∏ó‡∏±‡πâ‡∏á, fully, semi, ‡πÄ‡∏ö‡∏ô‡∏ã‡∏¥‡∏ô, ‡∏ô‡πâ‡∏≥...  \n",
       "26734  [honda, civic, hatchback, ‡∏ó‡∏µ‡πà‡∏°‡∏≤, ‡∏Å‡∏±‡∏ö, ‡∏™‡∏µ‡πÅ‡∏î‡∏á, ‡∏£...  \n",
       "26735  [‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏à‡∏∞, ‡πÄ‡∏£‡∏¥‡πà‡∏°, skincare, ‡∏ï‡∏±‡∏ß, ‡∏Ñ‡∏∑‡∏≠, stemfacto...  \n",
       "26736  [‡∏û‡∏µ‡πà, ‡∏Ñ‡∏∞, ‡∏´‡∏ô‡∏π, ‡∏≠‡∏¢‡∏≤‡∏Å, ‡∏™‡∏≠‡∏ö‡∏ñ‡∏≤‡∏°, ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á, ‡∏ô‡∏∂‡∏á, ‡∏Ñ‡πà‡∏∞,...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ws.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7077f068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texts</th>\n",
       "      <th>targets</th>\n",
       "      <th>processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>26737</td>\n",
       "      <td>26737</td>\n",
       "      <td>26737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>26713</td>\n",
       "      <td>4</td>\n",
       "      <td>26383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>‡∏≠‡∏∏‡∏î‡∏£‡∏°‡∏µ‡πÑ‡∏´‡∏°‡∏Ñ‡πà‡∏∞</td>\n",
       "      <td>neu</td>\n",
       "      <td>[xxrep]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>2</td>\n",
       "      <td>14561</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               texts targets processed\n",
       "count          26737   26737     26737\n",
       "unique         26713       4     26383\n",
       "top     ‡∏≠‡∏∏‡∏î‡∏£‡∏°‡∏µ‡πÑ‡∏´‡∏°‡∏Ñ‡πà‡∏∞     neu   [xxrep]\n",
       "freq               2   14561        16"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ws.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c7d3bcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neu    0.633378\n",
       "pos    0.205822\n",
       "neg    0.160800\n",
       "Name: vote, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# class distribution\n",
    "df_kt.vote.value_counts() / df_kt.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae548ccd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neu    0.544601\n",
       "neg    0.255189\n",
       "pos    0.178704\n",
       "q      0.021506\n",
       "Name: targets, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# class distribution\n",
    "df_ws.targets.value_counts() / df_ws.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c6e20f",
   "metadata": {},
   "source": [
    "## Bag of words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b2eb3e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# BOW with unigram and bigrams\n",
    "bow1 = CountVectorizer(max_features =1394)\n",
    "#bow2 = CountVectorizer(tokenizer=process_text, ngram_range=(2, 2), min_df=5)\n",
    "\n",
    "# fit kt and transform to both datasets\n",
    "bow1_fit_kt = bow1.fit(df_kt['text'].apply(str))\n",
    "text_bow1_kt = bow1_fit_kt.transform(df_kt['text'].apply(str))\n",
    "# lex_bow1_kt = bow1_fit_kt.get_feature_names()\n",
    "# text_bow1_ws = bow1_fit_kt.transform(df_ws['texts'].apply(str))\n",
    "# lex_bow1_ws = bow1_fit_kt.get_feature_names()\n",
    "\n",
    "# bow2_fit_kt = bow2.fit(df_kt['text'].apply(str))\n",
    "# text_bow2_kt = bow2_fit_kt.transform(df_kt['text'].apply(str))\n",
    "# lex_bow2_kt = bow2_fit_kt.get_feature_names()\n",
    "# text_bow2_ws = bow2_fit_kt.transform(df_ws['texts'].apply(str))\n",
    "# lex_bow2_ws = bow2_fit_kt.get_feature_names()\n",
    "\n",
    "# print(text_bow1_kt.toarray().shape,  text_bow1_kt.toarray().shape)\n",
    "# print(text_bow2_kt.toarray().shape,  text_bow2_kt.toarray().shape)\n",
    "# print(\"\\n\")\n",
    "# print(text_bow1_ws.toarray().shape,  text_bow1_ws.toarray().shape)\n",
    "# print(text_bow2_ws.toarray().shape,  text_bow2_ws.toarray().shape)\n",
    "# print(\"\\n\")\n",
    "# print(len(lex_bow1_kt), len(lex_bow1_ws))\n",
    "# print(len(lex_bow2_kt), len(lex_bow2_ws))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566e61aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize texts\n",
    "from visualize import top_feats_all, plot_top_feats\n",
    "features = bow1_fit_kt.get_feature_names()\n",
    "%time ts = top_feats_all(text_bow1_kt.toarray(), y_kt, features)\n",
    "print(ts[0].shape)\n",
    "ts[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8410282",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time plot_top_feats(ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d7b552",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3232d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF with unigram and bigrams\n",
    "tfidf1 = TfidfVectorizer(tokenizer=process_text, ngram_range=(1, 1), min_df=5)\n",
    "tfidf2 = TfidfVectorizer(tokenizer=process_text, ngram_range=(2, 2), min_df=5)\n",
    "\n",
    "# fit kt and transform to both datasets\n",
    "tfidf1_fit_kt = tfidf1.fit(df_kt['text'].apply(str))\n",
    "text_tfidf1_kt = tfidf1_fit_kt.transform(df_kt['text'].apply(str))\n",
    "lex_tfidf1_kt = tfidf1_fit_kt.get_feature_names()\n",
    "text_tfidf1_ws = tfidf1_fit_kt.transform(df_ws['texts'].apply(str))\n",
    "lex_tfidf1_ws = tfidf1_fit_kt.get_feature_names()\n",
    "\n",
    "tfidf2_fit_kt = tfidf2.fit(df_kt['text'].apply(str))\n",
    "text_tfidf2_kt = tfidf2_fit_kt.transform(df_kt['text'].apply(str))\n",
    "lex_tfidf2_kt = tfidf2_fit_kt.get_feature_names()\n",
    "text_tfidf2_ws = tfidf2_fit_kt.transform(df_ws['texts'].apply(str))\n",
    "lex_tfidf2_ws = tfidf2_fit_kt.get_feature_names()\n",
    "\n",
    "print(text_tfidf1_kt.toarray().shape,  text_tfidf1_kt.toarray().shape)\n",
    "print(text_tfidf2_kt.toarray().shape,  text_tfidf2_kt.toarray().shape)\n",
    "print(\"\\n\")\n",
    "print(text_tfidf1_ws.toarray().shape,  text_tfidf1_ws.toarray().shape)\n",
    "print(text_tfidf2_ws.toarray().shape,  text_tfidf2_ws.toarray().shape)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(len(lex_tfidf1_kt), len(lex_tfidf1_ws))\n",
    "print(len(lex_tfidf2_kt), len(lex_tfidf2_ws))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2867680",
   "metadata": {},
   "source": [
    "## Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8551fe6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "\n",
    "class TfidfEmbeddingVectorizer(object):\n",
    "    def __init__(self, model):\n",
    "        w2v = {w: vec for w, vec in zip(model.wv.index_to_key, model.wv.vectors)}\n",
    "        self.word2vec = w2v\n",
    "        self.word2weight = None\n",
    "        self.dim = model.vector_size\n",
    "    \n",
    "    def fit(self, X):\n",
    "        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "        tfidf.fit(X)\n",
    "        max_idf = max(tfidf.idf_)\n",
    "        self.word2weight = defaultdict(\n",
    "            lambda: max_idf,\n",
    "            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "                np.mean([self.word2vec[w] * self.word2weight[w]\n",
    "                         for w in words if w in self.word2vec] or\n",
    "                        [np.zeros(self.dim)], axis=0)\n",
    "                for words in X\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856dd1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from pythainlp import word_vector\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "\n",
    "# create word2vec for kt corpus\n",
    "w2v_kt = Word2Vec(vector_size=300, min_count=1, window=4, workers=4)\n",
    "w2v_kt.build_vocab(df_kt['processed'])\n",
    "w2v_kt.train(df_kt['processed'], total_examples=w2v_kt.corpus_count, epochs=100)\n",
    "w2v_kt.wv.most_similar(\"‡∏ö‡∏∞‡∏´‡∏°‡∏µ‡πà\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542624be",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_tfidf_emb_kt = TfidfEmbeddingVectorizer(w2v_kt)\n",
    "w2v_tifdf_fit_kt = w2v_tfidf_emb_kt.fit(df_kt['text'].apply(str))\n",
    "\n",
    "# transfrom on both corpuses\n",
    "text_w2v_tfidf_kt = w2v_tifdf_fit_kt.transform(df_kt['text'].apply(str))\n",
    "text_w2v_tfidf_ws = w2v_tifdf_fit_kt.transform(df_ws['texts'].apply(str))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2102229e",
   "metadata": {},
   "source": [
    "## POS Tagging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0387c5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of exisiting pos-tagging techniques.  \n",
    "# TODO: 1. test performance of ech\n",
    "\n",
    "# concate word and tag with underscore (‡∏°‡∏±‡∏ô_ADV)\n",
    "def word_tag(pos):\n",
    "    tag_list = []\n",
    "    for item in pos:\n",
    "        tag = ['_'.join(map(str, tups)) for tups in item]\n",
    "        tag_list.append(' '.join(tag))\n",
    "    return tag_list\n",
    "\n",
    "# use only tag\n",
    "def tag(pos):\n",
    "    tag_list = []\n",
    "    for item in pos:\n",
    "        tmp = [el[1] for el in item]\n",
    "        tag = ' '.join(map(str, tmp))\n",
    "        tag_list.append(tag)\n",
    "    return tag_list\n",
    "\n",
    "# create custome tag for emoticon word\n",
    "def tag_emoj(pos):\n",
    "    tag_list = []\n",
    "    \n",
    "    for i in range(len(pos)):\n",
    "        pl = [list(el) for el in pos[i]]\n",
    "        tag_list.append(pl)\n",
    "\n",
    "    for item in tag_list:\n",
    "        for i, tag in enumerate(item):\n",
    "            for j, word in enumerate(tag):\n",
    "                if \"EMJ\" in word:\n",
    "                    item[i][1] = \"EMOJI\"\n",
    "    return tag_list\n",
    "\n",
    "# this approach convert nest-list to simply list of word follows by tag ['word', 'NOUN']\n",
    "def flatten(text):\n",
    "    res = list(sum(text, ()))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c257e9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from pythainlp.tag import pos_tag_sents\n",
    "\n",
    "pos = pos_tag_sents(df_kt['processed'].tolist(), corpus='orchid_ud')\n",
    "pos = tag_emoj(pos)\n",
    "df_kt['post_tag1'] = pd.DataFrame(tag(pos))\n",
    "df_kt['post_tag2'] = pd.DataFrame(word_tag(pos))\n",
    "\n",
    "pos = pos_tag_sents(df_ws['processed'].tolist(), corpus='orchid_ud')\n",
    "pos = tag_emoj(pos)\n",
    "df_ws['post_tag1'] = pd.DataFrame(tag(pos))\n",
    "df_ws['post_tag2'] = pd.DataFrame(word_tag(pos))\n",
    "\n",
    "print(df_ws[['processed', 'post_tag1']].iloc[1000:1010])\n",
    "print(\"\\n\")\n",
    "print(df_ws['post_tag2'].iloc[1000:1010])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3755ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create bow vectors\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "bow1 = CountVectorizer(ngram_range=(1, 1))\n",
    "#bow2 = CountVectorizer(ngram_range=(2, 2))\n",
    "\n",
    "text_pos_bow1_fit_kt = bow1.fit(df_kt['post_tag1'])\n",
    "text_pos_bow1_kt = text_pos_bow1_fit_kt.transform(df_kt['post_tag1'])\n",
    "text_pos_bow1_ws = text_pos_bow1_fit_kt.transform(df_ws['post_tag1'])\n",
    "\n",
    "# text_pos_bow2_fit_kt = bow2.fit(df_kt['post_tag1'])\n",
    "# text_pos_bow2_kt = text_pos_bow2_fit_kt.transform(df_kt['post_tag2'])\n",
    "# text_pos_bow2_ws = text_pos_bow2_fit_kt.transform(df_ws['post_tag2'])\n",
    "\n",
    "print(text_pos_bow1_kt.toarray().shape,  text_pos_bow1_kt.toarray().shape)\n",
    "print(text_pos_bow1_ws.toarray().shape,  text_pos_bow1_ws.toarray().shape)\n",
    "\n",
    "# print(text_pos_bow2_kt.toarray().shape,  text_pos_bow2_kt.toarray().shape)\n",
    "# print(text_pos_bow2_ws.toarray().shape,  text_pos_bow2_ws.toarray().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eef9cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pos_bow1_ws.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568848ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tfidf vectors\n",
    "# tfidf1 = TfidfVectorizer(ngram_range=(1, 1))\n",
    "# tfidf2 = TfidfVectorizer(ngram_range=(2, 2))\n",
    "\n",
    "# text_pos_tfidf1_fit_kt = tfidf1.fit(df_kt['post_tag1'])\n",
    "# text_pos_tfidf1_kt = text_pos_tfidf1_fit_kt.transform(df_kt['post_tag1'])\n",
    "# text_pos_tfidf1_ws = text_pos_tfidf1_fit_kt.transform(df_ws['post_tag1'])\n",
    "\n",
    "# text_pos_tfidf2_fit_kt = bow2.fit(df_kt['post_tag1'])\n",
    "# text_pos_tfidf2_kt = text_pos_bow2_fit_kt.transform(df_kt['post_tag2'])\n",
    "# text_pos_tfidf2_ws = text_pos_bow2_fit_kt.transform(df_ws['post_tag2'])\n",
    "\n",
    "# print(text_pos_bow1_kt.toarray().shape,  text_pos_bow2_kt.toarray().shape)\n",
    "# print(text_pos_bow1_ws.toarray().shape,  text_pos_bow2_ws.toarray().shape)\n",
    "\n",
    "# print(text_pos_bow2_kt.toarray().shape,  text_pos_bow2_kt.toarray().shape)\n",
    "# print(text_pos_bow2_ws.toarray().shape,  text_pos_bow2_ws.toarray().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c6949e",
   "metadata": {},
   "source": [
    "## Dictionary-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e922a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load list of our custom positive and negative words\n",
    "with open(os.path.dirname(os.getcwd()) + '\\\\data\\\\' + 'pos_words.txt', encoding='UTF-8') as f:\n",
    "    pos_words = [line.rstrip('\\n') for line in f]\n",
    "\n",
    "with open(os.path.dirname(os.getcwd()) + '\\\\data\\\\' + 'neg_words.txt', encoding='UTF-8') as f:\n",
    "    neg_words = [line.rstrip('\\n') for line in f]\n",
    "pos_words = list(set(pos_words))\n",
    "neg_words = list(set(neg_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad848e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow1 = CountVectorizer(tokenizer=process_text, ngram_range=(1, 1))\n",
    "bow2 = CountVectorizer(tokenizer=process_text, ngram_range=(2, 2))\n",
    "\n",
    "my_vocabs = pos_words + neg_words\n",
    "print('dict size: ', len(my_vocabs))\n",
    "\n",
    "text_dict_bow1_fit = bow1.fit(my_vocabs)\n",
    "text_dict_bow1_kt = text_dict_bow1_fit.transform(df_kt['text'].apply(str))\n",
    "text_dict_bow1_ws = text_dict_bow1_fit.transform(df_ws['texts'].apply(str))\n",
    "\n",
    "text_dict_bow2_fit = bow2.fit(my_vocabs)\n",
    "text_dict_bow2_kt = text_dict_bow2_fit.transform(df_kt['text'].apply(str))\n",
    "text_dict_bow2_ws = text_dict_bow2_fit.transform(df_ws['texts'].apply(str))\n",
    "\n",
    "print(text_dict_bow1_kt.toarray().shape,  text_dict_bow1_kt.toarray().shape)\n",
    "print(text_dict_bow1_ws.toarray().shape,  text_dict_bow1_ws.toarray().shape)\n",
    "\n",
    "print(text_dict_bow2_kt.toarray().shape,  text_dict_bow2_kt.toarray().shape)\n",
    "print(text_dict_bow2_ws.toarray().shape,  text_dict_bow2_ws.toarray().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b616abd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf1 = TfidfVectorizer(tokenizer=process_text, ngram_range=(1, 1))\n",
    "tfidf2 = TfidfVectorizer(tokenizer=process_text, ngram_range=(2, 2))\n",
    "\n",
    "text_dict_tfidf1_fit = tfidf1.fit(my_vocabs)\n",
    "text_dict_tfidf1_kt = text_dict_tfidf1_fit.transform(df_kt['text'].apply(str))\n",
    "text_dict_tfidf1_ws = text_dict_tfidf1_fit.transform(df_ws['texts'].apply(str))\n",
    "\n",
    "text_dict_tfidf2_fit = tfidf2.fit(my_vocabs)\n",
    "text_dict_tfidf2_kt = text_dict_tfidf2_fit.transform(df_kt['text'].apply(str))\n",
    "text_dict_tfidf2_ws = text_dict_tfidf2_fit.transform(df_ws['texts'].apply(str))\n",
    "\n",
    "print(text_dict_tfidf1_kt.toarray().shape,  text_dict_tfidf1_kt.toarray().shape)\n",
    "print(text_dict_tfidf1_ws.toarray().shape,  text_dict_tfidf1_ws.toarray().shape)\n",
    "\n",
    "print(text_dict_tfidf2_kt.toarray().shape,  text_dict_tfidf2_kt.toarray().shape)\n",
    "print(text_dict_tfidf2_ws.toarray().shape,  text_dict_tfidf2_ws.toarray().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd1b6f8",
   "metadata": {},
   "source": [
    "## Save to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29c11aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import joblib\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a417fd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_t_kt = y_kt.to_numpy().reshape(-1, 1)\n",
    "y_t_ws = y_ws.to_numpy().reshape(-1, 1)\n",
    "\n",
    "y_t_kt = sparse.csr_matrix(y_t_kt)\n",
    "y_t_ws = sparse.csr_matrix(y_t_ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b62645",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_bow1_kt = np.hstack((text_bow1_kt, y_t_kt))\n",
    "arr_bow2_kt = np.hstack((text_bow2_kt, y_t_kt))\n",
    "joblib.dump(arr_bow1_kt, model_path+'text_bow1_kt.pkl')\n",
    "joblib.dump(arr_bow2_kt, model_path+'text_bow2_kt.pkl')\n",
    "\n",
    "joblib.dump(lex_bow1_kt, lexicon_path+'lex_bow1_kt.pkl')\n",
    "joblib.dump(lex_bow2_kt, lexicon_path+'lex_bow2_kt.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1deed356",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_bow1_ws = np.hstack((text_bow1_ws, y_t_ws))\n",
    "arr_bow2_ws = np.hstack((text_bow2_ws, y_t_ws))\n",
    "joblib.dump(arr_bow1_ws, model_path+'text_bow1_ws.pkl')\n",
    "joblib.dump(arr_bow2_ws, model_path+'text_bow2_ws.pkl')\n",
    "\n",
    "joblib.dump(lex_bow1_ws, lexicon_path+'lex_bow1_ws.pkl')\n",
    "joblib.dump(lex_bow2_ws, lexicon_path+'lex_bow2_ws.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b779dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_tfidf1_kt = np.hstack((text_tfidf1_kt, y_t_kt))\n",
    "arr_tfidf2_kt = np.hstack((text_tfidf2_kt, y_t_kt))\n",
    "joblib.dump(arr_tfidf1_kt, model_path+'text_tfidf1_kt.pkl')\n",
    "joblib.dump(arr_tfidf2_kt, model_path+'text_tfidf2_kt.pkl')\n",
    "\n",
    "joblib.dump(lex_tfidf1_kt, lexicon_path+'lex_tfidf1_kt.pkl')\n",
    "joblib.dump(lex_tfidf2_kt, lexicon_path+'lex_tfidf2_kt.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0cd71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_tfidf1_ws = np.hstack((text_tfidf1_ws, y_t_ws))\n",
    "arr_tfidf2_ws = np.hstack((text_tfidf2_ws, y_t_ws))\n",
    "joblib.dump(arr_tfidf1_ws, model_path+'text_tfidf1_ws.pkl')\n",
    "joblib.dump(arr_tfidf2_ws, model_path+'text_tfidf2_ws.pkl')\n",
    "\n",
    "joblib.dump(lex_tfidf1_ws, lexicon_path+'lex_tfidf1_ws.pkl')\n",
    "joblib.dump(lex_tfidf2_ws, lexicon_path+'lex_tfidf2_ws.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04c1868",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_w2v_tfidf_kt = np.hstack(( sparse.csr_matrix(text_w2v_tfidf_kt), y_t_kt))\n",
    "arr_w2v_tfidf_ws = np.hstack(( sparse.csr_matrix(text_w2v_tfidf_ws), y_t_ws))\n",
    "joblib.dump(arr_w2v_tfidf_kt, model_path+'text_w2v_tfidf_kt.pkl')\n",
    "joblib.dump(arr_w2v_tfidf_ws, model_path+'text_w2v_tfidf_ws.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7bbd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_pos_bow1_kt = np.hstack((text_pos_bow1_kt, y_t_kt))\n",
    "#arr_pos_bow2_kt = np.hstack((text_pos_bow2_kt, y_t_kt))\n",
    "joblib.dump(arr_pos_bow1_kt, model_path+'text_pos_bow1_kt.pkl')\n",
    "#joblib.dump(arr_pos_bow2_kt, model_path+'text_pos_bow2_kt.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc35210",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_pos_bow1_ws = np.hstack((text_pos_bow1_ws, y_t_ws))\n",
    "#arr_pos_bow2_ws = np.hstack((text_pos_bow2_ws, y_t_ws))\n",
    "joblib.dump(arr_pos_bow1_ws, model_path+'text_pos_bow1_ws.pkl')\n",
    "#joblib.dump(arr_pos_bow2_ws, model_path+'text_pos_bow2_ws.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd754034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arr_pos_tfidf1_kt = np.hstack((text_pos_tfidf1_kt, y_t_kt))\n",
    "# arr_pos_tfidf2_kt = np.hstack((text_pos_tfidf2_kt, y_t_kt))\n",
    "# joblib.dump(arr_pos_tfidf1_kt, model_path+'text_pos_tfidf1_kt.pkl')\n",
    "# joblib.dump(arr_pos_tfidf2_kt, model_path+'text_pos_tfidf2_kt.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff31a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arr_pos_tfidf1_ws = np.hstack((text_pos_tfidf1_ws, y_t_ws))\n",
    "# arr_pos_tfidf2_ws = np.hstack((text_pos_tfidf2_ws, y_t_ws))\n",
    "# joblib.dump(arr_pos_tfidf1_ws, model_path+'text_pos_tfidf1_ws.pkl')\n",
    "# joblib.dump(arr_pos_tfidf2_ws, model_path+'text_pos_tfidf2_ws.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78281902",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_dict_bow1_kt = np.hstack((text_dict_bow1_kt, y_t_kt))\n",
    "arr_dict_bow2_kt = np.hstack((text_dict_bow2_kt, y_t_kt))\n",
    "joblib.dump(arr_dict_bow1_kt, model_path+'text_dict_bow1_kt.pkl')\n",
    "joblib.dump(arr_dict_bow2_kt, model_path+'text_dict_bow2_kt.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a3abf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_dict_bow1_ws = np.hstack((text_dict_bow1_ws, y_t_ws))\n",
    "arr_dict_bow2_ws = np.hstack((text_dict_bow2_ws, y_t_ws))\n",
    "joblib.dump(arr_dict_bow1_ws, model_path+'text_dict_bow1_ws.pkl')\n",
    "joblib.dump(arr_dict_bow2_ws, model_path+'text_dict_bow2_ws.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ccbf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_dict_tfidf1_kt = np.hstack((text_dict_tfidf1_kt, y_t_kt))\n",
    "arr_dict_tfidf2_kt = np.hstack((text_dict_tfidf2_kt, y_t_kt))\n",
    "joblib.dump(arr_dict_tfidf1_kt, model_path+'text_dict_tfidf1_kt.pkl')\n",
    "joblib.dump(arr_dict_tfidf2_kt, model_path+'text_dict_tfidf2_kt.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa13781e",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_dict_tfidf1_ws = np.hstack((text_dict_tfidf1_ws, y_t_ws))\n",
    "arr_dict_tfidf2_ws = np.hstack((text_dict_tfidf2_ws, y_t_ws))\n",
    "joblib.dump(arr_dict_tfidf1_ws, model_path+'text_dict_tfidf1_ws.pkl')\n",
    "joblib.dump(arr_dict_tfidf2_ws, model_path+'text_dict_tfidf2_ws.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f1e6f2",
   "metadata": {},
   "source": [
    "## Demonstrate usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e15f9a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load kt corpus extracted features array \n",
    "#arr_kt = np.load(model_path+'text_tfidf2_kt.joblib')\n",
    "    \n",
    "item = joblib.load(model_path+'text_bow1_kt.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5b9f719f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#arr_kt = np.concatenate((item[0].A, item[1].A), axis=1)\n",
    "arr_bow1_kt = np.hstack((text_bow1_kt, y_t_kt))\n",
    "#arr_kt = np.concatenate(text_bow1_kt, y_t_kt)\n",
    "arr_kt = np.concatenate((arr_bow1_kt[0].A, arr_bow1_kt[1].A), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79abee6",
   "metadata": {},
   "source": [
    "## Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7f2e673d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((48064, 1395), (12017, 1395))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "train_df, test_df = train_test_split(arr_kt, test_size=0.20, random_state=42)\n",
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520d359d",
   "metadata": {},
   "source": [
    "## Train-valid split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "87519fa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((40854, 1394), (7210, 1394), (40854,), (7210,))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(train_df[:,:-1], train_df[:,-1], test_size=0.15, random_state=42)\n",
    "X_train.shape, X_valid.shape, y_train.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3159c026",
   "metadata": {},
   "source": [
    "## Test the extracted features with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fc1413dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6694868238557559"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quick test with out cv\n",
    "#fit logistic regression models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(C=2., penalty=\"l2\", solver=\"liblinear\", dual=False, multi_class=\"ovr\")\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_valid, y_valid)\n",
    "#y_pred = model.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d6282242",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
    "def build_model(model):\n",
    "    scores = (cross_val_score(model, X_train, y_train, cv = 5).mean())\n",
    "    model = model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_valid)\n",
    "    acc_sc = accuracy_score(y_valid, y_pred)\n",
    "    pre_sc = precision_score(y_valid, y_pred, average='weighted')\n",
    "    rec_sc = recall_score(y_valid, y_pred, average='weighted')\n",
    "    f1_sc = f1_score(y_valid, y_pred, average='weighted')\n",
    "    print('Accuracy :',acc_sc)\n",
    "    print('Confusion Matrix :\\n', confusion_matrix(y_valid, y_pred))\n",
    "    print('Precision :', pre_sc)\n",
    "    print('Recall :', rec_sc)\n",
    "    print('F1-score :', f1_sc)\n",
    "    print('Classification Report :\\n', classification_report(y_valid, y_pred))\n",
    "    print('Average accuracy of k-fold (5-fold) :', scores ,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3e120d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.6694868238557559\n",
      "Confusion Matrix :\n",
      " [[ 208  902   71]\n",
      " [ 122 4184  204]\n",
      " [  63 1021  435]]\n",
      "Precision : 0.6443251170595544\n",
      "Recall : 0.6694868238557559\n",
      "F1-score : 0.6185379590395134\n",
      "Classification Report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.18      0.26      1181\n",
      "           1       0.69      0.93      0.79      4510\n",
      "           2       0.61      0.29      0.39      1519\n",
      "\n",
      "    accuracy                           0.67      7210\n",
      "   macro avg       0.61      0.46      0.48      7210\n",
      "weighted avg       0.64      0.67      0.62      7210\n",
      "\n",
      "Average accuracy of k-fold (5-fold) : 0.6694325410027732 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "build_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "18ed0983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26737, 17304)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict on kt dataset\n",
    "#arr_ws = np.load(model_path+'text_tfidf2_ws.npy')\n",
    "item = joblib.load(model_path+'text_tfidf2_ws.pkl')\n",
    "arr_ws = np.concatenate((item[0].A, item[1].A), axis=1)\n",
    "arr_ws.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4c086afc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((21389, 17304), (5348, 17304))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df, test_df = train_test_split(arr_ws, test_size=0.20, random_state=42)\n",
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6d73174f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((18180, 17303), (3209, 17303), (18180,), (3209,))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(train_df[:,:-1], train_df[:,-1], test_size=0.15, random_state=42)\n",
    "X_train.shape, X_valid.shape, y_train.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "640f33fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.6251168588345279\n",
      "Confusion Matrix :\n",
      " [[ 327  442   21    0]\n",
      " [ 116 1571   67    5]\n",
      " [  43  426  107    2]\n",
      " [   2   76    3    1]]\n",
      "Precision : 0.6078938333813285\n",
      "Recall : 0.6251168588345279\n",
      "F1-score : 0.5791858773594074\n",
      "Classification Report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.67      0.41      0.51       790\n",
      "         1.0       0.62      0.89      0.74      1759\n",
      "         2.0       0.54      0.19      0.28       578\n",
      "         3.0       0.12      0.01      0.02        82\n",
      "\n",
      "    accuracy                           0.63      3209\n",
      "   macro avg       0.49      0.38      0.39      3209\n",
      "weighted avg       0.61      0.63      0.58      3209\n",
      "\n",
      "Average accuracy of k-fold (5-fold) : 0.6190319031903191 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "build_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fa3504",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
